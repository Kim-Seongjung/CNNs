{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 28\n",
      "28 28\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "#tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "#save_and_restore =Not yet\n",
    "#tensorboard = Not yet\n",
    "\"\"\"\n",
    "created by kim Seong jung , Medi-Whale.Inc \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "import math\n",
    "\n",
    "file_locate='/home/ncc/Desktop/npy/'\n",
    "sess = tf.InteractiveSession()\n",
    "test_img=np.load(file_locate+'test_img.npy');\n",
    "\n",
    "img_row =28\n",
    "img_col =28\n",
    "batch_size=30\n",
    "print img_row ,img_col\n",
    "n_classes =10\n",
    "in_ch =1\n",
    "out_ch1=20\n",
    "out_ch2=20\n",
    "out_ch3=20\n",
    "out_ch4=20\n",
    "out_ch5=20\n",
    "\n",
    "\n",
    "fully_ch1=300\n",
    "fully_ch2 =300\n",
    "fully_ch3 =1024\n",
    "\n",
    "\n",
    "############convolution layer setting#############\n",
    "conv_weight_1 =[3,3,in_ch,out_ch1]\n",
    "conv_weight_2 =[3,3,out_ch1,out_ch2]\n",
    "conv_weight_3 =[3,3,out_ch2,out_ch3]\n",
    "conv_weight_4 =[3,3,out_ch3,out_ch4]\n",
    "conv_weight_5 =[3,3,out_ch4,out_ch5]\n",
    "\n",
    "conv_strides_1=[1,1,1,1]\n",
    "conv_strides_2=[1,1,1,1]\n",
    "conv_strides_3=[1,1,1,1]\n",
    "conv_strides_4=[1,1,1,1]\n",
    "conv_strides_5=[1,1,1,1]\n",
    "\n",
    "############pooling layer settting #############\n",
    "\n",
    "pool_weight_1=[1,2,2,1]\n",
    "pool_weight_2=[1,2,2,1]\n",
    "pool_wieght_5=[1,2,2,1]\n",
    "\n",
    "pool_strides_1=[1,2,2,1]\n",
    "pool_strides_2=[1,2,2,1]\n",
    "pool_strides_5=[1,2,2,1]\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "#                                   AlexNet                                           #\n",
    "#                                                                                     #\n",
    "# conv1 <K: 1,11,11,1> <S: 1,4,4,1>->lrn-->relu-> max pooling <K 1,3,3,1> ,<S 1,2,2,1>#\n",
    "# conv2 <K: 1,5,5,1> <S: 1,1,1,1>->lrn-->relu-> max pooling <K 1,3,3,1> ,<S 1,2,2,1>  #\n",
    "# conv3 <K: 1,3,3,1> <S: 1,1,1,1>                                                     #\n",
    "# conv4 <K: 1,3,3,1> <S: 1,1,1,1>                                                     #\n",
    "# conv5 <K: 1,3,3,1> <S: 1,1,1,1>->lrn->relu-> max pooling <K 1,3,3,1> ,<S 1,2,2,1>   #\n",
    "# fc1-->matmul-->relu-->dropout                                                       #\n",
    "# fc2-->matmul-->relu-->dropout                                                       #\n",
    "# softmax-->y_conv                                                                    #\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x= tf.placeholder(\"float\",shape=[30,img_col*img_row *in_ch],  name = 'x-input')\n",
    "y_=tf.placeholder(\"float\",shape=[30 , n_classes] , name = 'y-input')\n",
    "\n",
    "\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "x_image= tf.reshape(x,[30,img_row,img_col,in_ch])\n",
    "\n",
    "iterate=30000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pooling_row_size1=int(img_row/2)\n",
    "pooling_row_size2=int(pooling_row_size1/2)\n",
    "pooling_row_size3=int(pooling_row_size2/2)\n",
    "pooling_row_size4=int(pooling_row_size3/2)\n",
    "pooling_row_size5=int(pooling_row_size4/2)\n",
    "pooling_col_size1=int(img_col/2)\n",
    "pooling_col_size2=int(pooling_col_size1/2)\n",
    "pooling_col_size3=int(pooling_col_size2/2)\n",
    "pooling_col_size4=int(pooling_col_size3/2)\n",
    "pooling_col_size5=int(pooling_col_size4/2)\n",
    "\n",
    "print img_col , img_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (55000, 784)\n",
      "Training Data Label (55000, 10)\n",
      "Test Data Label (10000, 10)\n",
      "val Data Label (5000, 784)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "    #with tf.device('/gpu:1'):\n",
    "    train_img=np.load(file_locate+'train_img.npy');\n",
    "    train_lab=np.load(file_locate+'train_lab.npy');\n",
    "    val_img= np.load(file_locate+'val_img.npy');\n",
    "    val_lab = np.load(file_locate+'val_lab.npy');\n",
    "    test_img=np.load(file_locate+'test_img.npy');\n",
    "    test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "    print \"Training Data\",np.shape(train_img)\n",
    "    print \"Training Data Label\",np.shape(train_lab)\n",
    "    print \"Test Data Label\",np.shape(test_lab)\n",
    "    print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "    n_train= np.shape(train_img)[0]\n",
    "    n_train_lab = np.shape(train_lab)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def aug_and_train(batch_x , batch_y , crop_row , crop_col ,keep_prob , sess , reflect = 'horizon' ):\n",
    "    \"\"\"\n",
    "    \n",
    "    image size is 256 ,256 \n",
    "    \n",
    "        for i in range(aug_num):\n",
    "         for start_x in range(remainder_row):\n",
    "        for start_y in range(remainder_col):\n",
    "            print start_x , start_y\n",
    "            cropped_img= img[start_x :start_x+width ,start_y:start_y+height,:]\n",
    "            rev_cropped_img =np.fliplr(cropped_img)\n",
    "            plt.imshow(new_img)\n",
    "            img[start_x :start_x+width ,start_y:start_y+height,:]=rev_cropped_img\n",
    "        sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "    np_pic shape have to [# , row , col , ch]\n",
    "    \"\"\"\n",
    "    ori_img=np_pic\n",
    "    size_batch,row ,col , ch = np.shape(ori_img)\n",
    "    print size_batch, row ,col ,ch\n",
    "\n",
    "\n",
    "    img=ori_img.copy()\n",
    "    remainder_row=row-crop_row\n",
    "    remainder_col=col-crop_col\n",
    "    new_img = np.zeros([row-remainder_row, col-remainder_col, ch])\n",
    "    print np.shape(new_img)\n",
    "    #print img\n",
    "    height = col -remainder_col\n",
    "    width =  row - remainder_row\n",
    "    i=0 ;j=0;\n",
    "    start_time=time.time()\n",
    "    for ind_batch in range(len(batch_x)):\n",
    "        img_batch = batch_x[ind_batch:ind_batch+1 , : ,: ,: ]\n",
    "        lab_batch = batch_y[ind_batch:ind_batch+1,  :, : ,: ]\n",
    "        for start_x in range(remainder_row):\n",
    "            for start_y in range(remainder_col):\n",
    "                print start_x , start_y\n",
    "                cropped_img= img[start_x :start_x+width ,start_y:start_y+height,:] #crop image that appointed  \n",
    "                if reflect== 'horizon':\n",
    "                    rev_cropped_img =np.fliplr(cropped_img) # horizontal reflection \n",
    "                img[start_x :start_x+width ,start_y:start_y+height,:]=rev_cropped_img # attach image                      \n",
    "                sess.run(train_step ,feed_dict={x:img, y_:lab , keep_prob : 0.5}) #training imgae \n",
    "                end_time = time.time()\n",
    "        total_time=end_time-start_time\n",
    "        print total_time     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "with tf.device('/gpu:0'):\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1 , shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    def conv2d(x,w,conv_strides_):\n",
    "        return tf.nn.conv2d(x,w, strides = conv_strides_, padding='SAME')\n",
    "    def max_pool(x , ksize=[1,2,2,1] ,  strides = [1,2,2,1], padding = 'SAME'):\n",
    "        return tf.nn.max_pool(x , ksize ,strides, padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def local_response_nomalization(conv , radius , alpha , beta , bias , batch_size =30):\n",
    "    \n",
    "    \"\"\"\n",
    "    in paper , alexent \n",
    "    hyperparameter  n=5, alpha= 10^-4 , beta=0.75 , bias = 2 \n",
    "    \n",
    "    sqr_sum[a, b, c, d] =\n",
    "    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)\n",
    "    output = input / (bias + alpha * sqr_sum) ** beta\n",
    "    \n",
    "    if you don't understand above code look at the code at below\n",
    "    #############################################################\n",
    "    a=[1,2,3,4,]\n",
    "    a=np.ndarray(shape=(2,3))\n",
    "    a[0,0]=1;a[0,1]=2;a[0,2]=3 ;a[1,0]=4;a[1,1]=5;a[1,2]=6\n",
    "    \n",
    "    \n",
    "    c=(a[1,:3] **2 )\n",
    "    print a    \n",
    "    print c\n",
    "    [[ 1.  2.  3.]\n",
    "     [ 4.  5.  6.]]\n",
    "    [ 16.  25.  36.]\n",
    "    #############################################################\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    if type(conv).__module__ == np.__name__:\n",
    "        N , row , col , n_ch =np.shape(conv)\n",
    "        lrn_conv = np.zeros(shape=[N,row ,col , n_ch])\n",
    "        for i in range(N):\n",
    "            for r in range(row):\n",
    "                for c in range(col):\n",
    "                    #conv[row][col] in paper , Alex == a(x,y) (at apointed kernel i )\n",
    "                    max_range =max(0,c - radius/2)\n",
    "                    min_range =min(n_ch-1,c+radius/2)       \n",
    "                    #if c==0:   \n",
    "                    #print max_range , min_range\n",
    "                    \n",
    "                    for ch in range(max_range,min_range):\n",
    "                        \n",
    "                        normalized_constant=(bias+alpha*(sum(conv[i,r,c,min_range : max_range])**2))**beta\n",
    "                        lrn_img=conv[i,:,:,:]/normalized_constant\n",
    "                        lrn_conv[i,:,:,:]=lrn_img \n",
    "\n",
    "    return lrn_conv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a=np.ndarray(shape=[2,2])\n",
    "print type(a).__module__ =='numpy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 256, 256, 3)\n",
      "<type 'numpy.ndarray'>\n",
      "[[[[  23.   23.   23.]\n",
      "   [  24.   24.   24.]\n",
      "   [  24.   24.   24.]\n",
      "   ..., \n",
      "   [  26.   26.   26.]\n",
      "   [  22.   22.   22.]\n",
      "   [  29.   29.   29.]]\n",
      "\n",
      "  [[  17.   17.   17.]\n",
      "   [  17.   17.   17.]\n",
      "   [  18.   18.   18.]\n",
      "   ..., \n",
      "   [  17.   17.   15.]\n",
      "   [  20.   20.   20.]\n",
      "   [  14.   14.   12.]]\n",
      "\n",
      "  [[ 197.  197.  197.]\n",
      "   [ 197.  197.  197.]\n",
      "   [ 197.  197.  197.]\n",
      "   ..., \n",
      "   [ 204.  203.  199.]\n",
      "   [ 202.  201.  199.]\n",
      "   [ 165.  164.  160.]]\n",
      "\n",
      "  ..., \n",
      "  [[  27.   24.   19.]\n",
      "   [  21.   20.   15.]\n",
      "   [  31.   28.   23.]\n",
      "   ..., \n",
      "   [  25.   26.   28.]\n",
      "   [  24.   26.   25.]\n",
      "   [  23.   25.   24.]]\n",
      "\n",
      "  [[  23.   22.   20.]\n",
      "   [  28.   28.   26.]\n",
      "   [  24.   23.   21.]\n",
      "   ..., \n",
      "   [  27.   27.   29.]\n",
      "   [  25.   25.   25.]\n",
      "   [  22.   22.   22.]]\n",
      "\n",
      "  [[  25.   25.   25.]\n",
      "   [  23.   23.   23.]\n",
      "   [  20.   20.   20.]\n",
      "   ..., \n",
      "   [  21.   21.   23.]\n",
      "   [  24.   22.   23.]\n",
      "   [  27.   25.   26.]]]\n",
      "\n",
      "\n",
      " [[[ 100.   80.   53.]\n",
      "   [ 101.   81.   54.]\n",
      "   [ 101.   81.   54.]\n",
      "   ..., \n",
      "   [  66.   50.   37.]\n",
      "   [  68.   51.   41.]\n",
      "   [  62.   45.   35.]]\n",
      "\n",
      "  [[  99.   79.   52.]\n",
      "   [  99.   79.   52.]\n",
      "   [  98.   78.   51.]\n",
      "   ..., \n",
      "   [  57.   41.   28.]\n",
      "   [  60.   43.   33.]\n",
      "   [  58.   41.   31.]]\n",
      "\n",
      "  [[  99.   79.   54.]\n",
      "   [  99.   79.   54.]\n",
      "   [  98.   78.   53.]\n",
      "   ..., \n",
      "   [  51.   34.   24.]\n",
      "   [  53.   39.   28.]\n",
      "   [  56.   42.   31.]]\n",
      "\n",
      "  ..., \n",
      "  [[ 140.  108.   95.]\n",
      "   [ 135.  103.   88.]\n",
      "   [ 142.  113.   99.]\n",
      "   ..., \n",
      "   [ 169.  144.  113.]\n",
      "   [ 162.  137.  106.]\n",
      "   [ 161.  136.  105.]]\n",
      "\n",
      "  [[ 149.  117.  106.]\n",
      "   [ 145.  113.  100.]\n",
      "   [ 135.  105.   94.]\n",
      "   ..., \n",
      "   [ 161.  136.  105.]\n",
      "   [ 166.  141.  110.]\n",
      "   [ 162.  137.  106.]]\n",
      "\n",
      "  [[ 148.  116.  105.]\n",
      "   [ 160.  128.  117.]\n",
      "   [ 142.  112.  101.]\n",
      "   ..., \n",
      "   [ 162.  137.  106.]\n",
      "   [ 161.  136.  105.]\n",
      "   [ 145.  120.   89.]]]\n",
      "\n",
      "\n",
      " [[[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 122.  136.  136.]\n",
      "   [ 123.  137.  137.]\n",
      "   [ 122.  138.  137.]]\n",
      "\n",
      "  [[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 125.  139.  139.]\n",
      "   [ 124.  138.  138.]\n",
      "   [ 121.  137.  136.]]\n",
      "\n",
      "  [[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 125.  139.  140.]\n",
      "   [ 122.  136.  136.]\n",
      "   [ 118.  134.  133.]]\n",
      "\n",
      "  ..., \n",
      "  [[   1.    5.    4.]\n",
      "   [   0.    4.    3.]\n",
      "   [   1.    3.    2.]\n",
      "   ..., \n",
      "   [  65.   69.   52.]\n",
      "   [  83.   91.   70.]\n",
      "   [  99.  107.   86.]]\n",
      "\n",
      "  [[   6.   12.   12.]\n",
      "   [   6.   12.   12.]\n",
      "   [   5.   11.   11.]\n",
      "   ..., \n",
      "   [  59.   63.   46.]\n",
      "   [  78.   86.   65.]\n",
      "   [  96.  106.   82.]]\n",
      "\n",
      "  [[  12.   21.   20.]\n",
      "   [  11.   20.   19.]\n",
      "   [  11.   20.   19.]\n",
      "   ..., \n",
      "   [  54.   62.   41.]\n",
      "   [  76.   84.   63.]\n",
      "   [  95.  105.   81.]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 107.  107.  109.]\n",
      "   [ 133.  133.  135.]\n",
      "   [ 138.  138.  140.]\n",
      "   ..., \n",
      "   [ 169.  169.  169.]\n",
      "   [ 167.  167.  167.]\n",
      "   [ 165.  165.  165.]]\n",
      "\n",
      "  [[ 121.  121.  123.]\n",
      "   [ 153.  153.  155.]\n",
      "   [ 167.  167.  169.]\n",
      "   ..., \n",
      "   [ 200.  200.  200.]\n",
      "   [ 198.  198.  198.]\n",
      "   [ 196.  196.  196.]]\n",
      "\n",
      "  [[ 129.  129.  131.]\n",
      "   [ 166.  166.  168.]\n",
      "   [ 184.  184.  186.]\n",
      "   ..., \n",
      "   [ 218.  218.  218.]\n",
      "   [ 217.  217.  217.]\n",
      "   [ 214.  214.  214.]]\n",
      "\n",
      "  ..., \n",
      "  [[  11.   11.   11.]\n",
      "   [  25.   25.   25.]\n",
      "   [  32.   32.   32.]\n",
      "   ..., \n",
      "   [ 255.  255.  255.]\n",
      "   [ 255.  255.  255.]\n",
      "   [ 252.  252.  252.]]\n",
      "\n",
      "  [[  12.   12.   12.]\n",
      "   [  26.   26.   26.]\n",
      "   [  31.   31.   31.]\n",
      "   ..., \n",
      "   [ 255.  255.  255.]\n",
      "   [ 255.  255.  255.]\n",
      "   [ 252.  252.  252.]]\n",
      "\n",
      "  [[   9.    9.    9.]\n",
      "   [  25.   25.   25.]\n",
      "   [  31.   31.   31.]\n",
      "   ..., \n",
      "   [ 255.  255.  255.]\n",
      "   [ 255.  255.  255.]\n",
      "   [ 252.  252.  252.]]]\n",
      "\n",
      "\n",
      " [[[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 122.  136.  136.]\n",
      "   [ 123.  137.  137.]\n",
      "   [ 122.  138.  137.]]\n",
      "\n",
      "  [[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 125.  139.  139.]\n",
      "   [ 124.  138.  138.]\n",
      "   [ 121.  137.  136.]]\n",
      "\n",
      "  [[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 125.  139.  140.]\n",
      "   [ 122.  136.  136.]\n",
      "   [ 118.  134.  133.]]\n",
      "\n",
      "  ..., \n",
      "  [[   1.    5.    4.]\n",
      "   [   0.    4.    3.]\n",
      "   [   1.    3.    2.]\n",
      "   ..., \n",
      "   [  65.   69.   52.]\n",
      "   [  83.   91.   70.]\n",
      "   [  99.  107.   86.]]\n",
      "\n",
      "  [[   6.   12.   12.]\n",
      "   [   6.   12.   12.]\n",
      "   [   5.   11.   11.]\n",
      "   ..., \n",
      "   [  59.   63.   46.]\n",
      "   [  78.   86.   65.]\n",
      "   [  96.  106.   82.]]\n",
      "\n",
      "  [[  12.   21.   20.]\n",
      "   [  11.   20.   19.]\n",
      "   [  11.   20.   19.]\n",
      "   ..., \n",
      "   [  54.   62.   41.]\n",
      "   [  76.   84.   63.]\n",
      "   [  95.  105.   81.]]]\n",
      "\n",
      "\n",
      " [[[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 122.  136.  136.]\n",
      "   [ 123.  137.  137.]\n",
      "   [ 122.  138.  137.]]\n",
      "\n",
      "  [[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 125.  139.  139.]\n",
      "   [ 124.  138.  138.]\n",
      "   [ 121.  137.  136.]]\n",
      "\n",
      "  [[  52.   42.   40.]\n",
      "   [  54.   44.   42.]\n",
      "   [  53.   43.   41.]\n",
      "   ..., \n",
      "   [ 125.  139.  140.]\n",
      "   [ 122.  136.  136.]\n",
      "   [ 118.  134.  133.]]\n",
      "\n",
      "  ..., \n",
      "  [[   1.    5.    4.]\n",
      "   [   0.    4.    3.]\n",
      "   [   1.    3.    2.]\n",
      "   ..., \n",
      "   [  65.   69.   52.]\n",
      "   [  83.   91.   70.]\n",
      "   [  99.  107.   86.]]\n",
      "\n",
      "  [[   6.   12.   12.]\n",
      "   [   6.   12.   12.]\n",
      "   [   5.   11.   11.]\n",
      "   ..., \n",
      "   [  59.   63.   46.]\n",
      "   [  78.   86.   65.]\n",
      "   [  96.  106.   82.]]\n",
      "\n",
      "  [[  12.   21.   20.]\n",
      "   [  11.   20.   19.]\n",
      "   [  11.   20.   19.]\n",
      "   ..., \n",
      "   [  54.   62.   41.]\n",
      "   [  76.   84.   63.]\n",
      "   [  95.  105.   81.]]]]\n",
      "[[[[  13.67588182   13.67588182   13.67588182]\n",
      "   [  14.27048538   14.27048538   14.27048538]\n",
      "   [  14.27048538   14.27048538   14.27048538]\n",
      "   ..., \n",
      "   [  15.4596925    15.4596925    15.4596925 ]\n",
      "   [  13.08127827   13.08127827   13.08127827]\n",
      "   [  17.24350317   17.24350317   17.24350317]]\n",
      "\n",
      "  [[  10.10826048   10.10826048   10.10826048]\n",
      "   [  10.10826048   10.10826048   10.10826048]\n",
      "   [  10.70286404   10.70286404   10.70286404]\n",
      "   ..., \n",
      "   [  10.10826048   10.10826048    8.91905336]\n",
      "   [  11.89207115   11.89207115   11.89207115]\n",
      "   [   8.32444981    8.32444981    7.13524269]]\n",
      "\n",
      "  [[ 117.13690083  117.13690083  117.13690083]\n",
      "   [ 117.13690083  117.13690083  117.13690083]\n",
      "   [ 117.13690083  117.13690083  117.13690083]\n",
      "   ..., \n",
      "   [ 121.29912573  120.70452217  118.32610794]\n",
      "   [ 120.10991862  119.51531506  118.32610794]\n",
      "   [  98.10958699   97.51498343   95.1365692 ]]\n",
      "\n",
      "  ..., \n",
      "  [[  16.05429605   14.27048538   11.29746759]\n",
      "   [  12.48667471   11.89207115    8.91905336]\n",
      "   [  18.43271028   16.64889961   13.67588182]\n",
      "   ..., \n",
      "   [  14.86508894   15.4596925    16.64889961]\n",
      "   [  14.27048538   15.4596925    14.86508894]\n",
      "   [  13.67588182   14.86508894   14.27048538]]\n",
      "\n",
      "  [[  13.67588182   13.08127827   11.89207115]\n",
      "   [  16.64889961   16.64889961   15.4596925 ]\n",
      "   [  14.27048538   13.67588182   12.48667471]\n",
      "   ..., \n",
      "   [  16.05429605   16.05429605   17.24350317]\n",
      "   [  14.86508894   14.86508894   14.86508894]\n",
      "   [  13.08127827   13.08127827   13.08127827]]\n",
      "\n",
      "  [[  14.86508894   14.86508894   14.86508894]\n",
      "   [  13.67588182   13.67588182   13.67588182]\n",
      "   [  11.89207115   11.89207115   11.89207115]\n",
      "   ..., \n",
      "   [  12.48667471   12.48667471   13.67588182]\n",
      "   [  14.27048538   13.08127827   13.67588182]\n",
      "   [  16.05429605   14.86508894   15.4596925 ]]]\n",
      "\n",
      "\n",
      " [[[  59.46035575   47.5682846    31.51398855]\n",
      "   [  60.05495931   48.16288816   32.10859211]\n",
      "   [  60.05495931   48.16288816   32.10859211]\n",
      "   ..., \n",
      "   [  39.2438348    29.73017788   22.00033163]\n",
      "   [  40.43304191   30.32478143   24.37874586]\n",
      "   [  36.86542057   26.75716009   20.81112451]]\n",
      "\n",
      "  [[  58.86575219   46.97368104   30.91938499]\n",
      "   [  58.86575219   46.97368104   30.91938499]\n",
      "   [  58.27114864   46.37907749   30.32478143]\n",
      "   ..., \n",
      "   [  33.89240278   24.37874586   16.64889961]\n",
      "   [  35.67621345   25.56795297   19.6219174 ]\n",
      "   [  34.48700634   24.37874586   18.43271028]]\n",
      "\n",
      "  [[  58.86575219   46.97368104   32.10859211]\n",
      "   [  58.86575219   46.97368104   32.10859211]\n",
      "   [  58.27114864   46.37907749   31.51398855]\n",
      "   ..., \n",
      "   [  30.32478143   20.21652096   14.27048538]\n",
      "   [  31.51398855   23.18953874   16.64889961]\n",
      "   [  33.29779922   24.97334942   18.43271028]]\n",
      "\n",
      "  ..., \n",
      "  [[  83.24449805   64.21718421   56.48733796]\n",
      "   [  80.27148026   61.24416642   52.32511306]\n",
      "   [  84.43370517   67.190202     58.86575219]\n",
      "   ..., \n",
      "   [ 100.48800122   85.62291228   67.190202  ]\n",
      "   [  96.32577632   81.46068738   63.0279771 ]\n",
      "   [  95.73117276   80.86608382   62.43337354]]\n",
      "\n",
      "  [[  88.59593007   69.56861623   63.0279771 ]\n",
      "   [  86.21751584   67.190202     59.46035575]\n",
      "   [  80.27148026   62.43337354   55.89273441]\n",
      "   ..., \n",
      "   [  95.73117276   80.86608382   62.43337354]\n",
      "   [  98.70419055   83.83910161   65.40639133]\n",
      "   [  96.32577632   81.46068738   63.0279771 ]]\n",
      "\n",
      "  [[  88.00132651   68.97401267   62.43337354]\n",
      "   [  95.1365692    76.10925536   69.56861623]\n",
      "   [  84.43370517   66.59559844   60.05495931]\n",
      "   ..., \n",
      "   [  96.32577632   81.46068738   63.0279771 ]\n",
      "   [  95.73117276   80.86608382   62.43337354]\n",
      "   [  86.21751584   71.3524269    52.91971662]]]\n",
      "\n",
      "\n",
      " [[[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  72.54163402   80.86608382   80.86608382]\n",
      "   [  73.13623757   81.46068738   81.46068738]\n",
      "   [  72.54163402   82.05529094   81.46068738]]\n",
      "\n",
      "  [[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  74.32544469   82.64989449   82.64989449]\n",
      "   [  73.73084113   82.05529094   82.05529094]\n",
      "   [  71.94703046   81.46068738   80.86608382]]\n",
      "\n",
      "  [[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  74.32544469   82.64989449   83.24449805]\n",
      "   [  72.54163402   80.86608382   80.86608382]\n",
      "   [  70.16321979   79.67687671   79.08227315]]\n",
      "\n",
      "  ..., \n",
      "  [[   0.59460356    2.97301779    2.37841423]\n",
      "   [   0.            2.37841423    1.78381067]\n",
      "   [   0.59460356    1.78381067    1.18920712]\n",
      "   ..., \n",
      "   [  38.64923124   41.02764547   30.91938499]\n",
      "   [  49.35209527   54.10892373   41.62224903]\n",
      "   [  58.86575219   63.62258065   51.13590595]]\n",
      "\n",
      "  [[   3.56762135    7.13524269    7.13524269]\n",
      "   [   3.56762135    7.13524269    7.13524269]\n",
      "   [   2.97301779    6.54063913    6.54063913]\n",
      "   ..., \n",
      "   [  35.08160989   37.46002412   27.35176365]\n",
      "   [  46.37907749   51.13590595   38.64923124]\n",
      "   [  57.08194152   63.0279771    48.75749172]]\n",
      "\n",
      "  [[   7.13524269   12.48667471   11.89207115]\n",
      "   [   6.54063913   11.89207115   11.29746759]\n",
      "   [   6.54063913   11.89207115   11.29746759]\n",
      "   ..., \n",
      "   [  32.10859211   36.86542057   24.37874586]\n",
      "   [  45.18987037   49.94669883   37.46002412]\n",
      "   [  56.48733796   62.43337354   48.16288816]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[  63.62258065   63.62258065   64.81178777]\n",
      "   [  79.08227315   79.08227315   80.27148026]\n",
      "   [  82.05529094   82.05529094   83.24449805]\n",
      "   ..., \n",
      "   [ 100.48800122  100.48800122  100.48800122]\n",
      "   [  99.2987941    99.2987941    99.2987941 ]\n",
      "   [  98.10958699   98.10958699   98.10958699]]\n",
      "\n",
      "  [[  71.94703046   71.94703046   73.13623757]\n",
      "   [  90.9743443    90.9743443    92.16355141]\n",
      "   [  99.2987941    99.2987941   100.48800122]\n",
      "   ..., \n",
      "   [ 118.9207115   118.9207115   118.9207115 ]\n",
      "   [ 117.73150439  117.73150439  117.73150439]\n",
      "   [ 116.54229727  116.54229727  116.54229727]]\n",
      "\n",
      "  [[  76.70385892   76.70385892   77.89306603]\n",
      "   [  98.70419055   98.70419055   99.89339766]\n",
      "   [ 109.40705458  109.40705458  110.5962617 ]\n",
      "   ..., \n",
      "   [ 129.62357554  129.62357554  129.62357554]\n",
      "   [ 129.02897198  129.02897198  129.02897198]\n",
      "   [ 127.24516131  127.24516131  127.24516131]]\n",
      "\n",
      "  ..., \n",
      "  [[   6.54063913    6.54063913    6.54063913]\n",
      "   [  14.86508894   14.86508894   14.86508894]\n",
      "   [  19.02731384   19.02731384   19.02731384]\n",
      "   ..., \n",
      "   [ 151.62390716  151.62390716  151.62390716]\n",
      "   [ 151.62390716  151.62390716  151.62390716]\n",
      "   [ 149.84009649  149.84009649  149.84009649]]\n",
      "\n",
      "  [[   7.13524269    7.13524269    7.13524269]\n",
      "   [  15.4596925    15.4596925    15.4596925 ]\n",
      "   [  18.43271028   18.43271028   18.43271028]\n",
      "   ..., \n",
      "   [ 151.62390716  151.62390716  151.62390716]\n",
      "   [ 151.62390716  151.62390716  151.62390716]\n",
      "   [ 149.84009649  149.84009649  149.84009649]]\n",
      "\n",
      "  [[   5.35143202    5.35143202    5.35143202]\n",
      "   [  14.86508894   14.86508894   14.86508894]\n",
      "   [  18.43271028   18.43271028   18.43271028]\n",
      "   ..., \n",
      "   [ 151.62390716  151.62390716  151.62390716]\n",
      "   [ 151.62390716  151.62390716  151.62390716]\n",
      "   [ 149.84009649  149.84009649  149.84009649]]]\n",
      "\n",
      "\n",
      " [[[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  72.54163402   80.86608382   80.86608382]\n",
      "   [  73.13623757   81.46068738   81.46068738]\n",
      "   [  72.54163402   82.05529094   81.46068738]]\n",
      "\n",
      "  [[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  74.32544469   82.64989449   82.64989449]\n",
      "   [  73.73084113   82.05529094   82.05529094]\n",
      "   [  71.94703046   81.46068738   80.86608382]]\n",
      "\n",
      "  [[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  74.32544469   82.64989449   83.24449805]\n",
      "   [  72.54163402   80.86608382   80.86608382]\n",
      "   [  70.16321979   79.67687671   79.08227315]]\n",
      "\n",
      "  ..., \n",
      "  [[   0.59460356    2.97301779    2.37841423]\n",
      "   [   0.            2.37841423    1.78381067]\n",
      "   [   0.59460356    1.78381067    1.18920712]\n",
      "   ..., \n",
      "   [  38.64923124   41.02764547   30.91938499]\n",
      "   [  49.35209527   54.10892373   41.62224903]\n",
      "   [  58.86575219   63.62258065   51.13590595]]\n",
      "\n",
      "  [[   3.56762135    7.13524269    7.13524269]\n",
      "   [   3.56762135    7.13524269    7.13524269]\n",
      "   [   2.97301779    6.54063913    6.54063913]\n",
      "   ..., \n",
      "   [  35.08160989   37.46002412   27.35176365]\n",
      "   [  46.37907749   51.13590595   38.64923124]\n",
      "   [  57.08194152   63.0279771    48.75749172]]\n",
      "\n",
      "  [[   7.13524269   12.48667471   11.89207115]\n",
      "   [   6.54063913   11.89207115   11.29746759]\n",
      "   [   6.54063913   11.89207115   11.29746759]\n",
      "   ..., \n",
      "   [  32.10859211   36.86542057   24.37874586]\n",
      "   [  45.18987037   49.94669883   37.46002412]\n",
      "   [  56.48733796   62.43337354   48.16288816]]]\n",
      "\n",
      "\n",
      " [[[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  72.54163402   80.86608382   80.86608382]\n",
      "   [  73.13623757   81.46068738   81.46068738]\n",
      "   [  72.54163402   82.05529094   81.46068738]]\n",
      "\n",
      "  [[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  74.32544469   82.64989449   82.64989449]\n",
      "   [  73.73084113   82.05529094   82.05529094]\n",
      "   [  71.94703046   81.46068738   80.86608382]]\n",
      "\n",
      "  [[  30.91938499   24.97334942   23.7841423 ]\n",
      "   [  32.10859211   26.16255653   24.97334942]\n",
      "   [  31.51398855   25.56795297   24.37874586]\n",
      "   ..., \n",
      "   [  74.32544469   82.64989449   83.24449805]\n",
      "   [  72.54163402   80.86608382   80.86608382]\n",
      "   [  70.16321979   79.67687671   79.08227315]]\n",
      "\n",
      "  ..., \n",
      "  [[   0.59460356    2.97301779    2.37841423]\n",
      "   [   0.            2.37841423    1.78381067]\n",
      "   [   0.59460356    1.78381067    1.18920712]\n",
      "   ..., \n",
      "   [  38.64923124   41.02764547   30.91938499]\n",
      "   [  49.35209527   54.10892373   41.62224903]\n",
      "   [  58.86575219   63.62258065   51.13590595]]\n",
      "\n",
      "  [[   3.56762135    7.13524269    7.13524269]\n",
      "   [   3.56762135    7.13524269    7.13524269]\n",
      "   [   2.97301779    6.54063913    6.54063913]\n",
      "   ..., \n",
      "   [  35.08160989   37.46002412   27.35176365]\n",
      "   [  46.37907749   51.13590595   38.64923124]\n",
      "   [  57.08194152   63.0279771    48.75749172]]\n",
      "\n",
      "  [[   7.13524269   12.48667471   11.89207115]\n",
      "   [   6.54063913   11.89207115   11.29746759]\n",
      "   [   6.54063913   11.89207115   11.29746759]\n",
      "   ..., \n",
      "   [  32.10859211   36.86542057   24.37874586]\n",
      "   [  45.18987037   49.94669883   37.46002412]\n",
      "   [  56.48733796   62.43337354   48.16288816]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image \n",
    "path ='/home/ncc/Desktop/image/'\n",
    "\n",
    "def make_dummy_img(path):\n",
    "    paths=os.listdir(path)\n",
    "    ret_img = np.zeros([len(paths) , 256,256,3])\n",
    "    \n",
    "    for i in range(len(paths)):\n",
    "        img=Image.open(path+paths[i])\n",
    "        img=np.array(img)\n",
    "        ret_img[i , :,:,:] = img\n",
    "\n",
    "\n",
    "\n",
    "    return ret_img\n",
    "conv= make_dummy_img(path)\n",
    "print np.shape(conv)\n",
    "print type(conv)\n",
    "radius=2 ;  alpha= 0.0004  ; beta=0.75 ;  bias = 2 \n",
    "lrn_conv=local_response_nomalization(conv , radius , alpha , beta , bias)\n",
    "#print result \n",
    "print conv\n",
    "print lrn_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.]\n",
      " [ 4.  5.  6.]]\n",
      "[ 16.  25.  36.]\n",
      "77.0\n",
      "[[ 0.01298701  0.02597403  0.03896104]\n",
      " [ 0.05194805  0.06493506  0.07792208]]\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3,4,]\n",
    "a=np.ndarray(shape=(2,3))\n",
    "a[0,0]=1;a[0,1]=2;a[0,2]=3 ;a[1,0]=4;a[1,1]=5;a[1,2]=6\n",
    "\n",
    "print a\n",
    "c=(a[1,:3] **2 )\n",
    "print c\n",
    "d=sum(c)\n",
    "print d\n",
    "print a/d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    try:\n",
    "        w_conv1 = tf.get_variable(\"W1\",conv_weight_1 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv1 = tf.get_variable(\"W1\",conv_weight_1 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    try:\n",
    "        b_conv1 = bias_variable([out_ch1])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv1 = bias_variable([out_ch1])\n",
    "                \n",
    "            \n",
    "            \n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    try:\n",
    "        w_conv2 = tf.get_variable(\"W2\",conv_weight_2 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv2 = tf.get_variable(\"W2\",conv_weight_2 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    try:\n",
    "        b_conv2= bias_variable([out_ch2])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv2= bias_variable([out_ch2])\n",
    "                \n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    try:\n",
    "        w_conv3 = tf.get_variable(\"W3\" ,conv_weight_3 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv3 = tf.get_variable(\"W3\" ,conv_weight_3 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    try:\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "        \n",
    "with tf.variable_scope('layer4') as scope:\n",
    "    try:\n",
    "        w_conv4 =tf.get_variable(\"W4\" ,conv_weight_4 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv3 = tf.get_variable(\"W4\" ,conv_weight_4 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer4') as scope:\n",
    "    try:\n",
    "        b_conv4 = bias_variable([out_ch4])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "        \n",
    "with tf.variable_scope('layer5') as scope:\n",
    "    try:\n",
    "        w_conv5 = tf.get_variable(\"W5\",conv_weight_5 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv3 = tf.get_variable(\"W5\" ,conv_weight_5 , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer5') as scope:\n",
    "    try:\n",
    "        b_conv5 = bias_variable([out_ch5])\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv3 = bias_variable([out_ch3])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(30, 28, 28, 20), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_1:0\", shape=(30, 14, 14, 20), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_2:0\", shape=(30, 7, 7, 20), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_3:0\", shape=(30, 7, 7, 20), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_4:0\", shape=(30, 7, 7, 20), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "#conncect hidden layer \n",
    "with tf.device('/gpu:0'):\n",
    "     \n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image , w_conv1 ,conv_strides_1)+b_conv1)\n",
    "    assert len(h_conv1.get_shape()[1:]) ==3 , h_conv1.get_shape()[1:]   \n",
    "    assert out_ch1==h_conv1.get_shape()[-1] , out_ch1   \n",
    "    row_h1 , col_h1 , ch_h1 =h_conv1.get_shape()[1:]\n",
    "    x_h1 = tf.placeholder(tf.float32,shape=[30,row_h1 , col_h1 , ch_h1],  name = 'layer1_lrn')\n",
    "    \n",
    "    #h_conv1_lrn = local_response_nomalization(h_conv1 , radius=2 , alpha=2e-05 , beta = 0.75 , bias = 1.0)\n",
    "    #논리적으로는 h_conv1 뒤에 lrn 이 나오지만 tensorflow 구조상 \n",
    "    #여기에는 보이지 않는다 .  아래 training 구문에서  에서는 실재로 lrn 코드가 여기 들어간다 \n",
    "    h_conv1_max = max_pool(x_h1)#pooling\n",
    " \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1_max , w_conv2 ,conv_strides_2)+b_conv2)\n",
    "    assert len(h_conv2.get_shape()[1:])==3  ,h_conv2.get_shape()[1:] \n",
    "    assert h_conv2.get_shape()[-1] == out_ch2 ,out_ch2\n",
    "    row_h2 , col_h2 , ch_h2 =h_conv2.get_shape()[1:]\n",
    "    \n",
    "    #h_conv2_lrn = local_response_nomalization(h_conv2 , radius=2 , alpha=2e-05 , beta = 0.75 , bias = 1.0)    \n",
    "    x_h2 = tf.placeholder( tf.float32 ,shape=[30,row_h2 , col_h2 , ch_h2],  name = 'layer2_lrn')\n",
    "    h_conv2_max = max_pool(x_h2 , pool_weight_2 , pool_strides_2)#pooling\n",
    "\n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2_max , w_conv3,conv_strides_3)+b_conv3)\n",
    "    \n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4,conv_strides_4)+b_conv4)\n",
    "#x_h5 = tf.placeholder(\"float\",shape=[None,img_col , img_row , 3],  name = 'layer3_lrn')\n",
    "    \n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, w_conv5,conv_strides_5)+b_conv5)\n",
    "    #h_conv5_lrn = local_response_nomalization(h_conv5 , radius=2 , alpha=2e-05 , beta = 0.75 , bias = 1.0)\n",
    "    row_h5 , col_h5 , ch_h5 =h_conv5.get_shape()[1:] \n",
    "    \n",
    "    x_h5 = tf.placeholder(tf.float32 ,shape=[30,row_h5 , col_h5 , ch_h5],  name = 'layer5_lrn')\n",
    "    assert len(h_conv5.get_shape()[1:])==3  ,h_conv5.get_shape()[1:] \n",
    "    assert h_conv5.get_shape()[-1] == out_ch5 , h_conv5.get_shape[-1]  \n",
    "    h_conv5_max = max_pool(x_h5)\n",
    "#\n",
    "print h_conv1\n",
    "print h_conv2\n",
    "print h_conv3\n",
    "print h_conv4\n",
    "print h_conv5\n",
    "\n",
    "    \n",
    "end_conv = h_conv5_max\n",
    "    #print conv2d(h_pool1 , w_conv2).get_shape()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "  \n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1 , w_conv2 ,conv_strides_2)+b_conv2)\n",
    "    #h_conv2 = local_response_nomalizatino(h_conv2 , radius=2 , alpha=2e-05 , beta = 0.75 , bias = 1.0)    \n",
    "    h_conv2 = max_pool( h_conv2 , pool_weight_2 , pool_strides_2)#pooling\n",
    "    \n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2 , w_conv3,conv_strides_3)+b_conv3)\n",
    "    \n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4,conv_strides_4)+b_conv4)\n",
    "    \n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, w_conv5,conv_strides_5)+b_conv5)\n",
    "    h_conv5 = local_response_nomalizatino(h_conv5 , radius=2 , alpha=2e-05 , beta = 0.75 , bias = 1.0)\n",
    "    h_conv5= max_pool( h_conv5,pool_weight_5 , pool_stries_5) #pooling \n",
    "#\n",
    "print h_conv1\n",
    "print h_conv2\n",
    "print h_conv3\n",
    "print h_conv4\n",
    "print h_conv5\n",
    "\n",
    "    \n",
    "end_conv = h_conv5\n",
    "    #print conv2d(h_pool1 , w_conv2).get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 1, 20)\n"
     ]
    }
   ],
   "source": [
    "print w_conv1.get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 20\n"
     ]
    }
   ],
   "source": [
    "end_conv_row=int(h_conv5_max.get_shape()[1])\n",
    "end_conv_col=int(h_conv5_max.get_shape()[2])\n",
    "end_conv_ch=int(h_conv5_max.get_shape()[3])\n",
    "\n",
    "print end_conv_row , end_conv_col , end_conv_ch\n",
    "#connect fully connected layer \n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope(\"fc1\") as scope:\n",
    "        try:\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc1 = bias_variable([fully_ch1])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc1 = bias_variable([fully_ch1])\n",
    "\n",
    "        \n",
    "with tf.device('/gpu:0'): # flat conv layer \n",
    "    end_flat_conv =tf.reshape(end_conv, [-1,end_conv_col*end_conv_row*end_conv_ch])\n",
    "   \n",
    "with tf.device('/gpu:0'): # connect flat layer with fully  connnected layer \n",
    "    h_fc1 = tf.nn.relu(tf.matmul(end_flat_conv , w_fc1)+ b_fc1)\n",
    "    h_fc1 = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        try:\n",
    "            w_fc2 =tf.get_variable(\"fc2_W\",[fully_ch1 , fully_ch2],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc2 =tf.get_variable(\"fc2_W\",[fully_ch1 , fully_ch2],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc2 = bias_variable([fully_ch2])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc2 = bias_variable([fully_ch2])\n",
    "\n",
    "with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1 , w_fc2)+b_fc2)\n",
    "    h_fc2= tf.nn.dropout(h_fc2 , keep_prob)\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_fc=h_fc2\n",
    "end_ch=fully_ch2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('end') as scope:\n",
    "        try:\n",
    "            w_end =tf.get_variable(\"end_W\",[end_ch, n_classes ],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_end =tf.get_variable(\"end_W\",[end_ch , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_end = bias_variable([n_classes])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_end = bias_variable([n_classes])\n",
    "\n",
    "with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "    y_conv = tf.nn.relu(tf.matmul(end_fc , w_end)+b_end)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is recorded at :2\n"
     ]
    }
   ],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "\n",
    "dirname='/home/ncc/Desktop/result/'\n",
    "\n",
    "count=0\n",
    "while(True):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    elif not os.path.isdir(dirname + str(count)):\n",
    "        dirname=dirname+str(count)\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "print 'it is recorded at :'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(dirname+\"/log.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , validation  accuracy 0.10743\n",
      "step 0 , validation loss : 2.42361\n",
      "step 100 , validation  accuracy 0.0955823\n",
      "step 100 , validation loss : 2.30259\n",
      "step 200 , validation  accuracy 0.0955823\n",
      "step 200 , validation loss : 2.30259\n",
      "step 300 , validation  accuracy 0.0955823\n",
      "step 300 , validation loss : 2.30259\n",
      "step 400 , validation  accuracy 0.0955823\n",
      "step 400 , validation loss : 2.30259\n",
      "step 500 , validation  accuracy 0.0955823\n",
      "step 500 , validation loss : 2.30259\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-60bc4a38c7df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;31m#print h_conv1.get_shape()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;31m#print np.shape(np_h_conv1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mnp_h_conv1_lrn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal_response_nomalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_h_conv1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mradius\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mnp_h_conv1_lrn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp_h_conv1_lrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f0cb147f927b>\u001b[0m in \u001b[0;36mlocal_response_nomalization\u001b[0;34m(conv, radius, alpha, beta, bias, batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_range\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                         \u001b[0mnormalized_constant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_range\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mmax_range\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                         \u001b[0mlrn_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnormalized_constant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[0mlrn_conv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlrn_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "\n",
    "    regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "with tf.device('/gpu:0'):\n",
    "    cost = cost+regular\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost) #1e-4\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for i in range(iterate):\n",
    "    \n",
    "    batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "   # batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "    if i%100 ==0: # in here add to validation \n",
    "        try:\n",
    "            np_h_conv1=sess.run(h_conv1 , feed_dict={x: val_img[:10], y_ : val_lab[:10] , keep_prob : 0.5})\n",
    "            np_h_conv1_lrn = local_response_nomalization(np_h_conv1 , radius , alpha , beta , bias , 'layer1' , batch_size)\n",
    "            np_h_conv1_lrn=np_h_conv1_lrn.astype(np.float32)\n",
    "\n",
    "            np_h_conv2=sess.run(h_conv2, feed_dict = {x_h1:np_h_conv1_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "            np_h_conv2_lrn = local_response_nomalization(np_h_conv2 , radius , alpha , beta , bias , 'layer2' , batch_size)\n",
    "            np_h_conv2_lrn=np_h_conv2_lrn.astype(np.float32)\n",
    "\n",
    "            np_h_conv5=sess.run(h_conv5, feed_dict = {x_h2:np_h_conv1_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "            np_h_conv5_lrn = local_response_nomalization(np_h_conv5 , radius , alpha , beta , bias , 'layer5' , batch_size)\n",
    "            np_h_conv5_lrn=np_h_conv5_lrn.astype(np.float32) \n",
    "\n",
    "            val_accuracy = sess.run( accuracy , feed_dict={x_h5:np_h_conv5_lrn , y_:val_lab , keep_prob: 1.0})        \n",
    "            val_loss = sess.run(cost , feed_dict = {x_h5:np_h_conv5_lrn , y_: val_lab , keep_prob: 1.0})\n",
    "            \"\"\"\n",
    "            np_h_conv1=sess.run(h_conv1 , feed_dict={x: batch_xs, y_ : keep_prob , keep_prob : 0.5})\n",
    "            np_h_conv1_lrn = local_response_nomalization(np_h_conv1 , radius , alpha , beta , bias , 'layer1' , batch_size)\n",
    "            np_h_conv1_lrn=np_h_conv1_lrn.astype(float32)\n",
    "\n",
    "            np_h_conv2=sess.run(h_conv2, feed_dict = {x2:np_h_conv1_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "            np_h_conv2_lrn = local_response_nomalization(np_h_conv2 , radius , alpha , beta , bias , 'layer2' , batch_size)\n",
    "            np_h_conv2_lrn=np_h_conv2_lrn.astype(np.float32)\n",
    "\n",
    "            np_h_conv5=sess.run(h_conv5, feed_dict = {x5:np_h_conv1_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "            np_h_conv5_lrn = local_response_nomalization(np_h_conv5 , radius , alpha , beta , bias , 'layer5' , batch_size)\n",
    "            np_h_conv5_lrn=np_h_conv5_lrn.astype(np.float32) \n",
    "\n",
    "            \n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:np_h_conv5_lrn , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:np_h_conv5_lrn, y_: batch_ys, keep_prob: 1.0})\n",
    "            \n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "            \"\"\"\n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "            \n",
    "            \n",
    "            f.write(val_str)\n",
    "            #f.write(train_str)\n",
    "        except :\n",
    "            list_acc=[]\n",
    "            list_loss=[]\n",
    "            n_divide=len(val_img)/batch_size\n",
    "            for j in range(n_divide):\n",
    "            \n",
    "            \n",
    "            #x:val_img[ j*batch_size :(j+1)*batch_size] , y_:val_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0\n",
    "                # j*batch_size :(j+1)*batch_size\n",
    "             \n",
    "                np_h_conv1=sess.run(h_conv1 , feed_dict={x: val_img[j*batch_size :(j+1)*batch_size], y_ : val_lab[j*batch_size :(j+1)*batch_size] , keep_prob : 0.5})\n",
    "                #print h_conv1.get_shape()\n",
    "                #print np.shape(np_h_conv1)\n",
    "                np_h_conv1_lrn = local_response_nomalization(np_h_conv1 , radius , alpha , beta , bias  , batch_size)\n",
    "                np_h_conv1_lrn=np_h_conv1_lrn.astype(np.float32)\n",
    "                \n",
    "                #print np.shape(np_h_conv1_lrn)\n",
    "                \n",
    "                np_h_conv2=sess.run(h_conv2, feed_dict = {x_h1:np_h_conv1_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "                np_h_conv2_lrn = local_response_nomalization(np_h_conv2 , radius , alpha , beta , bias  , batch_size)\n",
    "                np_h_conv2_lrn=np_h_conv2_lrn.astype(np.float32)\n",
    "\n",
    "                np_h_conv5=sess.run(h_conv5, feed_dict = {x_h2:np_h_conv2_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "                np_h_conv5_lrn = local_response_nomalization(np_h_conv5 , radius , alpha , beta , bias  , batch_size)\n",
    "                np_h_conv5_lrn=np_h_conv5_lrn.astype(float) \n",
    "                #print np.shape(np_h_conv5_lrn)\n",
    "\n",
    "                val_accuracy = sess.run( accuracy , feed_dict={x_h5:np_h_conv5_lrn ,y_ : val_lab[j*batch_size :(j+1)*batch_size] , keep_prob: 1.0})        \n",
    "                val_loss = sess.run(cost , feed_dict = {x_h5:np_h_conv5_lrn , y_: val_lab[j*batch_size :(j+1)*batch_size], keep_prob: 1.0 })\n",
    "                list_acc.append(float(val_accuracy))\n",
    "                list_loss.append(float(val_loss))\n",
    "\n",
    "            list_acc=np.asarray(list_acc)\n",
    "            list_loss= np.asarray(list_loss)\n",
    "            \n",
    "            val_accuracy=np.mean(list_acc)\n",
    "            val_loss = np.mean(list_loss)\n",
    "            \n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            \"\"\"\n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "            \"\"\"\n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "           \n",
    "            \n",
    "            f.write(val_str)\n",
    "            #f.write(train_str)\n",
    "    \n",
    "    #training\n",
    "    np_h_conv1=sess.run(h_conv1 , feed_dict={x: batch_xs, y_ : batch_ys , keep_prob : 0.5})\n",
    "    np_h_conv1_lrn = local_response_nomalization(np_h_conv1 , radius , alpha , beta , bias  , batch_size)\n",
    "    np_h_conv1_lrn=np_h_conv1_lrn.astype(np.float32)\n",
    "    \n",
    "    np_h_conv2=sess.run(h_conv2, feed_dict = {x_h1:np_h_conv1_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "    np_h_conv2_lrn = local_response_nomalization(np_h_conv2 , radius , alpha , beta , bias  , batch_size)\n",
    "    np_h_conv2_lrn=np_h_conv2_lrn.astype(np.float32)\n",
    "    \n",
    "    np_h_conv5=sess.run(h_conv5, feed_dict = {x_h2:np_h_conv2_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "    np_h_conv5_lrn = local_response_nomalization(np_h_conv5 , radius , alpha , beta , bias  , batch_size)\n",
    "    np_h_conv5_lrn=np_h_conv5_lrn.astype(np.float32) \n",
    "    \n",
    "    sess.run(train_step, feed_dict = {x_h5:np_h_conv5_lrn, y_:batch_ys , keep_prob : 0.5}  )\n",
    "        \n",
    "    \n",
    "    #aug_and_train(batch_x , batch_y , crop_row , crop_col ,keep_prob =0.5 , sess , reflect = 'horizon' ):\n",
    "print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "f.write(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def local_response_nomalization(conv , radius , alpha , beta , bias ,layer_name , batch_size =30 , ):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_accuracy = sess.run( accuracy , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})        \n",
    "    test_loss = sess.run(cost , feed_dict = {x:test_img , y_: test_lab , keep_prob: 1.0})\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "except :\n",
    "    list_acc=[]\n",
    "    list_loss=[]\n",
    "    n_divide=len(test_img)/batch_size\n",
    "    for j in range(n_divide):\n",
    "\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "        list_acc.append(float(test_accuracy))\n",
    "        list_loss.append(float(test_loss))\n",
    "    test_accuracy , test_loss=sess.run([accuracy,cost] , feed_dict={x:test_img[(j+1)*batch_size : ] , y_:test_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "    #right above code have to modify\n",
    "\n",
    "    list_acc.append(test_accuracy)\n",
    "    list_loss.append(test_loss)\n",
    "    list_acc=np.asarray(list_acc)\n",
    "    list_loss= np.asarray(list_loss)\n",
    "\n",
    "    test_accuracy=np.mean(list_acc)\n",
    "    test_loss = np.mean(list_loss)\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
