{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_TVT(divide_flag,file_locate):\n",
    "    if divide_flag == False:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data Image\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"Test Data Image\",np.shape(test_img)\n",
    "        print \"val Data Label\" , np.shape(val_lab)\n",
    "        print \"val Data Image\" , np.shape(val_img)\n",
    "        \n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n",
    "        return train_img ,train_lab,val_img,val_lab,test_img,test_lab\n",
    "   \n",
    "    if divide_flag == True:\n",
    "        print '트레이닝 파일이 여러개로 분할되어 있습니다. 분할된 트레이닝 파일에 대한 조치가 필요합니다'\n",
    "        train_images, train_labels =get_batch_list(file_locate)\n",
    "        print train_images ,train_labels\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "        print \"the number of training image batch\",len(train_images)\n",
    "        print \"the number of training label batch\",len(train_labels)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"Test Data Image\",np.shape(test_img)\n",
    "        print \"val Data Label\" , np.shape(val_lab)\n",
    "        print \"val Data Image\" , np.shape(val_img)\n",
    "        return train_images, train_labels,val_img,val_lab,test_img,test_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug_crop(np_img ,img_row ,img_col , color_ch, crop_img_row , crop_img_col , label):\n",
    "    n_img = np.shape(np_img)[0] \n",
    "    n_ret_img = n_img*(img_row - crop_img_row) * (img_col - crop_img_col)*2\n",
    "    ret_images = np.zeros([n_ret_img ,crop_img_row , crop_img_col,color_ch])\n",
    "    len_label= np.shape(label)[1]\n",
    "    ret_labels =  np.zeros([n_ret_img  ,len_label])\n",
    "    #print \"n_augmented image size : \" , n_ret_img \n",
    "    #print \"n classes :\", len_label\n",
    "    #copy label in factor by 2014\n",
    "\n",
    "    if len(np.shape(np_img))==2:\n",
    "        np_img=np.reshape(np_img , newshape = [np.shape(np_img)[0] , img_row , img_col ,color_ch])\n",
    "        print np.shape(np_img)\n",
    "    for n  in range(len(np_img)):\n",
    "        ret_labels[n*2 , : ] = label[n,:]\n",
    "        ret_labels[n*2+1 , : ] = label[n,:]\n",
    "        for r in range(img_row - crop_img_row):\n",
    "            for c in range(img_col - crop_img_col):\n",
    "\n",
    "                cropped_img = np_img[n, r:crop_img_row +r , c:crop_img_col+c ,: ]\n",
    "\n",
    "                ret_images[n*2,:,:,:]=cropped_img  \n",
    "                ret_images[(n*2+1) , :,:,:] =np.fliplr(cropped_img )\n",
    "\n",
    "\n",
    "    return ret_images ,ret_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug_8_times(x):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    x shape is [n_batch , row ,col , color_ch ]\n",
    "    x type is numpy \n",
    "    this code need too many time to run \n",
    "    we should find solution using parallel method to less run time maybe \n",
    "    \n",
    "    return x,np_rot90,np_rot180,np_rot270,lr_x,np_lr_rot90 ,np_lr_rot180 , np_lr_rot270 \n",
    "    \n",
    "    \"\"\"\n",
    "    start_time =time.time()\n",
    "    n_batch,row,col,ch=np.shape(x)\n",
    "    lr_x = np.flipud(x)\n",
    "\n",
    "    np_rot90 =np.zeros(shape =[n_batch , row ,col ,ch] )\n",
    "    np_rot180=np.zeros(shape =[n_batch , row ,col ,ch] )\n",
    "    np_rot270=np.zeros(shape =[n_batch , row ,col ,ch] )\n",
    "\n",
    "    np_lr_rot90 =np.zeros(shape =[n_batch , row ,col ,ch] )\n",
    "    np_lr_rot180=np.zeros(shape =[n_batch , row ,col ,ch] )\n",
    "    np_lr_rot270=np.zeros(shape =[n_batch , row ,col ,ch] )\n",
    "    \n",
    "    \n",
    "\n",
    "    for batch_ind in range(n_batch):\n",
    "\n",
    "        rot90=np.rot90(x[batch_ind,:,:,:])\n",
    "        rot180=np.rot90(rot90)\n",
    "        rot270=np.rot90(rot180)\n",
    "\n",
    "        np_rot90[batch_ind,:,:,:] = rot90\n",
    "        np_rot180[batch_ind,:,:,:]=rot180\n",
    "        np_rot270[batch_ind,:,:,:]=rot270\n",
    "\n",
    "        lr_rot90=np.rot90(lr_x[batch_ind,:,:,:])\n",
    "        lr_rot180=np.rot90(lr_rot90)\n",
    "        lr_rot270=np.rot90(lr_rot180)\n",
    "\n",
    "        np_lr_rot90[batch_ind,:,:,:]=lr_rot90\n",
    "        np_lr_rot180[batch_ind,:,:,:]=lr_rot180\n",
    "        np_lr_rot90[batch_ind,:,:,:]=lr_rot270\n",
    "    end_time =time.time()\n",
    "    print end_time - start_time\n",
    "    return x,np_rot90,np_rot180,np_rot270,lr_x,np_lr_rot90 ,np_lr_rot180 , np_lr_rot270 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def training(batch_xs , batch_ys):\n",
    "    sess.run(train_step ,feed_dict={x: batch_xs, y_:batch_ys , keep_prob : 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "def make_logdir(dirname):\n",
    "  \n",
    "\n",
    "    count=0\n",
    "    while(True):\n",
    "        if not os.path.isdir(dirname):\n",
    "            os.mkdir(dirname)\n",
    "            break\n",
    "        elif not os.path.isdir(dirname + str(count)):\n",
    "            dirname=dirname+str(count)\n",
    "            os.mkdir(dirname)\n",
    "            break\n",
    "        else:\n",
    "            count+=1\n",
    "    print 'it is recorded at :'+str(count)\n",
    "\n",
    "    f=open(dirname+\"/log.txt\",'w')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_log(step,train_acc, train_loss , val_acc , val_loss ,fp):\n",
    "    \"\"\"\n",
    "    fp = File Pointer\n",
    "    \n",
    "    \"\"\"\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print step\n",
    "    print(\"step %d , training  accuracy %g\" %(step,train_acc))\n",
    "    print(\"step %d , loss : %g\" %(step,train_loss))\n",
    "    train_str = 'step:\\t'+str(step)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_acc)+'\\n'\n",
    "\n",
    "    print(\"step %d , validation  accuracy %g\" %(step,val_acc))\n",
    "    print(\"step %d , validation loss : %g\" %(step,val_loss))\n",
    "    val_str = 'step:\\t'+str(step)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_acc)+'\\n'\n",
    "\n",
    "    fp.write(train_str)\n",
    "    fp.write(val_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_from_images(sess, images , labels , accuracy , cost ):\n",
    "    \"\"\"\n",
    "    input : x-\n",
    "    \n",
    "    x type  : numpy \n",
    "    x shape : [n , row , col , ch]\n",
    "    return  acc,  loss\n",
    "\n",
    "    \"\"\"\n",
    "    print accuracy\n",
    "    acc_list=[]\n",
    "    loss_list=[]\n",
    "    images_labels=zip(images,labels)\n",
    "    for img_ind ,(img,lab)  in enumerate(images_labels):\n",
    "\n",
    "        acc ,loss = sess.run( [accuracy,cost] ,feed_dict={x:img , y_: lab , keep_prob: 1.0})        \n",
    "        acc_list.append(acc)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "    acc_list=np.asarray(acc_list)\n",
    "    loss_list=np.asarray(loss_list)\n",
    "    acc=np.mean(acc_list)\n",
    "    loss=np.mean(loss_list)\n",
    "    \n",
    "    return  acc,  loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(img , lab ,accuracy ,cost ): #default\n",
    "    \"\"\"\n",
    "    return val_acc,  val_loss, train_acc, train_loss\n",
    "    \"\"\"    \n",
    "    acc,loss = sess.run([accuracy,cost] , feed_dict={x:img , y_:lab , keep_prob: 1.0})        \n",
    "    return acc,loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def divide_images(images , labels,batch_size):\n",
    "    \"\"\"\n",
    "    return list_images,list_labels\n",
    "    \"\"\"\n",
    "    n_divide=len(images)/batch_size\n",
    "\n",
    "    list_images=[]\n",
    "    list_labels=[]\n",
    "    for ind in range(n_divide):\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        image =images[ ind*batch_size :(ind+1)*batch_size] \n",
    "        label =labels[ ind*batch_size :(ind+1)*batch_size]\n",
    "        list_images.append(image)\n",
    "        list_labels.append(label)\n",
    "\n",
    "    #right above code have to modify\n",
    "    image = images[ (ind+1)*batch_size :  ] \n",
    "    label = labels[ (ind+1)*batch_size :  ]\n",
    "    list_images.append(image)\n",
    "    list_labels.append(label)\n",
    "\n",
    "    return list_images,list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "def validate_extract_imgs(val_img , val_lab , train_img , train_lab ):\n",
    "    \"\"\"\n",
    "    extract patch from ori-image\n",
    "    \"\"\"\n",
    "    color_ch = in_ch\n",
    "    val_images  =extract_test_img(val_img , 128 , 128 , color_ch   ,crop_img_row =118 , crop_img_col =118 )\n",
    "    train_images=extract_test_img(train_img ,128 , 128 , color_ch   ,crop_img_row =118 , crop_img_col =118 )\n",
    "    val_acc, val_loss =validate_from_images(sess, val_images , val_lab , accuracy ,cost )\n",
    "    train_acc , train_loss =validate_from_images(sess, train_images , train_lab ,accuracy , cost)\n",
    "    \n",
    "    return val_acc , val_loss, train_acc ,train_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BATCH_TRAINING_MANUAL(maxiter  , batch_size,file_locate, cost , train_step ,correct_prediction , \\\n",
    "                   accuracy , fp ):\n",
    "    \"\"\"\n",
    "    BATCH : 이 함수는 이미지를 한장씩 처리하는게 아닌 여러장을 묶어(batch) 처리 할 수 있게 합니다\n",
    "    MANUAL:이 함수는 Training batch파일을 순서대로 training 합니다.\n",
    "    AUG: 이 함수는 8배짜리 augmentation을 하고 트레이닝을 합니다.  \n",
    "    \n",
    "    \n",
    "    val_acc, val_loss =validate_from_images(val_imgs ,val_labs ,accuracy, cost)\n",
    "    여기 부분 검증해야 한다.            \n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "#with tf.device('/gpu:0'):\n",
    "    train_img, train_lab,val_img,val_lab,test_img,test_lab\\\n",
    "    =load_TVT(divide_flag,file_locate)\n",
    "    train_images , train_labels=divide_images(train_img , train_lab , batch_size)\n",
    "    train_images_labels=zip(train_images , train_labels)\n",
    "    \n",
    "    for step,(batch_xs , batch_ys) in train_images_labels:\n",
    "        if step%100 ==0:\n",
    "            try:\n",
    "                val_acc , val_loss=validate(val_img , val_lab)\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys)\n",
    "            except:\n",
    "                #한번에 트레이닝이 안되면 분할해서 validation한다.\n",
    "                val_imgs, val_labs = divide_images(val_img , val_lab, batch_size) #\n",
    "                val_acc, val_loss =validate_from_images(val_imgs ,val_labs ,accuracy, cost)\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys,accuracy, cost)\n",
    "            write_log(step , train_acc, train_loss , val_acc, val_loss , fp)\n",
    "            #np.save('./result/batch_xs' ,batch_xs)\n",
    "            #np.save('./result/batch_ys' ,batch_ys)\n",
    "        else:\n",
    "            training(batch_xs ,batch_ys)                    \n",
    "\n",
    "\n",
    "############################################aug_and_training############################\n",
    "    print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "    train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "    fp.write(train_time)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BATCH_TRAINING_MANUAL_AUG8(maxiter, batch_size,file_locate, tensor_info, fp ):\n",
    " \"\"\"\n",
    "  BATCH : 이 함수는 이미지를 한장씩 처리하는게 아닌 여러장을 묶어(batch) 처리 할 수 있게 합니다\n",
    "  MANUAL:이 함수는 Training batch파일을 순서대로 training 합니다.\n",
    "  AUG: 이 함수는 8배짜리 augmentation을 하고 트레이닝을 합니다.  \n",
    "  \n",
    "  to do: validation을 마지막에 학습시키는 코드를 추가해야 합니다.\n",
    "  \n",
    "  \n",
    " \"\"\"\n",
    "    cost,train_step,correct_prediction , accuracy , softmax = tensor_info\n",
    "    start_time = time.time()\n",
    "#with tf.device('/gpu:0'):\n",
    "    train_img, train_lab,val_img,val_lab,test_img,test_lab\\\n",
    "    =load_TVT(False,file_locate) #TVT is Train Validation Test\n",
    "    train_imgs, train_labs = divide_images(train_img , train_val, batch_size)\n",
    "    for step in range(len(train_imgs)*maxiter)\n",
    "        \n",
    "        if step > len(train_imgs):\n",
    "            ind =0\n",
    "        batch_xs , batch_ys = train_imgs[ind] , train_labs[ind]\n",
    "        ind+=1\n",
    "        \n",
    "        list_aug_x=aug_8_times(batch_xs)\n",
    "        for ele in list_aug_x:\n",
    "            training(ele,batch_ys)\n",
    "        \n",
    "        if step%100 ==0: #여기 if loop에서 validate을 합니다 \n",
    "            try:\n",
    "                val_acc , val_loss=validate(val_img , val_lab)\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys)\n",
    "            except:\n",
    "                #한번에 트레이닝이 안되면 분할해서 validation한다.\n",
    "                val_imgs, val_labs = divide_images(val_img , val_lab, batch_size)\n",
    "                print np.shape(val_imgs[0])\n",
    "                val_acc, val_loss =validate_from_images(sess,val_imgs ,val_labs ,accuracy ,cost )\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys,accuracy ,cost)\n",
    "\n",
    "            write_log(step , train_acc, train_loss , val_acc, val_loss , fp)\n",
    "            softmax_=sess.run(softmax ,feed_dict ={ x:batch_xs , y_:batch_ys , keep_prob:1.0 })\n",
    "            print softmax_\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "############################################aug_and_training############################\n",
    "    print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "    train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "    fp.write(train_time)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BATCH_TRAINING_RANDOM_AUG8(maxiter, batch_size,file_locate, cost , train_step ,correct_prediction , \\\n",
    "                   accuracy ,softmax, fp ):\n",
    " \"\"\"\n",
    " aug( O )\n",
    " \n",
    " \n",
    " \"\"\" \n",
    "    cost,train_step,correct_prediction , accuracy , softmax = tensor_info\n",
    "    start_time = time.time()\n",
    "#with tf.device('/gpu:0'):\n",
    "    train_img, train_lab,val_img,val_lab,test_img,test_lab\\\n",
    "    =load_TVT(False,file_locate) #TVT is Train Validation Test  \n",
    "    for step in range(maxiter):    \n",
    "        batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)       \n",
    "        if step%100 ==0: #여기 if loop에서 validate을 합니다 \n",
    "            try:\n",
    "                val_acc , val_loss=validate(val_img , val_lab)\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys)\n",
    "            except:\n",
    "                #한번에 트레이닝이 안되면 분할해서 validation한다.\n",
    "                val_imgs, val_labs = divide_images(val_img , val_lab, batch_size)\n",
    "                print np.shape(val_imgs[0])\n",
    "                val_acc, val_loss =validate_from_images(sess,val_imgs ,val_labs ,accuracy ,cost )\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys,accuracy ,cost)\n",
    "\n",
    "            write_log(step , train_acc, train_loss , val_acc, val_loss , fp)\n",
    "            softmax_=sess.run(softmax ,feed_dict ={ x:batch_xs , y_:batch_ys , keep_prob:1.0 })\n",
    "            print softmax_\n",
    "            np.save('./result/batch_xs' ,batch_xs)\n",
    "            np.save('./result/batch_ys' ,batch_ys)\n",
    "        list_aug_x=aug_8_times(batch_xs)\n",
    "        for ele in list_aug_x:\n",
    "            training(ele,batch_ys)\n",
    "\n",
    "############################################aug_and_training############################\n",
    "    print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "    train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "    fp.write(train_time)\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BATCH_TRAINING_RANDOM(maxiter, batch_size,file_locate, cost , train_step ,correct_prediction , \\\n",
    "                   accuracy ,softmax, fp ):\n",
    " \"\"\"\n",
    " aug( O )\n",
    " \n",
    " \n",
    " \"\"\"\n",
    "    cost,train_step,correct_prediction , accuracy , softmax = tensor_info\n",
    "    start_time = time.time()\n",
    "#with tf.device('/gpu:0'):\n",
    "    train_img, train_lab,val_img,val_lab,test_img,test_lab\\\n",
    "    =load_TVT(False,file_locate) #TVT is Train Validation Test  \n",
    "    for step in range(maxiter):    \n",
    "        batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)       \n",
    "        if step%100 ==0: #여기 if loop에서 validate을 합니다 \n",
    "            try:\n",
    "                val_acc , val_loss=validate(val_img , val_lab)\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys)\n",
    "            except:\n",
    "                #한번에 트레이닝이 안되면 분할해서 validation한다.\n",
    "                val_imgs, val_labs = divide_images(val_img , val_lab, batch_size)\n",
    "                print np.shape(val_imgs[0])\n",
    "                val_acc, val_loss =validate_from_images(sess,val_imgs ,val_labs ,accuracy ,cost )\n",
    "                train_acc , train_loss=validate(batch_xs,batch_ys,accuracy ,cost)\n",
    "\n",
    "            write_log(step , train_acc, train_loss , val_acc, val_loss , fp)\n",
    "            softmax_=sess.run(softmax ,feed_dict ={ x:batch_xs , y_:batch_ys , keep_prob:1.0 })\n",
    "            print softmax_\n",
    "        else\n",
    "            training(batch_xs,batch_ys)\n",
    "\n",
    "############################################aug_and_training############################\n",
    "    print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "    train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "    fp.write(train_time)\n",
    "    TEST(test_img, test_lab , tensor_info, batch_size ,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def TEST( test_img, test_lab , tensor_info, batch_size ,fp):\n",
    "    cost,train_step,correct_prediction , accuracy , softmax = tensor_info\n",
    "    test_imgs, test_labs=divide_images(test_img, test_lab , batch_size )\n",
    "    test_acc , test_loss=validate_from_images(sess,test_imgs ,test_labs ,accuracy ,cost)\n",
    "    train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "    \n",
    "    \n",
    "    print(\"test  accuracy %g\" %(test_acc))\n",
    "    print(\"test loss : %g\" %(test_loss))\n",
    "    test_str = '\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_acc)+'\\n'\n",
    "    fp.write(test_str)\n",
    "    fp.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_log(step,train_acc, train_loss , val_acc , val_loss ,fp):\n",
    "    \"\"\"\n",
    "    fp = File Pointer\n",
    "    \n",
    "    \"\"\"\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print step\n",
    "    print(\"step %d , training  accuracy %g\" %(step,train_acc))\n",
    "    print(\"step %d , loss : %g\" %(step,train_loss))\n",
    "    train_str = 'step:\\t'+str(step)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_acc)+'\\n'\n",
    "\n",
    "    print(\"step %d , validation  accuracy %g\" %(step,val_acc))\n",
    "    print(\"step %d , validation loss : %g\" %(step,val_loss))\n",
    "    val_str = 'step:\\t'+str(step)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_acc)+'\\n'\n",
    "\n",
    "    fp.write(train_str)\n",
    "    fp.write(val_str)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
