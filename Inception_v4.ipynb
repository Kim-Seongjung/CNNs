{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\"\"\"\n",
    "created by kim Seong jung\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2789, 128, 128, 3)\n",
      "(2789, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_locate='./eye_numpy_64/'\n",
    "#file_locate ='/media/seongjung/Seagate Backup Plus Drive/data/Eye/npy/npy_128/'\n",
    "sess = tf.InteractiveSession()\n",
    "test_img=np.load(file_locate+'test_img.npy');\n",
    "test_lab=np.load(file_locate+'test_lab.npy');\n",
    "try:\n",
    "    print np.shape(test_img)\n",
    "    print np.shape(test_lab)\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "except:\n",
    "    np.shape(test_img)\n",
    "    test_img=np.reshape(test_img , newshape = [np.shape(test_img)[0] , 32, 32 ,3] )\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "\n",
    "    \n",
    "divide_flag= True\n",
    "aug_flag = True\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "train_img=test_img[0:15]\n",
    "train_lab=test_lab[0:15]\n",
    "val_img =test_img[15:30]\n",
    "val_lab =test_lab[15:30]\n",
    "test_img = test_img[30:45]\n",
    "test_lab =test_lab[30:45]\n",
    "path='/home/seongjung/바탕화면/sample/'\n",
    "\n",
    "np.save(path+'train_img',train_img)\n",
    "np.save(path+'train_lab',train_lab)\n",
    "np.save(path+'val_img',val_img)\n",
    "np.save(path+'val_lab',val_lab)\n",
    "np.save(path+'test_img',test_img)\n",
    "np.save(path+'test_lab',test_lab)\n",
    "\n",
    "print np.shape(train_img)\n",
    "print np.shape(train_lab)\n",
    "print np.shape(val_img)\n",
    "print np.shape(val_lab)\n",
    "print np.shape(test_img)\n",
    "print np.shape(test_img)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load Training , Validation , Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (22302, 128, 128, 3)\n",
      "Training Data Label (22302, 2)\n",
      "Test Data Label (2789, 2)\n",
      "val Data Label (2787, 2)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "#with tf.device('/gpu:0'):\n",
    "    if divide_flag == False:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n",
    "\n",
    "    if divide_flag == True:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_lab)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_ch =3\n",
    "n_classes=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define Variable , and placeholder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x = tf.placeholder(\"float\",shape=[None,img_row , img_col , in_ch],  name = 'x-input')\n",
    "y_= tf.placeholder(\"float\",shape=[None , n_classes] , name = 'y-input')\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "x_image= tf.reshape(x,[-1,img_row,img_col,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1 , shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def conv2d(x,w,strides_):\n",
    "    return tf.nn.conv2d(x,w, strides = strides_, padding='SAME')\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME')\n",
    "def max_pool(x , ksize , strides , padding='SAME'):\n",
    "    return tf.nn.max_pool(x ,ksize , strides , padding )\n",
    "def make_weights_biases(layer_name , w_name , ksize ,device_name,initializer='xavier'):\n",
    "    if len(ksize)==4: # convolution filter shape [batch , row , col , color_ch]\n",
    "        out_ch=ksize[3]\n",
    "    elif len(ksize)==2: #fully connected layer shape [in_ch , output_ch]\n",
    "        out_ch=ksize[1]\n",
    "    with tf.device(device_name):\n",
    "        with tf.variable_scope(layer_name) as scope:\n",
    "            try:\n",
    "                w_conv = tf.get_variable(w_name, ksize , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_conv = tf.get_variable(w_name, ksize , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "        with tf.variable_scope(layer_name) as scope:\n",
    "            try:\n",
    "                b_conv = bias_variable([out_ch])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_conv = bias_variable([out_ch])\n",
    "    return w_conv , b_conv \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def STEM_A(x , device_):\n",
    "    with tf.device(device_):\n",
    "        in_ch=x.get_shape()[3]\n",
    "        out_ch1=32 ; out_ch2=32;out_ch3=64;out_ch4=96;\n",
    "\n",
    "        c_ksize1=[3,3,in_ch , out_ch1]\n",
    "        c_ksize2=[3,3,out_ch1 , out_ch2]\n",
    "        c_ksize3=[3,3,out_ch2 , out_ch3]\n",
    "        c_ksize4=[3,3,out_ch3 , out_ch4]\n",
    "        w_conv1 , b_conv1 =make_weights_biases('STEM_A' , 'W1' , c_ksize1 ,device_name = '/gpu:0')\n",
    "        w_conv2 , b_conv2= make_weights_biases('STEM_A' , 'W2' , c_ksize2 ,device_name = '/gpu:0')\n",
    "        w_conv3 , b_conv3= make_weights_biases('STEM_A' , 'W3' , c_ksize3 ,device_name = '/gpu:0')\n",
    "        w_conv4 , b_conv4= make_weights_biases('STEM_A' , 'W4' , c_ksize4 ,device_name = '/gpu:0')\n",
    "\n",
    "        c_strides1=[1,2,2,1]\n",
    "        c_strides2=[1,1,1,1]\n",
    "        c_strides3=[1,1,1,1]\n",
    "        c_strides4=[1,2,2,1]\n",
    "\n",
    "        c_pooling1='VALID'\n",
    "        c_pooling2='VALID'\n",
    "        c_pooling3='SAME'\n",
    "        c_pooling4='VALID'\n",
    "\n",
    "        b_p_ksize4=[1,3,3,1]\n",
    "        b_p_strides4=[1,2,2,1]\n",
    "        b_p_padding4 ='VALID'\n",
    "\n",
    "\n",
    "\n",
    "        layer1=tf.nn.conv2d(    x ,      w_conv1, c_strides1, c_pooling1  )+b_conv1\n",
    "        layer1=tf.nn.relu(layer1)\n",
    "        layer2=tf.nn.conv2d(    layer1 , w_conv2, c_strides2, c_pooling2  )+b_conv2\n",
    "        layer2=tf.nn.relu(layer2)\n",
    "        layer3=tf.nn.conv2d(    layer2 , w_conv3, c_strides3, c_pooling3  )+b_conv3\n",
    "        layer3=tf.nn.relu(layer3)\n",
    "        layer4=tf.nn.conv2d(    layer3 , w_conv4, c_strides4, c_pooling4  )+b_conv4\n",
    "        layer4=tf.nn.relu(layer4)\n",
    "\n",
    "\n",
    "        b_layer4=tf.nn.max_pool(layer3 , b_p_ksize4, b_p_strides4, b_p_padding4 ) #b is branch\n",
    "        b_layer4=tf.nn.relu(b_layer4)\n",
    "        print layer1\n",
    "        print layer2\n",
    "        print layer3\n",
    "        print layer4\n",
    "        print b_layer4\n",
    "\n",
    "\n",
    "        concat_layer=tf.concat(3 , [layer4 , b_layer4])\n",
    "        ret_layer=concat_layer\n",
    "        print concat_layer\n",
    "        return ret_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def STEM_B( x , device_):\n",
    "    with tf.device(device_):\n",
    "        in_ch=x.get_shape()[3]\n",
    "\n",
    "\n",
    "        #########################################################################\n",
    "        out_ch1=64 ; out_ch2=64;out_ch3=64;out_ch4=96;\n",
    "        ##############################right side##################################\n",
    "        c_ksize1=[1,1,in_ch , out_ch1]\n",
    "        c_ksize2=[7,1,out_ch1 , out_ch2]\n",
    "        c_ksize3=[1,7,out_ch2 , out_ch3]\n",
    "        c_ksize4=[3,3,out_ch3 , out_ch4]\n",
    "\n",
    "        w_conv1 , b_conv1 =make_weights_biases('STEM_B' , 'W1' , c_ksize1 ,device_name = '/gpu:0')\n",
    "        w_conv2 , b_conv2= make_weights_biases('STEM_B' , 'W2' , c_ksize2 ,device_name = '/gpu:0')\n",
    "        w_conv3 , b_conv3= make_weights_biases('STEM_B' , 'W3' , c_ksize3 ,device_name = '/gpu:0')\n",
    "        w_conv4 , b_conv4= make_weights_biases('STEM_B' , 'W4' , c_ksize4 ,device_name = '/gpu:0')\n",
    "\n",
    "        c_strides1=[1,1,1,1]\n",
    "        c_strides2=[1,1,1,1]\n",
    "        c_strides3=[1,1,1,1]\n",
    "        c_strides4=[1,1,1,1]\n",
    "\n",
    "        c_pooling1='SAME'\n",
    "        c_pooling2='SAME'\n",
    "        c_pooling3='SAME'\n",
    "        c_pooling4='VALID'\n",
    "\n",
    "        ##############################left side##################################\n",
    "        out_ch1=64 ; out_ch2=96;    \n",
    "        #########################################################################\n",
    "        b_c_ksize1=[1,1,in_ch , out_ch1]\n",
    "        b_c_ksize2=[3,3,out_ch1 , out_ch2]\n",
    "        b_w_conv1 , b_b_conv1 =make_weights_biases('STEM_B' , 'b_W1' , b_c_ksize1 ,device_name = '/gpu:0')\n",
    "        b_w_conv2 , b_b_conv2= make_weights_biases('STEM_B' , 'b_W2' , b_c_ksize2 ,device_name = '/gpu:0')\n",
    "\n",
    "        b_c_strides1=[1,1,1,1]\n",
    "        b_c_strides2=[1,1,1,1]\n",
    "\n",
    "        b_c_pooling1='SAME'\n",
    "        b_c_pooling2='VALID'\n",
    "\n",
    "\n",
    "        ##############################convolution ##################################\n",
    "        layer1 = tf.nn.conv2d(x ,      w_conv1 , c_strides1 , c_pooling1)+b_conv1\n",
    "        layer1 = tf.nn.relu(layer1)\n",
    "        layer2 = tf.nn.conv2d(layer1 , w_conv2 , c_strides2 , c_pooling2)+b_conv2\n",
    "        layer2 = tf.nn.relu(layer2)\n",
    "        layer3 = tf.nn.conv2d(layer2 , w_conv3 , c_strides3 , c_pooling3)+b_conv3\n",
    "        layer3 = tf.nn.relu(layer3)\n",
    "        layer4 = tf.nn.conv2d(layer3 , w_conv4 , c_strides4 , c_pooling4)+b_conv4\n",
    "        layer4 = tf.nn.relu(layer4)\n",
    "\n",
    "        b_layer1 =tf.nn.conv2d(    x ,      b_w_conv1, b_c_strides1, b_c_pooling1  ) + b_b_conv1\n",
    "        b_layer1 = tf.nn.relu(layer1)\n",
    "        b_layer2=tf.nn.conv2d(  b_layer1 , b_w_conv2, b_c_strides2, b_c_pooling2  ) + b_b_conv2\n",
    "\n",
    "        ##############################concatenate layers###########################\n",
    "        concat_layer=tf.concat(3 , [layer4 , b_layer2])\n",
    "        ret_layer=concat_layer\n",
    "    return ret_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def STEM_C( x , device_ ):\n",
    "    with tf.device(device_):\n",
    "        in_ch=x.get_shape()[3]\n",
    "        out_ch1 = 192\n",
    "\n",
    "\n",
    "        c_ksize=[3,3,in_ch,out_ch1]\n",
    "        w_conv , b_conv =make_weights_biases('STEM_C' , 'W1' , c_ksize ,device_name = '/gpu:0')\n",
    "        c_strides=[1,1,1,1]\n",
    "        c_pooling='SAME'\n",
    "\n",
    "        b_p_ksize=[1,2,2,1]\n",
    "        b_p_strides=[1,1,1,1]\n",
    "        b_p_pooling='SAME'\n",
    "\n",
    "        layer1 = tf.nn.conv2d(x , w_conv   , c_strides   , c_pooling)+b_conv\n",
    "        layer1 = tf.nn.relu(layer1)\n",
    "        b_layer1 = tf.nn.max_pool(x , b_p_ksize , b_p_strides , b_p_pooling)\n",
    "        b_layer1 = tf.nn.relu(b_layer1)\n",
    "\n",
    "        print layer1\n",
    "        print b_layer1\n",
    "\n",
    "        concat_layer=tf.concat(3,[layer1 , b_layer1])\n",
    "        ret_layer = concat_layer\n",
    "    return ret_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FLAT(x):\n",
    "    \n",
    "    row=int(x.get_shape()[1])\n",
    "    col=int(x.get_shape()[2])\n",
    "    ch=int(x.get_shape()[3])\n",
    "    \n",
    "    res_x = tf.reshape(x , shape=[-1,row*col*ch])\n",
    "    return res_x\n",
    "    #connect fully connected layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FC_A(x , n_classes , device_ , device_2= '/gpu:4'):\n",
    "    with tf.device(device_):\n",
    "    \n",
    "        fully_ch1=1024; fully_ch2=1024\n",
    "\n",
    "        fc_ksize1=[x.get_shape()[1],fully_ch1]\n",
    "        fc_ksize2=[fully_ch1,fully_ch2]\n",
    "\n",
    "        w_fc1 ,b_fc1 = make_weights_biases('fc1' , 'fc_W1' , fc_ksize1 ,  device_)\n",
    "        w_fc2 ,b_fc2 = make_weights_biases('fc2' , 'fc_W2' , fc_ksize2 ,  device_2)\n",
    "\n",
    "        h_fc1=tf.matmul(x, w_fc1 )+b_fc1\n",
    "        h_fc1=tf.nn.dropout(h_fc1 , keep_prob)\n",
    "        h_fc2=tf.matmul(h_fc1 , w_fc2 )+b_fc2\n",
    "        h_fc2=tf.nn.dropout(h_fc2 , keep_prob)\n",
    "        end_fc=h_fc2\n",
    "\n",
    "        end_ksize=[end_fc.get_shape()[1] , n_classes]   \n",
    "        w_end ,b_end = make_weights_biases('fc_end' , 'fc_end_W' , end_ksize ,  device_2)\n",
    "        y_conv = tf.matmul(end_fc , w_end)+b_end\n",
    "\n",
    "        print w_fc1.get_shape()\n",
    "    return y_conv\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FC_B(x , n_classes , device_ ):\n",
    "    with tf.device(device_):\n",
    "\n",
    "        end_ksize=[x.get_shape()[1] , n_classes]   \n",
    "        w_end ,b_end = make_weights_biases('fc_end' , 'fc_end_W' , end_ksize ,  '/gpu:0')\n",
    "        y_conv = tf.matmul(x , w_end)+b_end\n",
    "\n",
    "   \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def INCEPTION_MODULE_A(x , device_):\n",
    "\n",
    "    \"\"\"\n",
    "    input X shape is [n_batch , row , col, out_ch]\n",
    "    35 x 35 grid\n",
    "    \"\"\"\n",
    "\n",
    "    #################################################################################\n",
    "    in_ch=x.get_shape()[3]\n",
    "    out_ch1=64 ; out_ch2= 96;\n",
    "    c_ksize1 = [1 , 1 ,in_ch , out_ch1 ]\n",
    "    c_ksize2 = [3 , 3 ,in_ch , out_ch2 ]\n",
    "    w_conv1 , b_conv1 = make_weights_biases ('INCEPTION_MODULE_A' , 'W1' , c_ksize1 ,device)\n",
    "    w_conv2 , b_conv2 = make_weights_biases('INCEPTION_MODUEL_A' ,'W1' , c_ksize2 ,device)\n",
    "\n",
    "    c_strides1=[1,1,1,1]\n",
    "    c_strides2=[1,1,1,1]\n",
    "    layer1 = tf.nn.conv2d(x , w_conv1 , c_strides1 ,device_ )\n",
    "    layer2 = tf.nn.conv2d(layer1 , w_conv2 , c_strides2 ,device_ )\n",
    "\n",
    "    #################################################################################                     \n",
    "\n",
    "    b1_p_ksize   =[1,2,2,1]\n",
    "    b1_p_strides =[1,1,1,1]\n",
    "    b1_p_pooling='SAME'\n",
    "\n",
    "\n",
    "    b1_out_ch = 96;\n",
    "    b1_c_ksize  =[1,1, in_ch , b1_out_ch]\n",
    "    b1_c_strides=[1,1,1,1]\n",
    "    b1_c_pooling ='SAME'\n",
    "\n",
    "    b1_w_conv , b1_b_conv =make_weights_biases('INCEPTION_MODULE_A','b1_W',b1_c_ksize , device)\n",
    "\n",
    "\n",
    "    b1_layer1=tf.nn.avg_pool(x, b1_p_ksize , b1_p_strides,b1_c_pooling)\n",
    "    b1_layer1=tf.nn.relu(b1_layer1)\n",
    "    b1_layer2=tf.nn.conv2d(b1_layer1 , b1_w_conv , b1_c_strides , b1_c_pooling)+b1_b_conv\n",
    "    b1_layer2=tf.nn.relu(b1_layer2)\n",
    "\n",
    "    #################################################################################                     \n",
    "\n",
    "    b2_out_ch=96\n",
    "    b2_c_ksize = [1,1,in_ch , b2_out_ch1 ]\n",
    "    b2_w_conv , b2_b_conv = make_weights_biases('INCEPTION_MODULE_A','b2_W',b2_c_ksize , device)\n",
    "    b2_c_strides=[1,1,1,1]\n",
    "    b2_c_pooling='SAME'\n",
    "\n",
    "    b2_layer = tf.nn.conv2d( x, b2_w_conv , b2_c_strides ,b2_pooling ) +b2_b_conv \n",
    "    b2_layer = tf.nn.relu(b2_layer)\n",
    "\n",
    "\n",
    "\n",
    "    #################################################################################                     \n",
    "\n",
    "\n",
    "    b3_out_ch1=64;b3_out_ch2=96;b3_out_ch3=96;\n",
    "    b3_c_ksize1 =[1,1,in_ch , b3_out_ch1]\n",
    "    b3_c_ksize2 =[3,3,b3_out_ch1 , b3_out_ch2]\n",
    "    b3_c_ksize3 =[3,3,b3_out_ch2 , b3_out_ch3]\n",
    "    b3_w_conv1 , b3_b_conv1 = make_weights_biases('INCEPTION_MODULE_A','b3_W1',b3_c_ksize1 , device)\n",
    "    b3_w_conv2 , b3_b_conv2 = make_weights_biases('INCEPTION_MODULE_A','b3_W2',b3_c_ksize2 , device)\n",
    "    b3_w_conv3 , b3_b_conv3 = make_weights_biases('INCEPTION_MODULE_A','b3_W3',b3_c_ksize3 , device)\n",
    "    b3_c_strides1=[1,1,1,1]\n",
    "    b3_c_strides2=[1,1,1,1]\n",
    "    b3_c_strides3=[1,1,1,1]\n",
    "    b3_c_pooling1='SAME'\n",
    "    b3_c_pooling2='SAME'\n",
    "    b3_c_pooling3='SAME'\n",
    "\n",
    "    b3_laer1 = tf.nn.conv2d(x , b3_w_conv1 , b3_c_strides1 ,b3_c_pooling1)+b3_b_conv1 \n",
    "    b3_layer1 = tf.nn.relu(layer1)\n",
    "    b3_layer2 = tf.nn.conv2d(layer1 , b3_w_conv1 , b3_c_strides1 ,b3_c_pooling1)+b3_b_conv2 \n",
    "    b3_layer2 = tf.nn.relu(layer2)\n",
    "    b3_layer3 = tf.nn.conv2d(layer2 , b3_w_conv1 , b3_c_strides1 ,b3_c_pooling1)+b3_b_conv3 \n",
    "    b3_layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "    #################################################################################                     \n",
    "    concat_A=tf.concat(3,layer2 , b1_layer2)\n",
    "    concat_B=tf.concat(3,concat_A , b2_layer2)\n",
    "    concat_C=tf.concat(3,concat_B , b3_layer3)\n",
    "\n",
    "    return concat_C\n",
    "\n",
    "    #################################################################################                     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def INCEPTION_MODULE_B(x , device):\n",
    "\n",
    "    \"\"\"\n",
    "    for 17 X 17 grid \n",
    "    \"\"\"\n",
    "    out_ch1 =192 ; out_ch2=224; out_ch3 =256\n",
    "    c_size1 = [1,1, in_ch  ,out_ch1]\n",
    "    c_size2 = [1,7, out_ch1,out_ch2]\n",
    "    c_size3 = [7,1, out_ch2,out_ch3]\n",
    "\n",
    "    w_conv1 , b_conv1 =make_weights_biases('INCEPTION_MODULE_B' ,'W1' ,  c_ksize1 , device)\n",
    "    w_conv2 , b_conv2 =make_weights_biases('INCEPTION_MODULE_B' ,'W2' ,  c_ksize2 , device)\n",
    "    w_conv3 , b_conv3 =make_weights_biases('INCEPTION_MODULE_B' ,'W3' ,  c_ksize3 , device)\n",
    "    c_strides1 =[1,1,1,1]\n",
    "    c_strides2 =[1,1,1,1]\n",
    "    c_strides2 =[1,1,1,1]\n",
    "    c_pooling1 ='SAME'\n",
    "    c_pooling2 ='SAME'\n",
    "    c_pooling3 ='SAME'\n",
    "\n",
    "\n",
    "    layer1 = tf.nn.conv2d(x, w_conv1,c_strides1 , c_pooling1 ) +b_conv1\n",
    "    layer1 = tf.nn.relu(layer1)\n",
    "    layer2 = tf.nn.conv2d(layer2, w_conv2,c_strides2 , c_pooling2 ) +b_conv2\n",
    "    layer2 = tf.nn.relu(layer2)\n",
    "    layer3 = tf.nn.conv2d(layer2, w_conv3,c_strides3 , c_pooling3 ) +b_conv3\n",
    "    layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "\n",
    "    ################################################################################# \n",
    "\n",
    "\n",
    "    b1_p_ksize=[1,2,2,1]\n",
    "    b1_p_strides=[1,1,1,1]\n",
    "    b1_p_pooling ='SAME'\n",
    "\n",
    "    b1_out_ch = 128\n",
    "    b1_c_ksize=[1,1,in_ch,b1_out_ch]\n",
    "    b1_w_conv , b1_b_conv = make_weights_biases('INCEPTION_MODULE_B','b1_W1',b1_c_ksize, device )\n",
    "    b1_c_strides=[1,1,1,1]\n",
    "    b1_c_pooling='SAME'\n",
    "\n",
    "\n",
    "    b1_layer1=tf.nn.avg_pooling(x , b1_p_ksize , b1_p_strides , b1_p_pooling )\n",
    "    b1_layer1=tf.nn.relu(layer1)\n",
    "    b1_layer2 =tf.nn.conv2d(layer1 ,b1_w_conv , b1_c_strides, b1_c_pooling )+b1_b_conv \n",
    "    b1_layer2 =tf.nn.relu(layer2)\n",
    "\n",
    "    ################################################################################# \n",
    "    b2_out_ch=384\n",
    "    b2_p_ksize=[1,1,in_ch,b2_out_ch]\n",
    "    b2_w_conv , b2_b_conv =make_weights_biases('INCEPTION_MODULE_B','b2_W1' ,b2_c_ksize , device)\n",
    "    b2_layer = tf.nn.conv2d(x,b2_w_conv , b2_c_strides , b2_c_pooling)+b2_b_conv\n",
    "\n",
    "    ################################################################################# \n",
    "\n",
    "\n",
    "    b3_out_ch1=192; b3_out_ch2 =192; b3_out_ch3=224 ; b3_out_ch4=224 ; b3_out_ch5 =256\n",
    "    b3_c_ksize1=[1,1,in_ch,b3_out_ch1]\n",
    "    b3_c_ksize2=[1,7,b3_out_ch1 , b3_out_ch2]\n",
    "    b3_c_ksize3=[7,1,b3_out_ch2 , b3_out_ch3]\n",
    "    b3_c_ksize4=[1,7,b3_out_ch3 , b3_out_ch4]\n",
    "    b3_c_ksize5=[7,1,b3_out_ch4 , b3_out_ch5]\n",
    "\n",
    "    b3_w_conv1 , b3_b_conv1=make_weights_biases('INCEPTION_MODULE_B','b3_W1',b3_c_ksize1, device)\n",
    "    b3_w_conv2 , b3_b_conv2=make_weights_biases('INCEPTION_MODULE_B','b3_W2',b3_c_ksize2, device)\n",
    "    b3_w_conv3 , b3_b_conv3=make_weights_biases('INCEPTION_MODULE_B','b3_W3',b3_c_ksize3, device)\n",
    "    b3_w_conv4 , b3_b_conv4=make_weights_biases('INCEPTION_MODULE_B','b3_W4',b3_c_ksize4, device)\n",
    "    b3_w_conv5 , b3_b_conv5=make_weights_biases('INCEPTION_MODULE_B','b3_W5',b3_c_ksize5, device)\n",
    "\n",
    "\n",
    "    b3_layer1 = tf.nn.conv2d(x,b3_w_conv1 , b2_c_strides , b2_c_pooling)+b3_b_conv1\n",
    "    b3_layer1 = tf.nn.relu(b3_layer1)\n",
    "    b3_layer2 = tf.nn.conv2d(b3_layer1,b3_w_conv2 , b2_c_strides , b2_c_pooling)+b3_b_conv2\n",
    "    b3_layer2 = tf.nn.relu(b3_layer2)\n",
    "    b3_layer3 = tf.nn.conv2d(b3_layer2,b3_w_conv3 , b2_c_strides , b2_c_pooling)+b3_b_conv3\n",
    "    b3_layer3 = tf.nn.relu(b3_layer3)\n",
    "    b3_layer4 = tf.nn.conv2d(b3_layer3,b3_w_conv4 , b2_c_strides , b2_c_pooling)+b3_b_conv4\n",
    "    b3_layer4 = tf.nn.relu(b3_layer4)\n",
    "    b3_layer5 = tf.nn.conv2d(b3_layer4,b3_w_conv5 , b2_c_strides , b2_c_pooling)+b3_b_conv5\n",
    "    b3_layer5 = tf.nn.relu(b3_layer5)\n",
    "\n",
    "    ################################################################################# \n",
    "\n",
    "\n",
    "    layerA=tf.concat(3, layer3 , b1_layer2 )\n",
    "    layerB=tf.concat(3, layerA , b2_layer )\n",
    "    layerC=tf.concat(3, layerB , b3_layer5 )\n",
    "\n",
    "    print layerC\n",
    "    return layerC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def INCEPTION_MODULE_C(x , device):\n",
    "\n",
    "    \"\"\"\n",
    "    for 8 X 8 grid modules\n",
    "    \"\"\"\n",
    "    out_ch1 = 384 ; out_ch2_a =256 ;out_ch2_b = 256\n",
    "    c_ksize1 = [1,1,in_ch , out_ch1]\n",
    "    c_ksize2_a = [1,3,out_ch1 , out_ch2_a]\n",
    "    c_ksize2_b = [3,1,out_ch2_a , out_ch2_b]\n",
    "    w_conv1 ,b_conv1=make_weights_biases('INCEPTION_MODULE_C','W1', c_ksize1 ,device)\n",
    "    w_conv2_a ,b_conv2_a=make_weights_biases('INCEPTION_MODULE_C','W2_a', c_ksize2_a ,device)\n",
    "    w_conv2_b ,b_conv2_b=make_weights_biases('INCEPTION_MODULE_C','W2_a', c_ksize2_b ,device)\n",
    "    c_strides1=[1,1,1,1]\n",
    "    c_strides2_a=[1,1,1,1]\n",
    "    c_strides2_b=[1,1,1,1]\n",
    "    c_pooling1='SAME'\n",
    "    c_pooling2_a='SAME'\n",
    "    c_pooling2_b='SAME'\n",
    "\n",
    "    layer1 = tf.nn.conv2d(x, w_conv1 ,c_strides )+b_conv1\n",
    "    layer1=tf.nn.relu(layer1)\n",
    "    layer2_a = tf.nn.conv2d(layer1,  w_conv2_a ,c_strides2_a ,c_pooling2_a )+b_conv2_a\n",
    "    layer2_a = tf.nn.relu(layer2_a)\n",
    "    layer2_b = tf.nn.conv2d(layer1  , w_conv2_a ,c_strides2_a ,c_pooling2_b)+b_conv2_b\n",
    "    layer2_b = tf.nn.relu(layer2_b)\n",
    "    ################################################################################# \n",
    "\n",
    "\n",
    "    b1_p_ksize=[1,2,2,1]\n",
    "    b1_p_strides=[1,1,1,1]\n",
    "    b1_p_pooling='SAME'\n",
    "\n",
    "    b1_out_ch =256\n",
    "    b1_c_ksize=[1,1,in_ch , b1_out_ch]\n",
    "    b1_w_conv , b1_b_conv= make_weights_biases('INCEPTION_MODULE_C' , 'b1_W1' , b1_c_ksize , device)\n",
    "    b1_c_pooling = 'SAME'\n",
    "\n",
    "    b1_layer1 = tf.nn.avg_pool(x , b1_p_ksize , b1_p_strides , b1_p_pooling)\n",
    "    b1_layer2 = tf.nn.conv2d(b1_layer1 , b1_w_conv , b1_c_strides  ,b1_c_pooling)+b1_b_conv\n",
    "    b1_layer2 = tf.nn.relu(b1_layer2)\n",
    "    ################################################################################# \n",
    "    b2_c_ksize=[1,1,1,1]\n",
    "    b2_c_strides=[1,1,1,1]\n",
    "    b2_w_conv , b2_b_conv= make_weights_biases('INCEPTION_MODULE_C' , 'b2_W' , b2_c_ksize , device)\n",
    "    b2_c_pooling = 'SAME'\n",
    "    b2_layer = tf.nn.conv2d(x , b2_w_conv , b2_c_strides  ,b2_c_pooling)+b2_b_conv\n",
    "    b2_layer= tf.nn.relu(b2_layer2)\n",
    "    ################################################################################# \n",
    "\n",
    "\n",
    "    b3_out_ch1=384;b3_out_ch2=448;b3_out_ch3=512;\n",
    "    b3_c_ksize1 =[1,1,in_ch , b3_out_ch1]\n",
    "    b3_c_ksize2 =[1,3,b3_out_ch1 , b3_out_ch2]\n",
    "    b3_c_ksize3 =[3,1,b3_out_ch2 , b3_out_ch3]\n",
    "    b3_c_ksize4_a =[3,1,b3_out_ch3 , b3_out_ch4_a]\n",
    "    b3_c_ksize4_b =[3,1,b3_out_ch3s, b3_out_ch4_b]\n",
    "\n",
    "    b3_w_conv1 , b3_b_conv1 = make_weights_biases('INCEPTION_MODULE_C','b3_W1',b3_c_ksize1 , device)\n",
    "    b3_w_conv2 , b3_b_conv2 = make_weights_biases('INCEPTION_MODULE_C','b3_W2',b3_c_ksize2 , device)\n",
    "    b3_w_conv3 , b3_b_conv3 = make_weights_biases('INCEPTION_MODULE_C','b3_W3',b3_c_ksize3 , device)\n",
    "    b3_w_conv4_a , b3_b_conv4_a = make_weights_biases('INCEPTION_MODULE_C','b3_W4_a',b4_a_c_ksize3 , device)\n",
    "    b3_w_conv4_b , b3_b_conv4_b = make_weights_biases('INCEPTION_MODULE_C','b3_W4_b',b4_b_c_ksize3 , device)\n",
    "\n",
    "    b3_c_strides1=[1,1,1,1]\n",
    "    b3_c_strides2=[1,1,1,1]\n",
    "    b3_c_strides3=[1,1,1,1]\n",
    "    b3_c_strides4_a=[1,1,1,1]\n",
    "    b3_c_strides4_b=[1,1,1,1]\n",
    "\n",
    "    b3_c_pooling1='SAME'\n",
    "    b3_c_pooling2='SAME'\n",
    "    b3_c_pooling3='SAME'\n",
    "    b3_c_pooling4_a='SAME'\n",
    "    b3_c_pooling4_b='SAME'\n",
    "\n",
    "    b3_laer1 = tf.nn.conv2d(x , b3_w_conv1 , b3_c_strides1 ,b3_c_pooling1)+b3_b_conv1 \n",
    "    b3_layer1 = tf.nn.relu(layer1)\n",
    "    b3_layer2 = tf.nn.conv2d(layer1 , b3_w_conv2 , b3_c_strides2 ,b3_c_pooling2)+b3_b_conv2 \n",
    "    b3_layer2 = tf.nn.relu(layer2)\n",
    "    b3_layer3 = tf.nn.conv2d(layer2 , b3_w_conv3 , b3_c_strides3 ,b3_c_pooling3)+b3_b_conv3 \n",
    "    b3_layer3 = tf.nn.relu(layer3)\n",
    "\n",
    "\n",
    "    b3_layer4_a = tf.nn.conv2d(layer3 , b3_w_conv4_a , b3_c_strides4_a ,b3_c_pooling4_a)+b3_b_conv4_a \n",
    "    b3_layer4_a = tf.nn.relu(b3_layer4_a)\n",
    "\n",
    "    b3_layer4_b = tf.nn.conv2d(layer3 , b3_w_conv4_b , b3_c_strides4_b ,b3_c_pooling4_b)+b3_b_conv4_b \n",
    "    b3_layer4_b = tf.nn.relu(layer4_b)\n",
    "\n",
    "\n",
    "    ################################################################################# \n",
    "\n",
    "    layerA = tf.concat(3 , [layer2_a, layer2_b])\n",
    "    layerB = tf.concat(3 , [layerA_b, b1_layer2])\n",
    "    layerC = tf.concat(3 , [layerB  , b2_layer])\n",
    "    layerD = tf.concat(3 , [layerC  , b3_layer4_a])\n",
    "    layerE = tf.concat(3 , [layerD  , b3_layer4_b])\n",
    "\n",
    "    return layerE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def INCEPTION_REDUCTION_A(x , device):\n",
    "    \n",
    "    \"\"\"\n",
    "    usage:\n",
    "    x shape =[ n_batch , row , col , ch] \n",
    "    \"\"\"\n",
    "    in_ch=x.get_shape()[3]\n",
    "    out_ch = 384\n",
    "    c_ksize= [3,3,in_ch , out_ch]\n",
    "    w_conv , b_conv =make_weights_biases('INCEPTION_REDUCTION_A','W1',c_ksize,device)\n",
    "    c_strides=[1,2,2,1]\n",
    "    \n",
    "    layer=tf.nn.conv2d(x,w_conv,c_strides , c_pooling)+b_conv\n",
    "    layer=tf.nn.relu(layer1)\n",
    "\n",
    "    \n",
    "    b1_p_ksize=[1,3,3,1]\n",
    "    b1_p_strides=[1,1,1,1]\n",
    "    b1_p_pooling='VALID'\n",
    "    b1_layer = tf.nn.max_pool(x,b1_p_ksize , b1_p_strides , b1_p_pooling)\n",
    "\n",
    "    b2_out_ch1 = 192; b2_out_ch2=288 ;b2_out_ch3 = 256;\n",
    "    b2_c_ksize1=[1,1,in_ch,b2_out_ch1]\n",
    "    b2_c_ksize2=[3,3,b2_out_ch1,b2_out_ch2]\n",
    "    b2_c_ksize3=[3,3,b2_out_ch2,b2_out_ch3]\n",
    "    w_conv1 , b_conv1 = make_weights_biases('INCEPTION_REDUCTION_A' ,'b2_W1' , b2_c_ksize1 , device)\n",
    "    w_conv2 , b_conv2 = make_weights_biases('INCEPTION_REDUCTION_A' ,'b2_W2' , b2_c_ksize2 , device)\n",
    "    w_conv3 , b_conv3 = make_weights_biases('INCEPTION_REDUCTION_A' ,'b2_W3' , b2_c_ksize3 , device)\n",
    "    \n",
    "    b2_c_strides1=[1,1,1,1]\n",
    "    b2_c_strides2=[1,1,1,1]\n",
    "    b2_c_strides3=[1,1,1,1]\n",
    "    b2_c_pooling1='SAME'\n",
    "    b2_c_pooling2='SAME'\n",
    "    b2_c_pooling3='VALID'\n",
    "    \n",
    "    b2_layer1 = tf.nn.conv2d(x        , b2_w_conv1 ,b2_c_strides1 ,b2_c_pooling1)+b2_b_conv1\n",
    "    b2_layer1 = tf.nn.relu(b2_layer1)\n",
    "    b2_layer2 = tf.nn.conv2d(b2_layer1, b2_w_conv2 ,b2_c_strides2 ,b2_c_pooling2)+b2_b_conv2\n",
    "    b2_layer2 = tf.nn.relu(b2_layer2)\n",
    "    b2_layer3 = tf.nn.conv2d(b2_layer2, b2_w_conv3 ,b2_c_strides3 ,b2_c_pooling3)+b2_b_conv3\n",
    "    b2_layer3 = tf.nn.relu(b2_layer3)\n",
    "    \n",
    "    layerA=tf.concat(3 ,[layer1 ,b1_layer ])\n",
    "    layerB=tf.concat(3 ,[layerA ,b2_layer3])\n",
    "    \n",
    "    return layerB\n",
    "    # hi im wonjoong, i want to use thi scomputer ,just 5 minutes\n",
    "    #ok my bro , enjoy!!and i have something to tell u, your data was run in here!\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-19-df8392ca56a5>, line 52)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-19-df8392ca56a5>\"\u001b[0;36m, line \u001b[0;32m52\u001b[0m\n\u001b[0;31m    b2_layer1 = tf.nn.conv2d(x, b2_w_conv1  b2_c_strides1 , b2_c_pooling1 )+b2_b_conv1\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def INCEPTION_REDUCTION_B(x , device):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    ####################################################################################\n",
    "    in_ch=x.get_shape()[3]\n",
    "    out_ch1=192 ; out_ch2=192;\n",
    "\n",
    "    c_ksize1 =[1,1,in_ch , out_ch1]\n",
    "    c_ksize2 =[3,3,out_ch1,out_ch2]\n",
    "    w_conv1, b_conv1 = make_weights_biases('INCEPTION_REDUCTION_B','W1',c_ksize1,device)\n",
    "    w_conv2, b_conv2 = make_weights_biases('INCEPTION_REDUCTION_B','W2',c_ksize2,device)\n",
    "    \n",
    "    c_strides1=[1,1,1,1]\n",
    "    c_strides2=[1,2,2,1]\n",
    "    \n",
    "    c_pooling1='SAME'\n",
    "    c_pooling2='VALID'\n",
    "    \n",
    "    layer1 = tf.nn.conv2d(x,w_conv1 , c_strides1, c_pooling1)\n",
    "    layer2 = tf.nn.conv2d(layer1 , w_conv2 , c_strides2 , c_pooling2)    \n",
    "    \n",
    "    b1_p_ksize=[1,3,3,1]\n",
    "    b1_p_strides=[1,2,2,1]\n",
    "    b1_p_pooling='VALID'\n",
    "    \n",
    "    b1_layer1=tf.nn.max_pool(x ,b1_p_ksize , b1_p_strides , b1_p_pooling)\n",
    "    \n",
    "    ####################################################################################\n",
    "    b2_out_ch1=256 ; b2_out_ch2 = 256 ; b2_out_ch3=320 ; b2_out_ch4=320;\n",
    "    b2_p_ksize1 =[1,1,in_ch , b2_out_ch1]\n",
    "    b2_p_ksize2 =[1,7,b2_out_ch1 , b2_out_ch2]\n",
    "    b2_p_ksize3 =[7,1,b2_out_ch2 , b2_out_ch3]\n",
    "    b2_p_ksize4 =[3,3,b2_out_ch3 , b2_out_ch4]\n",
    "    \n",
    "    b2_w_conv1, b2_b_conv1 = make_weights_biases('INCEPTION_REDUCTION_B','b2_W1' ,b2_c_ksize1,device)\n",
    "    b2_w_conv2, b2_b_conv2 = make_weights_biases('INCEPTION_REDUCTION_B','b2_W2' ,b2_c_ksize2,device)\n",
    "    b2_w_conv3, b2_b_conv3 = make_weights_biases('INCEPTION_REDUCTION_B','b2_W3' ,b2_c_ksize3,device)\n",
    "    b2_w_conv4, b2_b_conv4 = make_weights_biases('INCEPTION_REDUCTION_B','b2_W4' ,b2_c_ksize4,device)\n",
    "    \n",
    "    b2_c_strides1=[1,1,1,1]\n",
    "    b2_c_strides2=[1,1,1,1]\n",
    "    b2_c_strides3=[1,1,1,1]\n",
    "    b2_c_strides4=[1,1,1,1]\n",
    "    \n",
    "    b2_c_pooling1= 'SAME'\n",
    "    b2_c_pooling2= 'SAME'\n",
    "    b2_c_pooling3 = 'SAME'\n",
    "    b2_c_pooling4 = 'VALID'\n",
    "    \n",
    "    b2_layer1 = tf.nn.conv2d(x, b2_w_conv1  b2_c_strides1 , b2_c_pooling1 )+b2_b_conv1\n",
    "    b2_layer2 = tf.nn.conv2d(x, b2_w_conv2  b2_c_strides2 , b2_c_pooling2 )+b2_b_conv2\n",
    "    b2_layer3 = tf.nn.conv2d(x, b2_w_conv3  b2_c_strides3 , b2_c_pooling3 )+b2_b_conv3\n",
    "    b2_layer4 = tf.nn.conv2d(x, b2_w_conv4  b2_c_strides4 , b2_c_pooling4 )+b2_b_conv4\n",
    "    \n",
    "    ####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_list(folder_path):\n",
    "    list_files=os.walk(folder_path).next()[2]\n",
    "    ret_train_img_list=[]\n",
    "    ret_train_lab_list=[]\n",
    "    for i , ele in enumerate(list_files):\n",
    "\n",
    "        if 'train'  in ele and 'img'in ele:\n",
    "            ret_train_img_list.append(ele)\n",
    "        elif 'train' in ele  and  'lab' in ele:\n",
    "            ret_train_lab_list.append(ele)\n",
    "    return ret_train_img_list ,ret_train_lab_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_images , train_labels  = get_batch_list(file_locate)\n",
    "print train_images , train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "\n",
    "train_images.sort(key=natural_keys)\n",
    "train_labels.sort(key = natural_keys)\n",
    "print(train_images)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aug(np_img ,img_row ,img_col , color_ch, crop_img_row , crop_img_col , label):\n",
    "    \n",
    "    n_img = np.shape(np_img)[0] \n",
    "    n_ret_img = n_img*(img_row - crop_img_row) * (img_col - crop_img_col)*2\n",
    "    ret_images = np.zeros([n_ret_img ,crop_img_row , crop_img_col,color_ch])\n",
    "    len_label= np.shape(label)[1]\n",
    "    ret_labels =  np.zeros([n_ret_img  ,len_label])\n",
    "    #print \"n_augmented image size : \" , n_ret_img \n",
    "    #print \"n classes :\", len_label\n",
    "    #copy label in factor by 2014\n",
    "    \n",
    "    if len(np.shape(np_img))==2:\n",
    "        np_img=np.reshape(np_img , newshape = [np.shape(np_img)[0] , img_row , img_col ,color_ch])\n",
    "        print np.shape(np_img)\n",
    "    for n  in range(len(np_img)):\n",
    "        ret_labels[n*2 , : ] = label[n,:]\n",
    "        ret_labels[n*2+1 , : ] = label[n,:]\n",
    "        for r in range(img_row - crop_img_row):\n",
    "            for c in range(img_col - crop_img_col):\n",
    "                \n",
    "                cropped_img = np_img[n, r:crop_img_row +r , c:crop_img_col+c ,: ]\n",
    "                \n",
    "                ret_images[n*2,:,:,:]=cropped_img  \n",
    "                ret_images[(n*2+1) , :,:,:] =np.fliplr(cropped_img )\n",
    "\n",
    "    \n",
    "    return ret_images ,ret_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_test_img(np_img ,img_row ,img_col , color_ch, crop_img_row , crop_img_col ):\n",
    "    left_top =(0,0)\n",
    "    right_top =(  img_row  - crop_img_row  , 0 )\n",
    "    center =  ((img_row  - crop_img_row)/2  , (img_col - crop_img_row)/2)\n",
    "    left_buttom = (0,(img_col - crop_img_row)/2 )\n",
    "    right_buttom =  (img_row  - crop_img_row , img_col - crop_img_row)\n",
    "    \n",
    "    left_top_images  = np_img[: , left_top[0]:crop_img_row+left_top[0] , left_top[1] : crop_img_col+left_top[1] , :  ]\n",
    "    right_top_images = np_img[: , right_top[0]:crop_img_row +right_top[0], right_top[1] : crop_img_col +right_top[1], :  ]\n",
    "    center_images    = np_img[: , center[0]:crop_img_row +center[0], center[1] : crop_img_col +center[1], :  ]\n",
    "    left_buttom_images=np_img[: , left_buttom[0]:crop_img_row +left_buttom[0], left_buttom[1] : crop_img_col +left_buttom[1], :  ]\n",
    "    right_buttom_images= np_img[: , right_buttom[0]:crop_img_row+right_buttom[0] , right_buttom[1] : crop_img_col +right_buttom[1] , :  ]\n",
    "\n",
    "    \n",
    "        \n",
    "    return left_top_images , right_top_images , center_images , left_buttom_images , right_buttom_images \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def TRAIN_STRUCTURE_A(y_conv , y_ , device_ = '/gpu:4'):\n",
    "    \"\"\"\n",
    "    Return Value : cost , train_step ,correct_prediction , accuracy \n",
    "    \n",
    "    \"\"\"\n",
    "    with tf.device(device_):\n",
    "    #sm_conv= tf.nn.softmax(y_conv)\n",
    "        #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "\n",
    "\n",
    "        regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "    with tf.device(device_):\n",
    "        cost = cost+regular\n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cost) #1e-4\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "    return cost , train_step ,correct_prediction , accuracy \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def START_SESS():\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    return sess \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Terminal Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "def make_logdir(dirname):\n",
    "  \n",
    "\n",
    "    count=0\n",
    "    while(True):\n",
    "        if not os.path.isdir(dirname):\n",
    "            os.mkdir(dirname)\n",
    "            break\n",
    "        elif not os.path.isdir(dirname + str(count)):\n",
    "            dirname=dirname+str(count)\n",
    "            os.mkdir(dirname)\n",
    "            break\n",
    "        else:\n",
    "            count+=1\n",
    "    print 'it is recorded at :'+str(count)\n",
    "\n",
    "    f=open(dirname+\"/log.txt\",'w')\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_acc_loss(sess, x , y ):\n",
    "    \"\"\"\n",
    "    x type  : numpy \n",
    "    x shape : [n , row , col , ch]\n",
    "    \"\"\"\n",
    "    labels=ys\n",
    "    acc_list=[]\n",
    "    loss_list=[]\n",
    "    for img_ind ,ele  in enumerate(x):\n",
    "        accuracy = sess.run( accuracy , feed_dict={x:ele , y_: labels , keep_prob: 1.0})        \n",
    "        loss = sess.run(cost , feed_dict = {x:ele , y_:  labels , keep_prob: 1.0})\n",
    "        acc_list.append(accuracy)\n",
    "        loss_list.append(loss)\n",
    "\n",
    "    acc_list=np.asarray(acc_list)\n",
    "    loss_list=np.asarray(loss_list)\n",
    "    acc=np.mean(acc_list)\n",
    "    loss=np.mean(loss_list)\n",
    "    \n",
    "    return  acc,  loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_log(step,train_acc, train_loss , val_acc , val_loss ,fp):\n",
    "    \"\"\"\n",
    "    fp = File Pointer\n",
    "    \n",
    "    \"\"\"\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print step\n",
    "    print(\"step %d , training  accuracy %g\" %(step,train_acc))\n",
    "    print(\"step %d , loss : %g\" %(step,train_loss))\n",
    "    train_str = 'step:\\t'+str(step)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_acc)+'\\n'\n",
    "\n",
    "    print(\"step %d , validation  accuracy %g\" %(step,val_acc))\n",
    "    print(\"step %d , validation loss : %g\" %(step,val_loss))\n",
    "    val_str = 'step:\\t'+str(step)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_acc)+'\\n'\n",
    "\n",
    "    fp.write(train_str)\n",
    "    fp.write(val_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def BATCH_TRAINING(maxiter  , batch_size,file_locate, cost , train_step ,correct_prediction , \\\n",
    "                   accuracy , fp,divide_flag = False , aug_flag =False , random_flag = True ):\n",
    "\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "#with tf.device('/gpu:0'):\n",
    "    if divide_flag == False:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n",
    "\n",
    "    if divide_flag == True:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_lab)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n",
    "\n",
    "    for i in range(maxiter): \n",
    "        #print i\n",
    "        if divide_flag ==True:\n",
    "            n_batch =len(train_images)\n",
    "            batch_count=0\n",
    "            print batch_count , i\n",
    "            if batch_count >= n_batch:    \n",
    "                train_img =np.load(file_locate+train_images[batch_count])\n",
    "                train_lab =np.load(file_locate+train_labels[batch_count])\n",
    "            \n",
    "        batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)    \n",
    "        \n",
    "        \n",
    "        \n",
    "        if i%100 ==0: # in here, add to validation \n",
    "            try:\n",
    "                if aug_flag == True:\n",
    "                    color_ch = in_ch\n",
    "                    val_images=extract_test_img(val_img , 128 , 128 , color_ch   ,crop_img_row =118 , crop_img_col =118 )\n",
    "                    train_images=extract_test_img(val_img ,128 , 128 , color_ch   ,crop_img_row =118 , crop_img_col =118 )\n",
    "                    val_acc, val_loss =get_acc_loss(sess, val_images , val_batch_ys)\n",
    "                    train_acc, train_loss =get_acc_loss(sess, batch_xs , batch_ys)\n",
    "                else:\n",
    "                    val_acc,val_loss = sess.run([accuracy,loss] , feed_dict={x:val_img , y_:val_lab , keep_prob: 1.0})        \n",
    "                    train_acc ,train_loss = sess.run([accurac,loss] , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "                write_log(i,train_acc, train_loss , val_acc , val_loss ,fp)\n",
    "\n",
    "                if divide_flag ==True:\n",
    "                    batch_count+=1\n",
    "            except :\n",
    "                n_divide=len(val_img)/batch_size\n",
    "                j=0\n",
    "                if aug_flag == True:\n",
    "                    list_val_acc=[]\n",
    "                    list_val_loss=[]\n",
    "                    for j in range(n_divide):\n",
    "                        \n",
    "                        # j*batch_size :(j+1)*batch_size\n",
    "                        val_batch_xs =val_img[ j*batch_size :(j+1)*batch_size] \n",
    "                        val_batch_ys =val_lab[ j*batch_size :(j+1)*batch_size]\n",
    "                        val_images = extract_test_img( val_batch_xs  ,128 , 128 , color_ch   ,crop_img_row =118 , crop_img_col =118 )\n",
    "                        val_acc ,val_loss =get_acc_loss(sess, val_images , val_batch_ys)    \n",
    "                        list_val_acc.append(val_acc)\n",
    "                        list_val_loss.append(val_loss)\n",
    "                        \n",
    "                    #right above code have to modify\n",
    "                    val_batch_xs = val_img[ (j+1)*batch_size :  ] \n",
    "                    val_batch_ys = val_lab[ (j+1)*batch_size :  ]\n",
    "                    val_images = extract_test_img( val_batch_xs  ,128 , 128 , color_ch   ,crop_img_row =118 , crop_img_col =118 )\n",
    "                    val_acc ,val_loss =get_acc_loss(sess, val_images , val_batch_ys)\n",
    "                    list_val_acc.append(val_acc)\n",
    "                    list_val_loss.append(val_loss)\n",
    "                                            \n",
    "                    val_acc_list=np.asarray(val_acc_list)\n",
    "                    val_loss_list= np.asarray(val_loss_list)\n",
    "                    val_acc=np.mean(val_acc_list)\n",
    "                    val_loss = np.mean(val_loss_list)\n",
    "\n",
    "                    #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "                    train_images = extract_test_img( batch_xs  ,128 , 128 , color_ch   ,crop_img_row =118 , crop_img_col =118 )\n",
    "                    train_acc, train_loss = get_acc_loss(train_images,batch_ys )\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    for j in range(n_divide):\n",
    "                        list_acc=[]\n",
    "                        list_loss=[]\n",
    "                        # j*batch_size :(j+1)*batch_size\n",
    "                        val_batch_xs =  val_img[ j*batch_size :(j+1)*batch_size] \n",
    "                        val_batch_ys =  val_lab[ j*batch_size :(j+1)*batch_size]                        \n",
    "                        val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_batch_xs , y_:val_batch_ys , keep_prob: 1.0})        \n",
    "                        list_acc.append(float(val_accuracy))\n",
    "                        list_loss.append(float(val_loss))\n",
    "                        #right above code have to modify\n",
    "                    val_batch_xs = val_img[ (j+1)*batch_size :  ] \n",
    "                    val_batch_ys = val_lab[ (j+1)*batch_size :  ]\n",
    "                    list_acc=np.asarray(list_acc)\n",
    "                    list_loss= np.asarray(list_loss)\n",
    "                    val_acc=np.mean(list_acc)\n",
    "                    val_loss = np.mean(list_loss)\n",
    "\n",
    "                    #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "\n",
    "                    train_acc,train_loss = sess.run( [accuracy,cost] , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "                    \n",
    "\n",
    "                write_log(i,train_acc, train_loss , val_acc , val_loss ,fp)\n",
    "\n",
    "                if divide_flag == True:\n",
    "                    batch_count+=1\n",
    "\n",
    "        if aug_flag == True:\n",
    "            aug_batch_xs , aug_batch_ys=aug(np_img=batch_xs[:int(batch_size)] ,img_row= img_row  ,img_col =img_col\\\n",
    "                                            , color_ch=3, crop_img_row =118, crop_img_col =118, label = batch_ys )\n",
    "            share= len(aug_batch_xs) / batch_size\n",
    "\n",
    "            for i in xrange(share):\n",
    "                batch_xs=aug_batch_xs[i*batch_size : (i+1)*batch_size,:,:,:]  \n",
    "                batch_ys=aug_batch_ys[i*batch_size : (i+1)*batch_size,:]  \n",
    "                sess.run(train_step ,feed_dict={x: batch_xs, y_:batch_ys , keep_prob : 0.7})\n",
    "            share= len(aug_batch_xs) / batch_size\n",
    "\n",
    "        else:\n",
    "            sess.run(train_step ,feed_dict={x: batch_xs, y_:batch_ys , keep_prob : 0.7})\n",
    "\n",
    "    print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "    train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "    fp.write(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dirname='./result/'\n",
    "A=STEM_A(x ,'/gpu:0')\n",
    "B=STEM_B(A ,'/gpu:1')\n",
    "C=STEM_C(B ,'/gpu:2')\n",
    "flat_C=FLAT(C)\n",
    "y_conv=FC_A(flat_C , 2,'/gpu:3')\n",
    "cost , train_step ,correct_prediction , accuracy = TRAIN_STRUCTURE_A(y_conv , y_)\n",
    "sess=START_SESS()\n",
    "fp=make_logdir(dirname)\n",
    "BATCH_TRAINING(132000 , 30,file_locate , cost , train_step ,correct_prediction , accuracy  ,fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    test_accuracy = sess.run( accuracy , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})        \n",
    "    test_loss = sess.run(cost , feed_dict = {x:test_img , y_: test_lab , keep_prob: 1.0})\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "except :\n",
    "    list_acc=[]\n",
    "    list_loss=[]\n",
    "    n_divide=len(test_img)/batch_size\n",
    "    for j in range(n_divide):\n",
    "\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "        list_acc.append(float(test_accuracy))\n",
    "        list_loss.append(float(test_loss))\n",
    "    test_accuracy , test_loss=sess.run([accuracy,cost] , feed_dict={x:test_img[(j+1)*batch_size : ] , y_:test_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "    #right above code have to modify\n",
    "\n",
    "    list_acc.append(test_accuracy)\n",
    "    list_loss.append(test_loss)\n",
    "    list_acc=np.asarray(list_acc)\n",
    "    list_loss= np.asarray(list_loss)\n",
    "\n",
    "    test_accuracy=np.mean(list_acc)\n",
    "    test_loss = np.mean(list_loss)\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
