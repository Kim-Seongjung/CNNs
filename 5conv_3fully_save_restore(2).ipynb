{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 100, 100, 3)\n",
      "100 100\n",
      "100 100\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\"\"\"\n",
    "created by kim Seong jung\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n",
    "file_locate='/home/seongjung/바탕화면/Numpy_ASAN/Mal_vs_Benign/100_100/4/0/'\n",
    "sess = tf.InteractiveSession()\n",
    "test_img=np.load(file_locate+'test_img.npy');\n",
    "try:\n",
    "    print np.shape(test_img)\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "except:\n",
    "    np.shape(test_img)\n",
    "    test_img=np.reshape(test_img , newshape = [np.shape(test_img)[0] , 32, 32 ,3] )\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "\n",
    "    \n",
    "divide_flag= False\n",
    "restore_flag =False\n",
    "#odel_save_path='/media/seongjung/Seagate Backup Plus Drive/data/ASAN/ASAN_weight_bias/3_0/'\n",
    "model_save_path='/home/seongjung/바탕화면/4_0/'\n",
    "if restore_flag ==True:\n",
    "#   restore_path='/media/seongjung/Seagate Backup Plus Drive/data/ASAN/ASAN_weight_bias/3_0/'\n",
    "    restore_path='/home/seongjung/바탕화면/4_0/'\n",
    "batch_size=30\n",
    "print img_row ,img_col\n",
    "n_classes =2\n",
    "in_ch =3\n",
    "out_ch1=200\n",
    "out_ch2=200\n",
    "out_ch3=200\n",
    "out_ch4=200\n",
    "out_ch5=200\n",
    "\n",
    "\n",
    "fully_ch1=1024\n",
    "fully_ch2 =1024\n",
    "fully_ch3 =1024\n",
    "\n",
    "\n",
    "\n",
    "strides_1=[1,2,2,1]\n",
    "strides_2=[1,1,1,1]\n",
    "strides_3=[1,1,1,1]\n",
    "strides_4=[1,1,1,1]\n",
    "strides_5=[1,1,1,1]\n",
    "\n",
    "\n",
    "x= tf.placeholder(\"float\",shape=[None,img_col , img_row , 3],  name = 'x-input')\n",
    "y_=tf.placeholder(\"float\",shape=[None , n_classes] , name = 'y-input')\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "x_image= tf.reshape(x,[-1,img_row,img_col,3])\n",
    "\n",
    "iterate=100000\n",
    "\n",
    "\n",
    "\n",
    "weight_row =3 ; weight_col=3\n",
    "\n",
    "\n",
    "pooling_row_size1=int(img_row/2)\n",
    "pooling_row_size2=int(pooling_row_size1/2)\n",
    "pooling_row_size3=int(pooling_row_size2/2)\n",
    "pooling_row_size4=int(pooling_row_size3/2)\n",
    "pooling_row_size5=int(pooling_row_size4/2)\n",
    "pooling_col_size1=int(img_col/2)\n",
    "pooling_col_size2=int(pooling_col_size1/2)\n",
    "pooling_col_size3=int(pooling_col_size2/2)\n",
    "pooling_col_size4=int(pooling_col_size3/2)\n",
    "pooling_col_size5=int(pooling_col_size4/2)\n",
    "\n",
    "print img_col , img_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Weight and Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/seongjung/jupyter'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (306, 100, 100, 3)\n",
      "Training Data Label (306, 2)\n",
      "Test Data Label (39, 2)\n",
      "val Data Label (38, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "#with tf.device('/gpu:0'):\n",
    "\n",
    "    if divide_flag == False:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n",
    "\n",
    "    if divide_flag == True:\n",
    "        train_img=np.load(file_locate+'train_img_1.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab_1.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_lab)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "with tf.device('/gpu:0'):\n",
    "    def bias_variable(shape):\n",
    "        initial = tf.constant(0.1 , shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    def conv2d(x,w,strides_):\n",
    "        return tf.nn.conv2d(x,w, strides = strides_, padding='SAME')\n",
    "    def max_pool_2x2(x):\n",
    "        return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if restore_flag==False:\n",
    "    with tf.variable_scope(\"layer1\") as scope:\n",
    "        try:\n",
    "            w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,3,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,3,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    with tf.variable_scope(\"layer1\") as scope:\n",
    "        try:\n",
    "            b_conv1 = bias_variable([out_ch1])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv1 = bias_variable([out_ch1])\n",
    "    with tf.variable_scope('layer2') as scope:\n",
    "        try:\n",
    "            w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    with tf.variable_scope('layer2') as scope:\n",
    "        try:\n",
    "            b_conv2= bias_variable([out_ch2])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv2= bias_variable([out_ch2])\n",
    "\n",
    "    with tf.variable_scope('layer3') as scope:\n",
    "        try:\n",
    "            w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    with tf.variable_scope('layer3') as scope:\n",
    "        try:\n",
    "            b_conv3 = bias_variable([out_ch3])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv3 = bias_variable([out_ch3])\n",
    "\n",
    "    with tf.variable_scope('layer4') as scope:\n",
    "        try:\n",
    "            w_conv4 =tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch3,out_ch4] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv3 = tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    with tf.variable_scope('layer4') as scope:\n",
    "        try:\n",
    "            b_conv4 = bias_variable([out_ch4])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv3 = bias_variable([out_ch3])\n",
    "\n",
    "    with tf.variable_scope('layer5') as scope:\n",
    "        try:\n",
    "            w_conv5 = tf.get_variable(\"W5\",[weight_row,weight_col,out_ch4,out_ch5] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv3 = tf.get_variable(\"W5\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    with tf.variable_scope('layer5') as scope:\n",
    "        try:\n",
    "            b_conv5 = bias_variable([out_ch5])\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv3 = bias_variable([out_ch3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if restore_flag==True:\n",
    "    with tf.variable_scope(\"layer1\") as scope:\n",
    "        try:\n",
    "            w_conv1 = tf.Variable(np.load(restore_path+'/w_conv1.npy'),name=\"W1\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv1 = tf.Variable(np.load(restore_path+'/w_conv1.npy'),name=\"W1\")\n",
    "    with tf.variable_scope(\"layer1\") as scope:\n",
    "        try:\n",
    "            b_conv1 = tf.Variable(np.load(restore_path+'/b_conv1.npy'),name=\"B1\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv1 =tf.Variable(np.load(restore_path+'/b_conv1.npy'),name=\"B1\")\n",
    "    with tf.variable_scope(\"layer2\") as scope:\n",
    "        try:\n",
    "            w_conv2 = tf.Variable(np.load(restore_path+'/w_conv2.npy'),name=\"W2\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv2 = tf.Variable(np.load(restore_path+'/w_conv2.npy'),name=\"W2\")\n",
    "    with tf.variable_scope(\"layer2\") as scope:\n",
    "        try:\n",
    "            b_conv2 = tf.Variable(np.load(restore_path+'/b_conv2.npy'),name=\"B2\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv2 =tf.Variable(np.load(restore_path+'/b_conv2.npy'),name=\"B2\")\n",
    "    with tf.variable_scope(\"layer3\") as scope:\n",
    "        try:\n",
    "            w_conv3 = tf.Variable(np.load(restore_path+'/w_conv3.npy'),name=\"W3\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv3 = tf.Variable(np.load(restore_path+'/w_conv3.npy'),name=\"W3\")\n",
    "    with tf.variable_scope(\"layer3\") as scope:\n",
    "        try:\n",
    "            b_conv3 = tf.Variable(np.load(restore_path+'/b_conv3.npy'),name=\"B3\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv3 =tf.Variable(np.load(restore_path+'/b_conv3.npy'),name=\"B3\")\n",
    "    with tf.variable_scope(\"layer4\") as scope:\n",
    "        try:\n",
    "            w_conv4 = tf.Variable(np.load(restore_path+'/w_conv4.npy'),name=\"W4\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv4 = tf.Variable(np.load(restore_path+'/w_conv4.npy'),name=\"W4\")\n",
    "    with tf.variable_scope(\"layer4\") as scope:\n",
    "        try:\n",
    "            b_conv4 = tf.Variable(np.load(restore_path+'/b_conv4.npy'),name=\"B4\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv4 =tf.Variable(np.load(restore_path+'/b_conv4.npy'),name=\"B4\")\n",
    "    with tf.variable_scope(\"layer5\") as scope:\n",
    "        try:\n",
    "            w_conv5 = tf.Variable(np.load(restore_path+'/w_conv5.npy'),name=\"W5\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_conv5 = tf.Variable(np.load(restore_path+'/w_conv5.npy'),name=\"W5\")\n",
    "    with tf.variable_scope(\"layer5\") as scope:\n",
    "        try:\n",
    "            b_conv5 = tf.Variable(np.load(restore_path+'/b_conv5.npy'),name=\"B5\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_conv5 =tf.Variable(np.load(restore_path+'/b_conv5.npy'),name=\"B5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(?, 50, 50, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"MaxPool:0\", shape=(?, 25, 25, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_2:0\", shape=(?, 25, 25, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Relu_3:0\", shape=(?, 25, 25, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"MaxPool_2:0\", shape=(?, 13, 13, 200), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "#conncect hidden layer \n",
    "with tf.device('/gpu:0'):\n",
    "    h_conv1 = tf.nn.relu(conv2d(x_image , w_conv1 ,strides_1)+b_conv1)\n",
    "    h_conv2 = tf.nn.relu(conv2d(h_conv1 , w_conv2 ,strides_2)+b_conv2)\n",
    "    h_conv2 = max_pool_2x2(h_conv2)#pooling\n",
    "    \n",
    "    h_conv3 = tf.nn.relu(conv2d(h_conv2 , w_conv3,strides_3)+b_conv3)\n",
    "    h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4,strides_4)+b_conv4)\n",
    "    h_pool4 = max_pool_2x2(h_conv4) #pooling \n",
    "\n",
    "    h_conv5 = tf.nn.relu(conv2d(h_conv4, w_conv5,strides_5)+b_conv5)\n",
    "    h_conv5= max_pool_2x2(h_conv5) #pooling \n",
    "\n",
    "    print h_conv1\n",
    "    print h_conv2\n",
    "    print h_conv3\n",
    "    print h_conv4\n",
    "    print h_conv5\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end_conv = h_conv5\n",
    "#print conv2d(h_pool1 , w_conv2).get_shape()\n",
    "end_conv_row=int(h_conv5.get_shape()[1])\n",
    "end_conv_col=int(h_conv5.get_shape()[2])\n",
    "end_conv_ch=int(h_conv5.get_shape()[3])\n",
    "#connect fully connected layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connect fully connected layer \n",
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"fc1\") as scope:\n",
    "            try:\n",
    "                w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_fc1 = bias_variable([fully_ch1])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc1 = bias_variable([fully_ch1])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"fc1\") as scope:\n",
    "            try:\n",
    "                w_fc1=tf.Variable(np.load(restore_path+'/w_fc1.npy'),name=\"fc1_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc1=tf.Variable(np.load(restore_path+'/w_fc1.npy'),name=\"fc1_W\")\n",
    "            try:\n",
    "                b_fc1=tf.Variable(np.load(restore_path+'/b_fc1.npy'),name=\"fc1_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc1=tf.Variable(np.load(restore_path+'/b_fc1.npy'),name=\"fc1_B\")\n",
    "\n",
    "        \n",
    "with tf.device('/gpu:0'): # flat conv layer \n",
    "    end_flat_conv =tf.reshape(end_conv, [-1,end_conv_col*end_conv_row*end_conv_ch])\n",
    "   \n",
    "with tf.device('/gpu:0'): # connect flat layer with fully  connnected layer \n",
    "    h_fc1 = tf.nn.relu(tf.matmul(end_flat_conv , w_fc1)+ b_fc1)\n",
    "    h_fc1 = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12800, 1024)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.load('/home/seongjung/variable_save/w_fc1.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#connect fully connected layer \n",
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"fc2\") as scope:\n",
    "            try:\n",
    "                w_fc2=tf.get_variable(\"fc2_W\",[fully_ch1,fully_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc2=tf.get_variable(\"fc2_W\",[fully_ch1,fully_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_fc2 = bias_variable([fully_ch2])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc2 = bias_variable([fully_ch2])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"fc2\") as scope:\n",
    "            try:\n",
    "                w_fc2=tf.Variable(np.load(restore_path+'/w_fc2.npy'),name=\"fc2_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc2=tf.Variable(np.load(restore_path+'/w_fc2.npy'),name=\"fc2_W\")\n",
    "            try:\n",
    "                b_fc2=tf.Variable(np.load(restore_path+'/b_fc2.npy'),name=\"fc2_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc2=tf.Variable(np.load(restore_path+'/b_fc2.npy'),name=\"fc2_B\")\n",
    "\n",
    "with tf.device('/gpu:0'): # connect flat layer with fully  connnected layer \n",
    "    h_fc2 = tf.nn.relu(tf.matmul(h_fc1 , w_fc2)+ b_fc2)\n",
    "    h_fc2 = tf.nn.dropout(h_fc2, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#connect fully connected layer \n",
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"fc3\") as scope:\n",
    "            try:\n",
    "                w_fc3=tf.get_variable(\"fc3_W\",[fully_ch2,fully_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc3=tf.get_variable(\"fc3_W\",[fully_ch2,fully_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_fc3 = bias_variable([fully_ch3])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc3 = bias_variable([fully_ch3])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"fc3\") as scope:\n",
    "            try:\n",
    "                w_fc3=tf.Variable(np.load(restore_path+'/w_fc3.npy'),name=\"fc3_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_fc3=tf.Variable(np.load(restore_path+'/w_fc3.npy'),name=\"fc3_W\")\n",
    "            try:\n",
    "                b_fc3=tf.Variable(np.load(restore_path+'/b_fc3.npy'),name=\"fc3_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_fc3=tf.Variable(np.load(restore_path+'/b_fc3.npy',name=\"fc3_B\"))\n",
    "\n",
    "with tf.device('/gpu:0'): # connect flat layer with fully  connnected layer \n",
    "    h_fc3 = tf.nn.relu(tf.matmul(h_fc2 , w_fc3)+ b_fc3)\n",
    "    h_fc3 = tf.nn.dropout(h_fc3, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "end_fc=h_fc3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if restore_flag==False:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope('fc3') as scope:\n",
    "            try:\n",
    "                w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes ],initializer = tf.contrib.layers.xavier_initializer())\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "            try:\n",
    "                b_end = bias_variable([n_classes])\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_end = bias_variable([n_classes])\n",
    "elif restore_flag==True:\n",
    "    with tf.device('/gpu:0'):\n",
    "        with tf.variable_scope(\"fc3\") as scope:\n",
    "            try:\n",
    "                w_end=tf.Variable(np.load(restore_path+'/w_end.npy'),name=\"end_W\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                w_end=tf.Variable(np.load(restore_path+'/w_end.npy'),name=\"end_W\")\n",
    "            try:\n",
    "                b_end=tf.Variable(np.load(restore_path+'/b_end.npy'),name=\"end_B\")\n",
    "            except:\n",
    "                scope.reuse_variables()\n",
    "                b_end=tf.Variable(np.load(restore_path+'/b_end.npy'),name=\"end_B\")\n",
    "\n",
    "with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "    y_conv = tf.matmul(end_fc , w_end)+b_end\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is recorded at :12\n"
     ]
    }
   ],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "\n",
    "dirname='/media/seongjung/Seagate Backup Plus Drive/data/ASAN/result/'\n",
    "dirname='/home/seongjung/바탕화면/result_temp/'    \n",
    "count=0\n",
    "while(True):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    elif not os.path.isdir(dirname + str(count)):\n",
    "        dirname=dirname+str(count)\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "print 'it is recorded at :'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(dirname+\"/log.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_list(folder_path):\n",
    "    list_files=os.walk(folder_path).next()[2]\n",
    "    print list_files\n",
    "    ret_train_img_list=[]\n",
    "    ret_train_lab_list=[]\n",
    "    for i , ele in enumerate(list_files):\n",
    "\n",
    "        if 'train'  in ele and 'img'in ele:\n",
    "            ret_train_img_list.append(ele)\n",
    "        elif 'train' in ele  and  'lab' in ele:\n",
    "            ret_train_lab_list.append(ele)\n",
    "    return ret_train_img_list ,ret_train_lab_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['val_lab.npy', 'test_lab.npy', 'train_lab.npy', 'val_img.npy', 'test_img.npy', 'train_img.npy']\n"
     ]
    }
   ],
   "source": [
    "train_images , train_labels  = get_batch_list(file_locate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_img.npy']\n",
      "['train_lab.npy']\n"
     ]
    }
   ],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "\n",
    "train_images.sort(key=natural_keys)\n",
    "train_labels.sort(key = natural_keys)\n",
    "print(train_images)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_numpy_weight( model_save_path ):\n",
    "    \n",
    "    np_w_conv1,np_w_conv2,np_w_conv3,np_w_conv4,np_w_conv5=sess.run([w_conv1,w_conv2,w_conv3,w_conv4,w_conv5])\n",
    "    np_b_conv1,np_b_conv2,np_b_conv3,np_b_conv4,np_b_conv5=sess.run([b_conv1,b_conv2,b_conv3,b_conv4,b_conv5])\n",
    "    np_w_fc1 , np_w_fc2,np_w_fc3,np_w_end=sess.run([w_fc1 , w_fc2,w_fc3 ,w_end])\n",
    "    np_b_fc1 , np_b_fc2,np_b_fc3,np_b_end=sess.run([b_fc1 , b_fc2,b_fc3,b_end])\n",
    "    \n",
    "    np_w_conv1=np.asarray(np_w_conv1)\n",
    "    np_w_conv2=np.asarray(np_w_conv2)\n",
    "    np_w_conv3=np.asarray(np_w_conv3)\n",
    "    np_w_conv4=np.asarray(np_w_conv4)\n",
    "    np_w_conv5=np.asarray(np_w_conv5)\n",
    "    \n",
    "    np_b_conv1=np.asarray(np_b_conv1)\n",
    "    np_b_conv2=np.asarray(np_b_conv2)\n",
    "    np_b_conv3=np.asarray(np_b_conv3)\n",
    "    np_b_conv4=np.asarray(np_b_conv4)\n",
    "    np_b_conv5=np.asarray(np_b_conv5)\n",
    "    \n",
    "    np_w_fc1=np.asarray(np_w_fc1)\n",
    "    np_w_fc2=np.asarray(np_w_fc2)\n",
    "    np_w_fc3=np.asarray(np_w_fc3)\n",
    "    np_w_end=np.asarray(np_w_end)\n",
    "    \n",
    "    np_b_fc1=np.asarray(np_b_fc1)\n",
    "    np_b_fc2=np.asarray(np_b_fc2)\n",
    "    np_b_fc3=np.asarray(np_b_fc3)\n",
    "    np_b_end=np.asarray(np_b_end)\n",
    "    \n",
    "    \n",
    "    np.save(model_save_path +'w_conv1' , np_w_conv1)\n",
    "    np.save(model_save_path +'w_conv2' , np_w_conv2)\n",
    "    np.save(model_save_path +'w_conv3' , np_w_conv3)\n",
    "    np.save(model_save_path +'w_conv4' , np_w_conv4)\n",
    "    np.save(model_save_path +'w_conv5' , np_w_conv5)\n",
    "    \n",
    "    np.save(model_save_path +'b_conv1' , np_b_conv1)\n",
    "    np.save(model_save_path +'b_conv2' , np_b_conv2)\n",
    "    np.save(model_save_path +'b_conv3' , np_b_conv3)\n",
    "    np.save(model_save_path +'b_conv4' , np_b_conv4)\n",
    "    np.save(model_save_path +'b_conv5' , np_b_conv5)\n",
    "\n",
    "    np.save(model_save_path +'w_fc1' , np_w_fc1)\n",
    "    np.save(model_save_path +'w_fc2' , np_w_fc2)\n",
    "    np.save(model_save_path +'w_fc3' , np_w_fc3)\n",
    "    np.save(model_save_path +'w_end' , np_w_end)\n",
    "    \n",
    "    np.save(model_save_path +'b_fc1' , np_b_fc1)\n",
    "    np.save(model_save_path +'b_fc2' , np_b_fc2)\n",
    "    np.save(model_save_path +'b_fc3' , np_b_fc3)\n",
    "    np.save(model_save_path +'b_end' , np_b_end)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-7bef9d9333bc>:19 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "a\n",
      "step 0 , training  accuracy 0.5\n",
      "step 0 , loss : 7.78817\n",
      "step 0 , validation  accuracy 0.526316\n",
      "step 0 , validation loss : 9.14033\n",
      "step 0 , test  accuracy 0.435897\n",
      "step 0 , test loss : 9.62062\n",
      "a\n",
      "step 100 , training  accuracy 0.566667\n",
      "step 100 , loss : 466.09\n",
      "step 100 , validation  accuracy 0.473684\n",
      "step 100 , validation loss : 565.006\n",
      "step 100 , test  accuracy 0.564103\n",
      "step 100 , test loss : 581.413\n",
      "step 200 , training  accuracy 0.4\n",
      "step 200 , loss : 2.98964\n",
      "step 200 , validation  accuracy 0.473684\n",
      "step 200 , validation loss : 2.93412\n",
      "step 200 , test  accuracy 0.564103\n",
      "step 200 , test loss : 3.08341\n",
      "step 300 , training  accuracy 0.466667\n",
      "step 300 , loss : 25.4664\n",
      "step 300 , validation  accuracy 0.526316\n",
      "step 300 , validation loss : 30.6398\n",
      "step 300 , test  accuracy 0.435897\n",
      "step 300 , test loss : 30.9997\n",
      "step 400 , training  accuracy 0.566667\n",
      "step 400 , loss : 15.0775\n",
      "step 400 , validation  accuracy 0.526316\n",
      "step 400 , validation loss : 19.5062\n",
      "step 400 , test  accuracy 0.435897\n",
      "step 400 , test loss : 19.9415\n",
      "step 500 , training  accuracy 0.6\n",
      "step 500 , loss : 2.84597\n",
      "step 500 , validation  accuracy 0.526316\n",
      "step 500 , validation loss : 3.40311\n",
      "step 500 , test  accuracy 0.435897\n",
      "step 500 , test loss : 3.90504\n",
      "step 600 , training  accuracy 0.533333\n",
      "step 600 , loss : 1.97547\n",
      "step 600 , validation  accuracy 0.526316\n",
      "step 600 , validation loss : 2.14507\n",
      "step 600 , test  accuracy 0.410256\n",
      "step 600 , test loss : 2.31742\n",
      "step 700 , training  accuracy 0.433333\n",
      "step 700 , loss : 5.67585\n",
      "step 700 , validation  accuracy 0.447368\n",
      "step 700 , validation loss : 6.87623\n",
      "step 700 , test  accuracy 0.461538\n",
      "step 700 , test loss : 7.06891\n",
      "step 800 , training  accuracy 0.433333\n",
      "step 800 , loss : 5.09789\n",
      "step 800 , validation  accuracy 0.473684\n",
      "step 800 , validation loss : 6.52901\n",
      "step 800 , test  accuracy 0.564103\n",
      "step 800 , test loss : 6.48853\n",
      "step 900 , training  accuracy 0.433333\n",
      "step 900 , loss : 3.3005\n",
      "step 900 , validation  accuracy 0.473684\n",
      "step 900 , validation loss : 4.00732\n",
      "step 900 , test  accuracy 0.564103\n",
      "step 900 , test loss : 3.86781\n",
      "step 1000 , training  accuracy 0.5\n",
      "step 1000 , loss : 1.63544\n",
      "step 1000 , validation  accuracy 0.473684\n",
      "step 1000 , validation loss : 1.88984\n",
      "step 1000 , test  accuracy 0.564103\n",
      "step 1000 , test loss : 1.74089\n",
      "step 1100 , training  accuracy 0.5\n",
      "step 1100 , loss : 0.860809\n",
      "step 1100 , validation  accuracy 0.473684\n",
      "step 1100 , validation loss : 0.935549\n",
      "step 1100 , test  accuracy 0.564103\n",
      "step 1100 , test loss : 0.872947\n",
      "a\n",
      "step 1200 , training  accuracy 0.6\n",
      "step 1200 , loss : 0.803817\n",
      "step 1200 , validation  accuracy 0.815789\n",
      "step 1200 , validation loss : 0.785969\n",
      "step 1200 , test  accuracy 0.435897\n",
      "step 1200 , test loss : 0.834032\n",
      "step 1300 , training  accuracy 0.6\n",
      "step 1300 , loss : 1.06127\n",
      "step 1300 , validation  accuracy 0.578947\n",
      "step 1300 , validation loss : 1.02882\n",
      "step 1300 , test  accuracy 0.435897\n",
      "step 1300 , test loss : 1.17144\n",
      "step 1400 , training  accuracy 0.633333\n",
      "step 1400 , loss : 1.00399\n",
      "step 1400 , validation  accuracy 0.526316\n",
      "step 1400 , validation loss : 1.15689\n",
      "step 1400 , test  accuracy 0.435897\n",
      "step 1400 , test loss : 1.33037\n",
      "step 1500 , training  accuracy 0.366667\n",
      "step 1500 , loss : 1.34387\n",
      "step 1500 , validation  accuracy 0.526316\n",
      "step 1500 , validation loss : 1.15937\n",
      "step 1500 , test  accuracy 0.435897\n",
      "step 1500 , test loss : 1.33413\n",
      "step 1600 , training  accuracy 0.7\n",
      "step 1600 , loss : 0.822874\n",
      "step 1600 , validation  accuracy 0.526316\n",
      "step 1600 , validation loss : 0.942636\n",
      "step 1600 , test  accuracy 0.435897\n",
      "step 1600 , test loss : 1.06708\n",
      "step 1700 , training  accuracy 0.5\n",
      "step 1700 , loss : 0.747947\n",
      "step 1700 , validation  accuracy 0.552632\n",
      "step 1700 , validation loss : 0.743095\n",
      "step 1700 , test  accuracy 0.435897\n",
      "step 1700 , test loss : 0.819256\n",
      "step 1800 , training  accuracy 0.366667\n",
      "step 1800 , loss : 0.731675\n",
      "step 1800 , validation  accuracy 0.631579\n",
      "step 1800 , validation loss : 0.679871\n",
      "step 1800 , test  accuracy 0.615385\n",
      "step 1800 , test loss : 0.707851\n",
      "step 1900 , training  accuracy 0.5\n",
      "step 1900 , loss : 0.770536\n",
      "step 1900 , validation  accuracy 0.552632\n",
      "step 1900 , validation loss : 0.751298\n",
      "step 1900 , test  accuracy 0.564103\n",
      "step 1900 , test loss : 0.745571\n",
      "step 2000 , training  accuracy 0.5\n",
      "step 2000 , loss : 0.840618\n",
      "step 2000 , validation  accuracy 0.473684\n",
      "step 2000 , validation loss : 0.891506\n",
      "step 2000 , test  accuracy 0.564103\n",
      "step 2000 , test loss : 0.858464\n",
      "step 2100 , training  accuracy 0.3\n",
      "step 2100 , loss : 1.0327\n",
      "step 2100 , validation  accuracy 0.473684\n",
      "step 2100 , validation loss : 0.965427\n",
      "step 2100 , test  accuracy 0.564103\n",
      "step 2100 , test loss : 0.93331\n",
      "step 2200 , training  accuracy 0.566667\n",
      "step 2200 , loss : 0.822964\n",
      "step 2200 , validation  accuracy 0.473684\n",
      "step 2200 , validation loss : 0.940983\n",
      "step 2200 , test  accuracy 0.564103\n",
      "step 2200 , test loss : 0.921993\n",
      "step 2300 , training  accuracy 0.433333\n",
      "step 2300 , loss : 0.937902\n",
      "step 2300 , validation  accuracy 0.473684\n",
      "step 2300 , validation loss : 0.918224\n",
      "step 2300 , test  accuracy 0.589744\n",
      "step 2300 , test loss : 0.904991\n",
      "step 2400 , training  accuracy 0.366667\n",
      "step 2400 , loss : 0.88251\n",
      "step 2400 , validation  accuracy 0.473684\n",
      "step 2400 , validation loss : 0.851612\n",
      "step 2400 , test  accuracy 0.589744\n",
      "step 2400 , test loss : 0.849703\n",
      "step 2500 , training  accuracy 0.533333\n",
      "step 2500 , loss : 0.786832\n",
      "step 2500 , validation  accuracy 0.631579\n",
      "step 2500 , validation loss : 0.779205\n",
      "step 2500 , test  accuracy 0.564103\n",
      "step 2500 , test loss : 0.793998\n",
      "a\n",
      "step 2600 , training  accuracy 0.7\n",
      "step 2600 , loss : 0.741324\n",
      "step 2600 , validation  accuracy 0.657895\n",
      "step 2600 , validation loss : 0.727943\n",
      "step 2600 , test  accuracy 0.641026\n",
      "step 2600 , test loss : 0.761803\n",
      "step 2700 , training  accuracy 0.7\n",
      "step 2700 , loss : 0.688774\n",
      "step 2700 , validation  accuracy 0.578947\n",
      "step 2700 , validation loss : 0.709603\n",
      "step 2700 , test  accuracy 0.538462\n",
      "step 2700 , test loss : 0.760869\n",
      "step 2800 , training  accuracy 0.533333\n",
      "step 2800 , loss : 0.743116\n",
      "step 2800 , validation  accuracy 0.631579\n",
      "step 2800 , validation loss : 0.709615\n",
      "step 2800 , test  accuracy 0.435897\n",
      "step 2800 , test loss : 0.773611\n",
      "step 2900 , training  accuracy 0.7\n",
      "step 2900 , loss : 0.682478\n",
      "step 2900 , validation  accuracy 0.631579\n",
      "step 2900 , validation loss : 0.698489\n",
      "step 2900 , test  accuracy 0.410256\n",
      "step 2900 , test loss : 0.76882\n",
      "step 3000 , training  accuracy 0.633333\n",
      "step 3000 , loss : 0.689976\n",
      "step 3000 , validation  accuracy 0.605263\n",
      "step 3000 , validation loss : 0.693496\n",
      "step 3000 , test  accuracy 0.410256\n",
      "step 3000 , test loss : 0.768992\n",
      "step 3100 , training  accuracy 0.6\n",
      "step 3100 , loss : 0.660989\n",
      "step 3100 , validation  accuracy 0.578947\n",
      "step 3100 , validation loss : 0.689377\n",
      "step 3100 , test  accuracy 0.435897\n",
      "step 3100 , test loss : 0.768052\n",
      "step 3200 , training  accuracy 0.3\n",
      "step 3200 , loss : 0.767647\n",
      "step 3200 , validation  accuracy 0.631579\n",
      "step 3200 , validation loss : 0.679947\n",
      "step 3200 , test  accuracy 0.435897\n",
      "step 3200 , test loss : 0.760165\n",
      "step 3300 , training  accuracy 0.7\n",
      "step 3300 , loss : 0.662955\n",
      "step 3300 , validation  accuracy 0.631579\n",
      "step 3300 , validation loss : 0.659402\n",
      "step 3300 , test  accuracy 0.435897\n",
      "step 3300 , test loss : 0.733767\n",
      "step 3400 , training  accuracy 0.533333\n",
      "step 3400 , loss : 0.69252\n",
      "step 3400 , validation  accuracy 0.763158\n",
      "step 3400 , validation loss : 0.651225\n",
      "step 3400 , test  accuracy 0.48718\n",
      "step 3400 , test loss : 0.717527\n",
      "a\n",
      "step 3500 , training  accuracy 0.533333\n",
      "step 3500 , loss : 0.672566\n",
      "step 3500 , validation  accuracy 0.763158\n",
      "step 3500 , validation loss : 0.651154\n",
      "step 3500 , test  accuracy 0.615385\n",
      "step 3500 , test loss : 0.708584\n",
      "a\n",
      "step 3600 , training  accuracy 0.533333\n",
      "step 3600 , loss : 0.697587\n",
      "step 3600 , validation  accuracy 0.789474\n",
      "step 3600 , validation loss : 0.653469\n",
      "step 3600 , test  accuracy 0.589744\n",
      "step 3600 , test loss : 0.706025\n",
      "step 3700 , training  accuracy 0.7\n",
      "step 3700 , loss : 0.674583\n",
      "step 3700 , validation  accuracy 0.789474\n",
      "step 3700 , validation loss : 0.654351\n",
      "step 3700 , test  accuracy 0.564103\n",
      "step 3700 , test loss : 0.704529\n",
      "step 3800 , training  accuracy 0.7\n",
      "step 3800 , loss : 0.67596\n",
      "step 3800 , validation  accuracy 0.763158\n",
      "step 3800 , validation loss : 0.655419\n",
      "step 3800 , test  accuracy 0.564103\n",
      "step 3800 , test loss : 0.702105\n",
      "step 3900 , training  accuracy 0.633333\n",
      "step 3900 , loss : 0.688321\n",
      "step 3900 , validation  accuracy 0.789474\n",
      "step 3900 , validation loss : 0.653699\n",
      "step 3900 , test  accuracy 0.564103\n",
      "step 3900 , test loss : 0.701354\n",
      "step 4000 , training  accuracy 0.633333\n",
      "step 4000 , loss : 0.671795\n",
      "step 4000 , validation  accuracy 0.789474\n",
      "step 4000 , validation loss : 0.652611\n",
      "step 4000 , test  accuracy 0.538462\n",
      "step 4000 , test loss : 0.700755\n",
      "step 4100 , training  accuracy 0.7\n",
      "step 4100 , loss : 0.668919\n",
      "step 4100 , validation  accuracy 0.789474\n",
      "step 4100 , validation loss : 0.652414\n",
      "step 4100 , test  accuracy 0.538462\n",
      "step 4100 , test loss : 0.699612\n",
      "step 4200 , training  accuracy 0.666667\n",
      "step 4200 , loss : 0.66725\n",
      "step 4200 , validation  accuracy 0.763158\n",
      "step 4200 , validation loss : 0.652109\n",
      "step 4200 , test  accuracy 0.538462\n",
      "step 4200 , test loss : 0.697548\n",
      "step 4300 , training  accuracy 0.5\n",
      "step 4300 , loss : 0.683266\n",
      "step 4300 , validation  accuracy 0.789474\n",
      "step 4300 , validation loss : 0.651282\n",
      "step 4300 , test  accuracy 0.538462\n",
      "step 4300 , test loss : 0.695319\n",
      "step 4400 , training  accuracy 0.666667\n",
      "step 4400 , loss : 0.683289\n",
      "step 4400 , validation  accuracy 0.789474\n",
      "step 4400 , validation loss : 0.650216\n",
      "step 4400 , test  accuracy 0.564103\n",
      "step 4400 , test loss : 0.694458\n",
      "a\n",
      "step 4500 , training  accuracy 0.733333\n",
      "step 4500 , loss : 0.662093\n",
      "step 4500 , validation  accuracy 0.868421\n",
      "step 4500 , validation loss : 0.649949\n",
      "step 4500 , test  accuracy 0.589744\n",
      "step 4500 , test loss : 0.696357\n",
      "a\n",
      "step 4600 , training  accuracy 0.766667\n",
      "step 4600 , loss : 0.657995\n",
      "step 4600 , validation  accuracy 0.81579\n",
      "step 4600 , validation loss : 0.65088\n",
      "step 4600 , test  accuracy 0.666667\n",
      "step 4600 , test loss : 0.701623\n",
      "step 4700 , training  accuracy 0.733333\n",
      "step 4700 , loss : 0.649671\n",
      "step 4700 , validation  accuracy 0.789474\n",
      "step 4700 , validation loss : 0.652226\n",
      "step 4700 , test  accuracy 0.615385\n",
      "step 4700 , test loss : 0.706858\n",
      "step 4800 , training  accuracy 0.6\n",
      "step 4800 , loss : 0.682556\n",
      "step 4800 , validation  accuracy 0.710526\n",
      "step 4800 , validation loss : 0.655365\n",
      "step 4800 , test  accuracy 0.589744\n",
      "step 4800 , test loss : 0.712704\n",
      "step 4900 , training  accuracy 0.7\n",
      "step 4900 , loss : 0.656849\n",
      "step 4900 , validation  accuracy 0.736842\n",
      "step 4900 , validation loss : 0.65506\n",
      "step 4900 , test  accuracy 0.589744\n",
      "step 4900 , test loss : 0.712179\n",
      "step 5000 , training  accuracy 0.733333\n",
      "step 5000 , loss : 0.668825\n",
      "step 5000 , validation  accuracy 0.789474\n",
      "step 5000 , validation loss : 0.654415\n",
      "step 5000 , test  accuracy 0.564103\n",
      "step 5000 , test loss : 0.711846\n",
      "step 5100 , training  accuracy 0.733333\n",
      "step 5100 , loss : 0.655278\n",
      "step 5100 , validation  accuracy 0.789474\n",
      "step 5100 , validation loss : 0.651154\n",
      "step 5100 , test  accuracy 0.615385\n",
      "step 5100 , test loss : 0.708138\n",
      "a\n",
      "step 5200 , training  accuracy 0.766667\n",
      "step 5200 , loss : 0.647544\n",
      "step 5200 , validation  accuracy 0.842105\n",
      "step 5200 , validation loss : 0.648666\n",
      "step 5200 , test  accuracy 0.641026\n",
      "step 5200 , test loss : 0.70356\n",
      "a\n",
      "step 5300 , training  accuracy 0.666667\n",
      "step 5300 , loss : 0.659356\n",
      "step 5300 , validation  accuracy 0.868421\n",
      "step 5300 , validation loss : 0.646573\n",
      "step 5300 , test  accuracy 0.692308\n",
      "step 5300 , test loss : 0.699275\n",
      "a\n",
      "step 5400 , training  accuracy 0.766667\n",
      "step 5400 , loss : 0.650429\n",
      "step 5400 , validation  accuracy 0.894737\n",
      "step 5400 , validation loss : 0.645993\n",
      "step 5400 , test  accuracy 0.666667\n",
      "step 5400 , test loss : 0.695443\n",
      "step 5500 , training  accuracy 0.733333\n",
      "step 5500 , loss : 0.649587\n",
      "step 5500 , validation  accuracy 0.842105\n",
      "step 5500 , validation loss : 0.646313\n",
      "step 5500 , test  accuracy 0.692308\n",
      "step 5500 , test loss : 0.693245\n",
      "step 5600 , training  accuracy 0.666667\n",
      "step 5600 , loss : 0.664641\n",
      "step 5600 , validation  accuracy 0.842105\n",
      "step 5600 , validation loss : 0.647346\n",
      "step 5600 , test  accuracy 0.692308\n",
      "step 5600 , test loss : 0.692504\n",
      "step 5700 , training  accuracy 0.766667\n",
      "step 5700 , loss : 0.636304\n",
      "step 5700 , validation  accuracy 0.842105\n",
      "step 5700 , validation loss : 0.648522\n",
      "step 5700 , test  accuracy 0.666667\n",
      "step 5700 , test loss : 0.692762\n",
      "step 5800 , training  accuracy 0.766667\n",
      "step 5800 , loss : 0.646799\n",
      "step 5800 , validation  accuracy 0.868421\n",
      "step 5800 , validation loss : 0.649823\n",
      "step 5800 , test  accuracy 0.666667\n",
      "step 5800 , test loss : 0.691772\n",
      "step 5900 , training  accuracy 0.733333\n",
      "step 5900 , loss : 0.641374\n",
      "step 5900 , validation  accuracy 0.868421\n",
      "step 5900 , validation loss : 0.651007\n",
      "step 5900 , test  accuracy 0.692308\n",
      "step 5900 , test loss : 0.69194\n",
      "step 6000 , training  accuracy 0.8\n",
      "step 6000 , loss : 0.620348\n",
      "step 6000 , validation  accuracy 0.815789\n",
      "step 6000 , validation loss : 0.651273\n",
      "step 6000 , test  accuracy 0.666667\n",
      "step 6000 , test loss : 0.693904\n",
      "step 6100 , training  accuracy 0.8\n",
      "step 6100 , loss : 0.623321\n",
      "step 6100 , validation  accuracy 0.789474\n",
      "step 6100 , validation loss : 0.650094\n",
      "step 6100 , test  accuracy 0.615385\n",
      "step 6100 , test loss : 0.695673\n",
      "step 6200 , training  accuracy 0.8\n",
      "step 6200 , loss : 0.632152\n",
      "step 6200 , validation  accuracy 0.789474\n",
      "step 6200 , validation loss : 0.648704\n",
      "step 6200 , test  accuracy 0.589744\n",
      "step 6200 , test loss : 0.696303\n",
      "step 6300 , training  accuracy 0.733333\n",
      "step 6300 , loss : 0.634254\n",
      "step 6300 , validation  accuracy 0.789474\n",
      "step 6300 , validation loss : 0.644806\n",
      "step 6300 , test  accuracy 0.641026\n",
      "step 6300 , test loss : 0.691602\n",
      "step 6400 , training  accuracy 0.866667\n",
      "step 6400 , loss : 0.596236\n",
      "step 6400 , validation  accuracy 0.868421\n",
      "step 6400 , validation loss : 0.639968\n",
      "step 6400 , test  accuracy 0.692308\n",
      "step 6400 , test loss : 0.682938\n",
      "a\n",
      "step 6500 , training  accuracy 0.7\n",
      "step 6500 , loss : 0.642338\n",
      "step 6500 , validation  accuracy 0.894737\n",
      "step 6500 , validation loss : 0.635203\n",
      "step 6500 , test  accuracy 0.74359\n",
      "step 6500 , test loss : 0.675563\n",
      "step 6600 , training  accuracy 0.7\n",
      "step 6600 , loss : 0.637275\n",
      "step 6600 , validation  accuracy 0.789474\n",
      "step 6600 , validation loss : 0.633552\n",
      "step 6600 , test  accuracy 0.692308\n",
      "step 6600 , test loss : 0.669875\n",
      "step 6700 , training  accuracy 0.833333\n",
      "step 6700 , loss : 0.622009\n",
      "step 6700 , validation  accuracy 0.789474\n",
      "step 6700 , validation loss : 0.632612\n",
      "step 6700 , test  accuracy 0.692308\n",
      "step 6700 , test loss : 0.666907\n",
      "step 6800 , training  accuracy 0.8\n",
      "step 6800 , loss : 0.631639\n",
      "step 6800 , validation  accuracy 0.789474\n",
      "step 6800 , validation loss : 0.629714\n",
      "step 6800 , test  accuracy 0.666667\n",
      "step 6800 , test loss : 0.665929\n",
      "step 6900 , training  accuracy 0.8\n",
      "step 6900 , loss : 0.618437\n",
      "step 6900 , validation  accuracy 0.868421\n",
      "step 6900 , validation loss : 0.626457\n",
      "step 6900 , test  accuracy 0.666667\n",
      "step 6900 , test loss : 0.665804\n",
      "step 7000 , training  accuracy 0.7\n",
      "step 7000 , loss : 0.646552\n",
      "step 7000 , validation  accuracy 0.789474\n",
      "step 7000 , validation loss : 0.625397\n",
      "step 7000 , test  accuracy 0.641026\n",
      "step 7000 , test loss : 0.665742\n",
      "step 7100 , training  accuracy 0.8\n",
      "step 7100 , loss : 0.616248\n",
      "step 7100 , validation  accuracy 0.763158\n",
      "step 7100 , validation loss : 0.628253\n",
      "step 7100 , test  accuracy 0.615385\n",
      "step 7100 , test loss : 0.666276\n",
      "step 7200 , training  accuracy 0.766667\n",
      "step 7200 , loss : 0.647781\n",
      "step 7200 , validation  accuracy 0.763158\n",
      "step 7200 , validation loss : 0.629116\n",
      "step 7200 , test  accuracy 0.615385\n",
      "step 7200 , test loss : 0.666666\n",
      "step 7300 , training  accuracy 0.833333\n",
      "step 7300 , loss : 0.621595\n",
      "step 7300 , validation  accuracy 0.789474\n",
      "step 7300 , validation loss : 0.622761\n",
      "step 7300 , test  accuracy 0.615385\n",
      "step 7300 , test loss : 0.66618\n",
      "step 7400 , training  accuracy 0.833333\n",
      "step 7400 , loss : 0.605807\n",
      "step 7400 , validation  accuracy 0.842105\n",
      "step 7400 , validation loss : 0.614775\n",
      "step 7400 , test  accuracy 0.666667\n",
      "step 7400 , test loss : 0.669891\n",
      "step 7500 , training  accuracy 0.7\n",
      "step 7500 , loss : 0.604894\n",
      "step 7500 , validation  accuracy 0.842105\n",
      "step 7500 , validation loss : 0.611843\n",
      "step 7500 , test  accuracy 0.666667\n",
      "step 7500 , test loss : 0.677764\n",
      "step 7600 , training  accuracy 0.766667\n",
      "step 7600 , loss : 0.58256\n",
      "step 7600 , validation  accuracy 0.81579\n",
      "step 7600 , validation loss : 0.614563\n",
      "step 7600 , test  accuracy 0.692308\n",
      "step 7600 , test loss : 0.682502\n",
      "step 7700 , training  accuracy 0.633333\n",
      "step 7700 , loss : 0.653101\n",
      "step 7700 , validation  accuracy 0.789474\n",
      "step 7700 , validation loss : 0.617919\n",
      "step 7700 , test  accuracy 0.641026\n",
      "step 7700 , test loss : 0.687846\n",
      "step 7800 , training  accuracy 0.766667\n",
      "step 7800 , loss : 0.598116\n",
      "step 7800 , validation  accuracy 0.789474\n",
      "step 7800 , validation loss : 0.61853\n",
      "step 7800 , test  accuracy 0.615385\n",
      "step 7800 , test loss : 0.686988\n",
      "step 7900 , training  accuracy 0.7\n",
      "step 7900 , loss : 0.626127\n",
      "step 7900 , validation  accuracy 0.789474\n",
      "step 7900 , validation loss : 0.621357\n",
      "step 7900 , test  accuracy 0.589744\n",
      "step 7900 , test loss : 0.688581\n",
      "step 8000 , training  accuracy 0.766667\n",
      "step 8000 , loss : 0.598062\n",
      "step 8000 , validation  accuracy 0.789474\n",
      "step 8000 , validation loss : 0.619782\n",
      "step 8000 , test  accuracy 0.615385\n",
      "step 8000 , test loss : 0.679623\n",
      "step 8100 , training  accuracy 0.866667\n",
      "step 8100 , loss : 0.566383\n",
      "step 8100 , validation  accuracy 0.789474\n",
      "step 8100 , validation loss : 0.61793\n",
      "step 8100 , test  accuracy 0.666667\n",
      "step 8100 , test loss : 0.673249\n",
      "step 8200 , training  accuracy 0.866667\n",
      "step 8200 , loss : 0.572812\n",
      "step 8200 , validation  accuracy 0.789474\n",
      "step 8200 , validation loss : 0.616939\n",
      "step 8200 , test  accuracy 0.692308\n",
      "step 8200 , test loss : 0.669411\n",
      "step 8300 , training  accuracy 0.8\n",
      "step 8300 , loss : 0.6159\n",
      "step 8300 , validation  accuracy 0.81579\n",
      "step 8300 , validation loss : 0.615416\n",
      "step 8300 , test  accuracy 0.717949\n",
      "step 8300 , test loss : 0.662195\n",
      "step 8400 , training  accuracy 0.866667\n",
      "step 8400 , loss : 0.573733\n",
      "step 8400 , validation  accuracy 0.842105\n",
      "step 8400 , validation loss : 0.616594\n",
      "step 8400 , test  accuracy 0.74359\n",
      "step 8400 , test loss : 0.657796\n",
      "step 8500 , training  accuracy 0.866667\n",
      "step 8500 , loss : 0.575593\n",
      "step 8500 , validation  accuracy 0.815789\n",
      "step 8500 , validation loss : 0.617306\n",
      "step 8500 , test  accuracy 0.769231\n",
      "step 8500 , test loss : 0.654391\n",
      "step 8600 , training  accuracy 0.866667\n",
      "step 8600 , loss : 0.576858\n",
      "step 8600 , validation  accuracy 0.815789\n",
      "step 8600 , validation loss : 0.61931\n",
      "step 8600 , test  accuracy 0.794872\n",
      "step 8600 , test loss : 0.651655\n",
      "step 8700 , training  accuracy 0.966667\n",
      "step 8700 , loss : 0.553912\n",
      "step 8700 , validation  accuracy 0.815789\n",
      "step 8700 , validation loss : 0.622343\n",
      "step 8700 , test  accuracy 0.769231\n",
      "step 8700 , test loss : 0.650402\n",
      "step 8800 , training  accuracy 0.9\n",
      "step 8800 , loss : 0.562494\n",
      "step 8800 , validation  accuracy 0.789474\n",
      "step 8800 , validation loss : 0.629018\n",
      "step 8800 , test  accuracy 0.717949\n",
      "step 8800 , test loss : 0.650173\n",
      "step 8900 , training  accuracy 0.666667\n",
      "step 8900 , loss : 0.651813\n",
      "step 8900 , validation  accuracy 0.789474\n",
      "step 8900 , validation loss : 0.636563\n",
      "step 8900 , test  accuracy 0.692308\n",
      "step 8900 , test loss : 0.651722\n",
      "step 9000 , training  accuracy 0.6\n",
      "step 9000 , loss : 0.66915\n",
      "step 9000 , validation  accuracy 0.789474\n",
      "step 9000 , validation loss : 0.637545\n",
      "step 9000 , test  accuracy 0.717949\n",
      "step 9000 , test loss : 0.651549\n",
      "step 9100 , training  accuracy 0.8\n",
      "step 9100 , loss : 0.624399\n",
      "step 9100 , validation  accuracy 0.789474\n",
      "step 9100 , validation loss : 0.6327\n",
      "step 9100 , test  accuracy 0.717949\n",
      "step 9100 , test loss : 0.650396\n",
      "step 9200 , training  accuracy 0.766667\n",
      "step 9200 , loss : 0.626476\n",
      "step 9200 , validation  accuracy 0.789474\n",
      "step 9200 , validation loss : 0.627776\n",
      "step 9200 , test  accuracy 0.769231\n",
      "step 9200 , test loss : 0.651732\n",
      "step 9300 , training  accuracy 0.8\n",
      "step 9300 , loss : 0.603614\n",
      "step 9300 , validation  accuracy 0.763158\n",
      "step 9300 , validation loss : 0.625541\n",
      "step 9300 , test  accuracy 0.717949\n",
      "step 9300 , test loss : 0.656993\n",
      "step 9400 , training  accuracy 0.8\n",
      "step 9400 , loss : 0.599501\n",
      "step 9400 , validation  accuracy 0.763158\n",
      "step 9400 , validation loss : 0.626738\n",
      "step 9400 , test  accuracy 0.692308\n",
      "step 9400 , test loss : 0.665565\n",
      "step 9500 , training  accuracy 0.633333\n",
      "step 9500 , loss : 0.65212\n",
      "step 9500 , validation  accuracy 0.815789\n",
      "step 9500 , validation loss : 0.628987\n",
      "step 9500 , test  accuracy 0.692308\n",
      "step 9500 , test loss : 0.672151\n",
      "step 9600 , training  accuracy 0.8\n",
      "step 9600 , loss : 0.581346\n",
      "step 9600 , validation  accuracy 0.789474\n",
      "step 9600 , validation loss : 0.630083\n",
      "step 9600 , test  accuracy 0.692308\n",
      "step 9600 , test loss : 0.67682\n",
      "step 9700 , training  accuracy 0.833333\n",
      "step 9700 , loss : 0.572588\n",
      "step 9700 , validation  accuracy 0.789474\n",
      "step 9700 , validation loss : 0.632301\n",
      "step 9700 , test  accuracy 0.641026\n",
      "step 9700 , test loss : 0.684147\n",
      "step 9800 , training  accuracy 0.766667\n",
      "step 9800 , loss : 0.591546\n",
      "step 9800 , validation  accuracy 0.789474\n",
      "step 9800 , validation loss : 0.634825\n",
      "step 9800 , test  accuracy 0.615385\n",
      "step 9800 , test loss : 0.691063\n",
      "step 9900 , training  accuracy 0.866667\n",
      "step 9900 , loss : 0.556863\n",
      "step 9900 , validation  accuracy 0.789474\n",
      "step 9900 , validation loss : 0.631694\n",
      "step 9900 , test  accuracy 0.641026\n",
      "step 9900 , test loss : 0.689357\n",
      "step 10000 , training  accuracy 0.866667\n",
      "step 10000 , loss : 0.555933\n",
      "step 10000 , validation  accuracy 0.789474\n",
      "step 10000 , validation loss : 0.625916\n",
      "step 10000 , test  accuracy 0.641026\n",
      "step 10000 , test loss : 0.684986\n",
      "step 10100 , training  accuracy 0.833333\n",
      "step 10100 , loss : 0.549038\n",
      "step 10100 , validation  accuracy 0.815789\n",
      "step 10100 , validation loss : 0.618762\n",
      "step 10100 , test  accuracy 0.666667\n",
      "step 10100 , test loss : 0.676518\n",
      "step 10200 , training  accuracy 0.833333\n",
      "step 10200 , loss : 0.596541\n",
      "step 10200 , validation  accuracy 0.842105\n",
      "step 10200 , validation loss : 0.612316\n",
      "step 10200 , test  accuracy 0.769231\n",
      "step 10200 , test loss : 0.663055\n",
      "step 10300 , training  accuracy 0.966667\n",
      "step 10300 , loss : 0.552303\n",
      "step 10300 , validation  accuracy 0.842105\n",
      "step 10300 , validation loss : 0.612282\n",
      "step 10300 , test  accuracy 0.717949\n",
      "step 10300 , test loss : 0.657307\n",
      "step 10400 , training  accuracy 0.933333\n",
      "step 10400 , loss : 0.532291\n",
      "step 10400 , validation  accuracy 0.789474\n",
      "step 10400 , validation loss : 0.617304\n",
      "step 10400 , test  accuracy 0.717949\n",
      "step 10400 , test loss : 0.655649\n",
      "step 10500 , training  accuracy 0.966667\n",
      "step 10500 , loss : 0.552599\n",
      "step 10500 , validation  accuracy 0.763158\n",
      "step 10500 , validation loss : 0.622378\n",
      "step 10500 , test  accuracy 0.717949\n",
      "step 10500 , test loss : 0.655883\n",
      "step 10600 , training  accuracy 0.8\n",
      "step 10600 , loss : 0.599029\n",
      "step 10600 , validation  accuracy 0.763158\n",
      "step 10600 , validation loss : 0.624726\n",
      "step 10600 , test  accuracy 0.692308\n",
      "step 10600 , test loss : 0.655933\n",
      "step 10700 , training  accuracy 0.833333\n",
      "step 10700 , loss : 0.587964\n",
      "step 10700 , validation  accuracy 0.763158\n",
      "step 10700 , validation loss : 0.622059\n",
      "step 10700 , test  accuracy 0.692308\n",
      "step 10700 , test loss : 0.654945\n",
      "step 10800 , training  accuracy 0.8\n",
      "step 10800 , loss : 0.598601\n",
      "step 10800 , validation  accuracy 0.815789\n",
      "step 10800 , validation loss : 0.617036\n",
      "step 10800 , test  accuracy 0.692308\n",
      "step 10800 , test loss : 0.654414\n",
      "step 10900 , training  accuracy 0.966667\n",
      "step 10900 , loss : 0.548415\n",
      "step 10900 , validation  accuracy 0.815789\n",
      "step 10900 , validation loss : 0.612598\n",
      "step 10900 , test  accuracy 0.717949\n",
      "step 10900 , test loss : 0.65581\n",
      "step 11000 , training  accuracy 0.8\n",
      "step 11000 , loss : 0.586062\n",
      "step 11000 , validation  accuracy 0.868421\n",
      "step 11000 , validation loss : 0.610113\n",
      "step 11000 , test  accuracy 0.74359\n",
      "step 11000 , test loss : 0.658926\n",
      "step 11100 , training  accuracy 0.866667\n",
      "step 11100 , loss : 0.571488\n",
      "step 11100 , validation  accuracy 0.868421\n",
      "step 11100 , validation loss : 0.609506\n",
      "step 11100 , test  accuracy 0.74359\n",
      "step 11100 , test loss : 0.664771\n",
      "step 11200 , training  accuracy 0.7\n",
      "step 11200 , loss : 0.624516\n",
      "step 11200 , validation  accuracy 0.868421\n",
      "step 11200 , validation loss : 0.613123\n",
      "step 11200 , test  accuracy 0.717949\n",
      "step 11200 , test loss : 0.676936\n",
      "step 11300 , training  accuracy 0.833333\n",
      "step 11300 , loss : 0.569427\n",
      "step 11300 , validation  accuracy 0.894737\n",
      "step 11300 , validation loss : 0.615314\n",
      "step 11300 , test  accuracy 0.692308\n",
      "step 11300 , test loss : 0.684546\n",
      "step 11400 , training  accuracy 0.866667\n",
      "step 11400 , loss : 0.561945\n",
      "step 11400 , validation  accuracy 0.789474\n",
      "step 11400 , validation loss : 0.617708\n",
      "step 11400 , test  accuracy 0.666667\n",
      "step 11400 , test loss : 0.692604\n",
      "step 11500 , training  accuracy 0.9\n",
      "step 11500 , loss : 0.529587\n",
      "step 11500 , validation  accuracy 0.868421\n",
      "step 11500 , validation loss : 0.616054\n",
      "step 11500 , test  accuracy 0.666667\n",
      "step 11500 , test loss : 0.692996\n",
      "step 11600 , training  accuracy 0.9\n",
      "step 11600 , loss : 0.540833\n",
      "step 11600 , validation  accuracy 0.894737\n",
      "step 11600 , validation loss : 0.612114\n",
      "step 11600 , test  accuracy 0.666667\n",
      "step 11600 , test loss : 0.6883\n",
      "step 11700 , training  accuracy 0.866667\n",
      "step 11700 , loss : 0.554318\n",
      "step 11700 , validation  accuracy 0.894737\n",
      "step 11700 , validation loss : 0.608396\n",
      "step 11700 , test  accuracy 0.692308\n",
      "step 11700 , test loss : 0.682353\n",
      "a\n",
      "step 11800 , training  accuracy 0.766667\n",
      "step 11800 , loss : 0.626231\n",
      "step 11800 , validation  accuracy 0.894737\n",
      "step 11800 , validation loss : 0.602718\n",
      "step 11800 , test  accuracy 0.769231\n",
      "step 11800 , test loss : 0.673261\n",
      "step 11900 , training  accuracy 0.9\n",
      "step 11900 , loss : 0.536955\n",
      "step 11900 , validation  accuracy 0.842105\n",
      "step 11900 , validation loss : 0.600961\n",
      "step 11900 , test  accuracy 0.74359\n",
      "step 11900 , test loss : 0.670385\n",
      "step 12000 , training  accuracy 0.966667\n",
      "step 12000 , loss : 0.534506\n",
      "step 12000 , validation  accuracy 0.815789\n",
      "step 12000 , validation loss : 0.600471\n",
      "step 12000 , test  accuracy 0.794872\n",
      "step 12000 , test loss : 0.666075\n",
      "step 12100 , training  accuracy 0.933333\n",
      "step 12100 , loss : 0.525736\n",
      "step 12100 , validation  accuracy 0.842105\n",
      "step 12100 , validation loss : 0.600549\n",
      "step 12100 , test  accuracy 0.820513\n",
      "step 12100 , test loss : 0.664433\n",
      "step 12200 , training  accuracy 0.966667\n",
      "step 12200 , loss : 0.529046\n",
      "step 12200 , validation  accuracy 0.842105\n",
      "step 12200 , validation loss : 0.601305\n",
      "step 12200 , test  accuracy 0.769231\n",
      "step 12200 , test loss : 0.661356\n",
      "step 12300 , training  accuracy 0.966667\n",
      "step 12300 , loss : 0.544213\n",
      "step 12300 , validation  accuracy 0.842105\n",
      "step 12300 , validation loss : 0.601594\n",
      "step 12300 , test  accuracy 0.769231\n",
      "step 12300 , test loss : 0.660778\n",
      "step 12400 , training  accuracy 1\n",
      "step 12400 , loss : 0.501395\n",
      "step 12400 , validation  accuracy 0.842105\n",
      "step 12400 , validation loss : 0.597114\n",
      "step 12400 , test  accuracy 0.820513\n",
      "step 12400 , test loss : 0.664002\n",
      "step 12500 , training  accuracy 0.933333\n",
      "step 12500 , loss : 0.546793\n",
      "step 12500 , validation  accuracy 0.815789\n",
      "step 12500 , validation loss : 0.594162\n",
      "step 12500 , test  accuracy 0.794872\n",
      "step 12500 , test loss : 0.66794\n",
      "step 12600 , training  accuracy 0.966667\n",
      "step 12600 , loss : 0.524286\n",
      "step 12600 , validation  accuracy 0.815789\n",
      "step 12600 , validation loss : 0.59145\n",
      "step 12600 , test  accuracy 0.794872\n",
      "step 12600 , test loss : 0.670551\n",
      "step 12700 , training  accuracy 0.9\n",
      "step 12700 , loss : 0.533402\n",
      "step 12700 , validation  accuracy 0.842105\n",
      "step 12700 , validation loss : 0.590667\n",
      "step 12700 , test  accuracy 0.794872\n",
      "step 12700 , test loss : 0.669503\n",
      "step 12800 , training  accuracy 1\n",
      "step 12800 , loss : 0.492984\n",
      "step 12800 , validation  accuracy 0.842105\n",
      "step 12800 , validation loss : 0.590086\n",
      "step 12800 , test  accuracy 0.769231\n",
      "step 12800 , test loss : 0.669377\n",
      "step 12900 , training  accuracy 0.966667\n",
      "step 12900 , loss : 0.526518\n",
      "step 12900 , validation  accuracy 0.842105\n",
      "step 12900 , validation loss : 0.589969\n",
      "step 12900 , test  accuracy 0.74359\n",
      "step 12900 , test loss : 0.670355\n",
      "step 13000 , training  accuracy 0.966667\n",
      "step 13000 , loss : 0.526931\n",
      "step 13000 , validation  accuracy 0.868421\n",
      "step 13000 , validation loss : 0.587403\n",
      "step 13000 , test  accuracy 0.769231\n",
      "step 13000 , test loss : 0.674339\n",
      "step 13100 , training  accuracy 0.833333\n",
      "step 13100 , loss : 0.564447\n",
      "step 13100 , validation  accuracy 0.894737\n",
      "step 13100 , validation loss : 0.587173\n",
      "step 13100 , test  accuracy 0.74359\n",
      "step 13100 , test loss : 0.676667\n",
      "a\n",
      "step 13200 , training  accuracy 0.933333\n",
      "step 13200 , loss : 0.557864\n",
      "step 13200 , validation  accuracy 0.921053\n",
      "step 13200 , validation loss : 0.584971\n",
      "step 13200 , test  accuracy 0.769231\n",
      "step 13200 , test loss : 0.683875\n",
      "step 13300 , training  accuracy 1\n",
      "step 13300 , loss : 0.509631\n",
      "step 13300 , validation  accuracy 0.868421\n",
      "step 13300 , validation loss : 0.587368\n",
      "step 13300 , test  accuracy 0.717949\n",
      "step 13300 , test loss : 0.706748\n",
      "step 13400 , training  accuracy 0.866667\n",
      "step 13400 , loss : 0.558691\n",
      "step 13400 , validation  accuracy 0.868421\n",
      "step 13400 , validation loss : 0.59419\n",
      "step 13400 , test  accuracy 0.692308\n",
      "step 13400 , test loss : 0.723687\n",
      "step 13500 , training  accuracy 0.866667\n",
      "step 13500 , loss : 0.547637\n",
      "step 13500 , validation  accuracy 0.868421\n",
      "step 13500 , validation loss : 0.597447\n",
      "step 13500 , test  accuracy 0.692308\n",
      "step 13500 , test loss : 0.726279\n",
      "step 13600 , training  accuracy 0.9\n",
      "step 13600 , loss : 0.534278\n",
      "step 13600 , validation  accuracy 0.868421\n",
      "step 13600 , validation loss : 0.59366\n",
      "step 13600 , test  accuracy 0.692308\n",
      "step 13600 , test loss : 0.713761\n",
      "step 13700 , training  accuracy 0.966667\n",
      "step 13700 , loss : 0.504785\n",
      "step 13700 , validation  accuracy 0.842105\n",
      "step 13700 , validation loss : 0.58968\n",
      "step 13700 , test  accuracy 0.717949\n",
      "step 13700 , test loss : 0.692632\n",
      "step 13800 , training  accuracy 0.966667\n",
      "step 13800 , loss : 0.50942\n",
      "step 13800 , validation  accuracy 0.868421\n",
      "step 13800 , validation loss : 0.592347\n",
      "step 13800 , test  accuracy 0.769231\n",
      "step 13800 , test loss : 0.672069\n",
      "step 13900 , training  accuracy 0.966667\n",
      "step 13900 , loss : 0.518107\n",
      "step 13900 , validation  accuracy 0.815789\n",
      "step 13900 , validation loss : 0.603576\n",
      "step 13900 , test  accuracy 0.74359\n",
      "step 13900 , test loss : 0.664134\n",
      "step 14000 , training  accuracy 1\n",
      "step 14000 , loss : 0.519297\n",
      "step 14000 , validation  accuracy 0.789474\n",
      "step 14000 , validation loss : 0.605846\n",
      "step 14000 , test  accuracy 0.74359\n",
      "step 14000 , test loss : 0.662947\n",
      "step 14100 , training  accuracy 0.866667\n",
      "step 14100 , loss : 0.542774\n",
      "step 14100 , validation  accuracy 0.842105\n",
      "step 14100 , validation loss : 0.596473\n",
      "step 14100 , test  accuracy 0.794872\n",
      "step 14100 , test loss : 0.662928\n",
      "step 14200 , training  accuracy 1\n",
      "step 14200 , loss : 0.489828\n",
      "step 14200 , validation  accuracy 0.868421\n",
      "step 14200 , validation loss : 0.590388\n",
      "step 14200 , test  accuracy 0.794872\n",
      "step 14200 , test loss : 0.663371\n",
      "step 14300 , training  accuracy 0.9\n",
      "step 14300 , loss : 0.562419\n",
      "step 14300 , validation  accuracy 0.894737\n",
      "step 14300 , validation loss : 0.582802\n",
      "step 14300 , test  accuracy 0.74359\n",
      "step 14300 , test loss : 0.670414\n",
      "step 14400 , training  accuracy 0.933333\n",
      "step 14400 , loss : 0.536163\n",
      "step 14400 , validation  accuracy 0.894737\n",
      "step 14400 , validation loss : 0.579007\n",
      "step 14400 , test  accuracy 0.717949\n",
      "step 14400 , test loss : 0.680536\n",
      "step 14500 , training  accuracy 0.933333\n",
      "step 14500 , loss : 0.515924\n",
      "step 14500 , validation  accuracy 0.894737\n",
      "step 14500 , validation loss : 0.576326\n",
      "step 14500 , test  accuracy 0.692308\n",
      "step 14500 , test loss : 0.685539\n",
      "step 14600 , training  accuracy 0.966667\n",
      "step 14600 , loss : 0.48745\n",
      "step 14600 , validation  accuracy 0.894737\n",
      "step 14600 , validation loss : 0.575087\n",
      "step 14600 , test  accuracy 0.692308\n",
      "step 14600 , test loss : 0.694386\n",
      "step 14700 , training  accuracy 1\n",
      "step 14700 , loss : 0.497971\n",
      "step 14700 , validation  accuracy 0.894737\n",
      "step 14700 , validation loss : 0.574375\n",
      "step 14700 , test  accuracy 0.692308\n",
      "step 14700 , test loss : 0.701492\n",
      "step 14800 , training  accuracy 0.966667\n",
      "step 14800 , loss : 0.511575\n",
      "step 14800 , validation  accuracy 0.894737\n",
      "step 14800 , validation loss : 0.571728\n",
      "step 14800 , test  accuracy 0.717949\n",
      "step 14800 , test loss : 0.698402\n",
      "step 14900 , training  accuracy 1\n",
      "step 14900 , loss : 0.48809\n",
      "step 14900 , validation  accuracy 0.894737\n",
      "step 14900 , validation loss : 0.57\n",
      "step 14900 , test  accuracy 0.74359\n",
      "step 14900 , test loss : 0.68772\n",
      "step 15000 , training  accuracy 0.966667\n",
      "step 15000 , loss : 0.515437\n",
      "step 15000 , validation  accuracy 0.921053\n",
      "step 15000 , validation loss : 0.571082\n",
      "step 15000 , test  accuracy 0.717949\n",
      "step 15000 , test loss : 0.679863\n",
      "step 15100 , training  accuracy 1\n",
      "step 15100 , loss : 0.486137\n",
      "step 15100 , validation  accuracy 0.868421\n",
      "step 15100 , validation loss : 0.574348\n",
      "step 15100 , test  accuracy 0.692308\n",
      "step 15100 , test loss : 0.673414\n",
      "step 15200 , training  accuracy 0.9\n",
      "step 15200 , loss : 0.550604\n",
      "step 15200 , validation  accuracy 0.842105\n",
      "step 15200 , validation loss : 0.578612\n",
      "step 15200 , test  accuracy 0.717949\n",
      "step 15200 , test loss : 0.668267\n",
      "step 15300 , training  accuracy 0.966667\n",
      "step 15300 , loss : 0.512216\n",
      "step 15300 , validation  accuracy 0.921053\n",
      "step 15300 , validation loss : 0.572803\n",
      "step 15300 , test  accuracy 0.769231\n",
      "step 15300 , test loss : 0.673636\n",
      "step 15400 , training  accuracy 1\n",
      "step 15400 , loss : 0.509506\n",
      "step 15400 , validation  accuracy 0.894737\n",
      "step 15400 , validation loss : 0.577462\n",
      "step 15400 , test  accuracy 0.717949\n",
      "step 15400 , test loss : 0.692596\n",
      "step 15500 , training  accuracy 0.966667\n",
      "step 15500 , loss : 0.506804\n",
      "step 15500 , validation  accuracy 0.894737\n",
      "step 15500 , validation loss : 0.590603\n",
      "step 15500 , test  accuracy 0.692308\n",
      "step 15500 , test loss : 0.711539\n",
      "step 15600 , training  accuracy 1\n",
      "step 15600 , loss : 0.504928\n",
      "step 15600 , validation  accuracy 0.842105\n",
      "step 15600 , validation loss : 0.59994\n",
      "step 15600 , test  accuracy 0.666667\n",
      "step 15600 , test loss : 0.72047\n",
      "step 15700 , training  accuracy 1\n",
      "step 15700 , loss : 0.48913\n",
      "step 15700 , validation  accuracy 0.868421\n",
      "step 15700 , validation loss : 0.594532\n",
      "step 15700 , test  accuracy 0.717949\n",
      "step 15700 , test loss : 0.706354\n",
      "step 15800 , training  accuracy 0.966667\n",
      "step 15800 , loss : 0.497511\n",
      "step 15800 , validation  accuracy 0.921053\n",
      "step 15800 , validation loss : 0.586942\n",
      "step 15800 , test  accuracy 0.692308\n",
      "step 15800 , test loss : 0.686714\n",
      "step 15900 , training  accuracy 1\n",
      "step 15900 , loss : 0.504955\n",
      "step 15900 , validation  accuracy 0.921053\n",
      "step 15900 , validation loss : 0.58438\n",
      "step 15900 , test  accuracy 0.717949\n",
      "step 15900 , test loss : 0.67495\n",
      "step 16000 , training  accuracy 1\n",
      "step 16000 , loss : 0.489753\n",
      "step 16000 , validation  accuracy 0.921053\n",
      "step 16000 , validation loss : 0.583452\n",
      "step 16000 , test  accuracy 0.74359\n",
      "step 16000 , test loss : 0.679002\n",
      "step 16100 , training  accuracy 1\n",
      "step 16100 , loss : 0.481984\n",
      "step 16100 , validation  accuracy 0.868421\n",
      "step 16100 , validation loss : 0.583403\n",
      "step 16100 , test  accuracy 0.717949\n",
      "step 16100 , test loss : 0.678202\n",
      "step 16200 , training  accuracy 1\n",
      "step 16200 , loss : 0.479464\n",
      "step 16200 , validation  accuracy 0.868421\n",
      "step 16200 , validation loss : 0.584823\n",
      "step 16200 , test  accuracy 0.692308\n",
      "step 16200 , test loss : 0.675636\n",
      "step 16300 , training  accuracy 1\n",
      "step 16300 , loss : 0.513503\n",
      "step 16300 , validation  accuracy 0.789474\n",
      "step 16300 , validation loss : 0.586705\n",
      "step 16300 , test  accuracy 0.74359\n",
      "step 16300 , test loss : 0.672802\n",
      "step 16400 , training  accuracy 0.833333\n",
      "step 16400 , loss : 0.574825\n",
      "step 16400 , validation  accuracy 0.842105\n",
      "step 16400 , validation loss : 0.583786\n",
      "step 16400 , test  accuracy 0.717949\n",
      "step 16400 , test loss : 0.673981\n",
      "step 16500 , training  accuracy 0.933333\n",
      "step 16500 , loss : 0.523466\n",
      "step 16500 , validation  accuracy 0.842105\n",
      "step 16500 , validation loss : 0.582971\n",
      "step 16500 , test  accuracy 0.769231\n",
      "step 16500 , test loss : 0.673108\n",
      "step 16600 , training  accuracy 1\n",
      "step 16600 , loss : 0.482228\n",
      "step 16600 , validation  accuracy 0.815789\n",
      "step 16600 , validation loss : 0.591496\n",
      "step 16600 , test  accuracy 0.74359\n",
      "step 16600 , test loss : 0.66885\n",
      "step 16700 , training  accuracy 1\n",
      "step 16700 , loss : 0.479173\n",
      "step 16700 , validation  accuracy 0.789474\n",
      "step 16700 , validation loss : 0.598768\n",
      "step 16700 , test  accuracy 0.717949\n",
      "step 16700 , test loss : 0.667807\n",
      "step 16800 , training  accuracy 1\n",
      "step 16800 , loss : 0.484976\n",
      "step 16800 , validation  accuracy 0.789474\n",
      "step 16800 , validation loss : 0.599057\n",
      "step 16800 , test  accuracy 0.717949\n",
      "step 16800 , test loss : 0.667955\n",
      "step 16900 , training  accuracy 1\n",
      "step 16900 , loss : 0.482808\n",
      "step 16900 , validation  accuracy 0.789474\n",
      "step 16900 , validation loss : 0.595689\n",
      "step 16900 , test  accuracy 0.74359\n",
      "step 16900 , test loss : 0.668506\n",
      "step 17000 , training  accuracy 1\n",
      "step 17000 , loss : 0.477604\n",
      "step 17000 , validation  accuracy 0.868421\n",
      "step 17000 , validation loss : 0.587477\n",
      "step 17000 , test  accuracy 0.74359\n",
      "step 17000 , test loss : 0.670236\n",
      "step 17100 , training  accuracy 1\n",
      "step 17100 , loss : 0.494117\n",
      "step 17100 , validation  accuracy 0.868421\n",
      "step 17100 , validation loss : 0.584402\n",
      "step 17100 , test  accuracy 0.74359\n",
      "step 17100 , test loss : 0.671776\n",
      "step 17200 , training  accuracy 1\n",
      "step 17200 , loss : 0.482643\n",
      "step 17200 , validation  accuracy 0.868421\n",
      "step 17200 , validation loss : 0.577655\n",
      "step 17200 , test  accuracy 0.794872\n",
      "step 17200 , test loss : 0.681972\n",
      "step 17300 , training  accuracy 1\n",
      "step 17300 , loss : 0.488918\n",
      "step 17300 , validation  accuracy 0.894737\n",
      "step 17300 , validation loss : 0.581881\n",
      "step 17300 , test  accuracy 0.74359\n",
      "step 17300 , test loss : 0.704945\n",
      "step 17400 , training  accuracy 1\n",
      "step 17400 , loss : 0.501258\n",
      "step 17400 , validation  accuracy 0.868421\n",
      "step 17400 , validation loss : 0.602831\n",
      "step 17400 , test  accuracy 0.692308\n",
      "step 17400 , test loss : 0.740195\n",
      "step 17500 , training  accuracy 1\n",
      "step 17500 , loss : 0.521721\n",
      "step 17500 , validation  accuracy 0.842105\n",
      "step 17500 , validation loss : 0.616151\n",
      "step 17500 , test  accuracy 0.666667\n",
      "step 17500 , test loss : 0.755855\n",
      "step 17600 , training  accuracy 0.966667\n",
      "step 17600 , loss : 0.524168\n",
      "step 17600 , validation  accuracy 0.868421\n",
      "step 17600 , validation loss : 0.59783\n",
      "step 17600 , test  accuracy 0.717949\n",
      "step 17600 , test loss : 0.721668\n",
      "step 17700 , training  accuracy 1\n",
      "step 17700 , loss : 0.481747\n",
      "step 17700 , validation  accuracy 0.894737\n",
      "step 17700 , validation loss : 0.586762\n",
      "step 17700 , test  accuracy 0.74359\n",
      "step 17700 , test loss : 0.691711\n",
      "step 17800 , training  accuracy 1\n",
      "step 17800 , loss : 0.511286\n",
      "step 17800 , validation  accuracy 0.81579\n",
      "step 17800 , validation loss : 0.590794\n",
      "step 17800 , test  accuracy 0.769231\n",
      "step 17800 , test loss : 0.672115\n",
      "step 17900 , training  accuracy 1\n",
      "step 17900 , loss : 0.495904\n",
      "step 17900 , validation  accuracy 0.81579\n",
      "step 17900 , validation loss : 0.604875\n",
      "step 17900 , test  accuracy 0.692308\n",
      "step 17900 , test loss : 0.667788\n",
      "step 18000 , training  accuracy 1\n",
      "step 18000 , loss : 0.491539\n",
      "step 18000 , validation  accuracy 0.815789\n",
      "step 18000 , validation loss : 0.606588\n",
      "step 18000 , test  accuracy 0.692308\n",
      "step 18000 , test loss : 0.667108\n",
      "step 18100 , training  accuracy 1\n",
      "step 18100 , loss : 0.485341\n",
      "step 18100 , validation  accuracy 0.815789\n",
      "step 18100 , validation loss : 0.60405\n",
      "step 18100 , test  accuracy 0.717949\n",
      "step 18100 , test loss : 0.665326\n",
      "step 18200 , training  accuracy 0.966667\n",
      "step 18200 , loss : 0.514933\n",
      "step 18200 , validation  accuracy 0.789474\n",
      "step 18200 , validation loss : 0.60088\n",
      "step 18200 , test  accuracy 0.769231\n",
      "step 18200 , test loss : 0.664141\n",
      "step 18300 , training  accuracy 1\n",
      "step 18300 , loss : 0.481899\n",
      "step 18300 , validation  accuracy 0.842105\n",
      "step 18300 , validation loss : 0.593094\n",
      "step 18300 , test  accuracy 0.794872\n",
      "step 18300 , test loss : 0.672127\n",
      "step 18400 , training  accuracy 1\n",
      "step 18400 , loss : 0.474426\n",
      "step 18400 , validation  accuracy 0.868421\n",
      "step 18400 , validation loss : 0.593707\n",
      "step 18400 , test  accuracy 0.794872\n",
      "step 18400 , test loss : 0.686088\n",
      "step 18500 , training  accuracy 1\n",
      "step 18500 , loss : 0.49188\n",
      "step 18500 , validation  accuracy 0.868421\n",
      "step 18500 , validation loss : 0.597994\n",
      "step 18500 , test  accuracy 0.717949\n",
      "step 18500 , test loss : 0.698099\n",
      "step 18600 , training  accuracy 1\n",
      "step 18600 , loss : 0.482113\n",
      "step 18600 , validation  accuracy 0.921053\n",
      "step 18600 , validation loss : 0.604524\n",
      "step 18600 , test  accuracy 0.692308\n",
      "step 18600 , test loss : 0.713558\n",
      "step 18700 , training  accuracy 0.966667\n",
      "step 18700 , loss : 0.544999\n",
      "step 18700 , validation  accuracy 0.921053\n",
      "step 18700 , validation loss : 0.60874\n",
      "step 18700 , test  accuracy 0.692308\n",
      "step 18700 , test loss : 0.725064\n",
      "step 18800 , training  accuracy 1\n",
      "step 18800 , loss : 0.47442\n",
      "step 18800 , validation  accuracy 0.868421\n",
      "step 18800 , validation loss : 0.593198\n",
      "step 18800 , test  accuracy 0.769231\n",
      "step 18800 , test loss : 0.701795\n",
      "step 18900 , training  accuracy 1\n",
      "step 18900 , loss : 0.478637\n",
      "step 18900 , validation  accuracy 0.789474\n",
      "step 18900 , validation loss : 0.590254\n",
      "step 18900 , test  accuracy 0.74359\n",
      "step 18900 , test loss : 0.679867\n",
      "step 19000 , training  accuracy 1\n",
      "step 19000 , loss : 0.499718\n",
      "step 19000 , validation  accuracy 0.736842\n",
      "step 19000 , validation loss : 0.618133\n",
      "step 19000 , test  accuracy 0.666667\n",
      "step 19000 , test loss : 0.682003\n",
      "step 19100 , training  accuracy 0.933333\n",
      "step 19100 , loss : 0.528985\n",
      "step 19100 , validation  accuracy 0.763158\n",
      "step 19100 , validation loss : 0.636245\n",
      "step 19100 , test  accuracy 0.692308\n",
      "step 19100 , test loss : 0.691641\n",
      "step 19200 , training  accuracy 1\n",
      "step 19200 , loss : 0.492417\n",
      "step 19200 , validation  accuracy 0.763158\n",
      "step 19200 , validation loss : 0.611676\n",
      "step 19200 , test  accuracy 0.666667\n",
      "step 19200 , test loss : 0.686304\n",
      "step 19300 , training  accuracy 1\n",
      "step 19300 , loss : 0.49371\n",
      "step 19300 , validation  accuracy 0.815789\n",
      "step 19300 , validation loss : 0.588112\n",
      "step 19300 , test  accuracy 0.74359\n",
      "step 19300 , test loss : 0.690998\n",
      "step 19400 , training  accuracy 1\n",
      "step 19400 , loss : 0.489787\n",
      "step 19400 , validation  accuracy 0.894737\n",
      "step 19400 , validation loss : 0.582865\n",
      "step 19400 , test  accuracy 0.74359\n",
      "step 19400 , test loss : 0.719019\n",
      "step 19500 , training  accuracy 1\n",
      "step 19500 , loss : 0.501011\n",
      "step 19500 , validation  accuracy 0.894737\n",
      "step 19500 , validation loss : 0.594181\n",
      "step 19500 , test  accuracy 0.666667\n",
      "step 19500 , test loss : 0.756342\n",
      "step 19600 , training  accuracy 1\n",
      "step 19600 , loss : 0.485816\n",
      "step 19600 , validation  accuracy 0.894737\n",
      "step 19600 , validation loss : 0.597348\n",
      "step 19600 , test  accuracy 0.666667\n",
      "step 19600 , test loss : 0.769206\n",
      "step 19700 , training  accuracy 1\n",
      "step 19700 , loss : 0.503037\n",
      "step 19700 , validation  accuracy 0.894737\n",
      "step 19700 , validation loss : 0.593925\n",
      "step 19700 , test  accuracy 0.641026\n",
      "step 19700 , test loss : 0.76688\n",
      "step 19800 , training  accuracy 1\n",
      "step 19800 , loss : 0.499216\n",
      "step 19800 , validation  accuracy 0.894737\n",
      "step 19800 , validation loss : 0.579357\n",
      "step 19800 , test  accuracy 0.717949\n",
      "step 19800 , test loss : 0.735816\n",
      "step 19900 , training  accuracy 1\n",
      "step 19900 , loss : 0.487216\n",
      "step 19900 , validation  accuracy 0.868421\n",
      "step 19900 , validation loss : 0.574641\n",
      "step 19900 , test  accuracy 0.769231\n",
      "step 19900 , test loss : 0.706692\n",
      "step 20000 , training  accuracy 1\n",
      "step 20000 , loss : 0.480233\n",
      "step 20000 , validation  accuracy 0.815789\n",
      "step 20000 , validation loss : 0.588845\n",
      "step 20000 , test  accuracy 0.692308\n",
      "step 20000 , test loss : 0.693244\n",
      "step 20100 , training  accuracy 1\n",
      "step 20100 , loss : 0.491407\n",
      "step 20100 , validation  accuracy 0.815789\n",
      "step 20100 , validation loss : 0.595005\n",
      "step 20100 , test  accuracy 0.717949\n",
      "step 20100 , test loss : 0.694043\n",
      "step 20200 , training  accuracy 1\n",
      "step 20200 , loss : 0.495819\n",
      "step 20200 , validation  accuracy 0.815789\n",
      "step 20200 , validation loss : 0.587758\n",
      "step 20200 , test  accuracy 0.717949\n",
      "step 20200 , test loss : 0.695808\n",
      "step 20300 , training  accuracy 1\n",
      "step 20300 , loss : 0.492125\n",
      "step 20300 , validation  accuracy 0.842105\n",
      "step 20300 , validation loss : 0.576994\n",
      "step 20300 , test  accuracy 0.74359\n",
      "step 20300 , test loss : 0.701154\n",
      "step 20400 , training  accuracy 1\n",
      "step 20400 , loss : 0.475787\n",
      "step 20400 , validation  accuracy 0.842105\n",
      "step 20400 , validation loss : 0.572098\n",
      "step 20400 , test  accuracy 0.74359\n",
      "step 20400 , test loss : 0.705941\n",
      "step 20500 , training  accuracy 1\n",
      "step 20500 , loss : 0.471572\n",
      "step 20500 , validation  accuracy 0.868421\n",
      "step 20500 , validation loss : 0.568755\n",
      "step 20500 , test  accuracy 0.717949\n",
      "step 20500 , test loss : 0.708908\n",
      "step 20600 , training  accuracy 1\n",
      "step 20600 , loss : 0.498305\n",
      "step 20600 , validation  accuracy 0.868421\n",
      "step 20600 , validation loss : 0.566981\n",
      "step 20600 , test  accuracy 0.74359\n",
      "step 20600 , test loss : 0.712106\n",
      "step 20700 , training  accuracy 1\n",
      "step 20700 , loss : 0.49285\n",
      "step 20700 , validation  accuracy 0.842105\n",
      "step 20700 , validation loss : 0.567803\n",
      "step 20700 , test  accuracy 0.717949\n",
      "step 20700 , test loss : 0.705516\n",
      "step 20800 , training  accuracy 1\n",
      "step 20800 , loss : 0.471783\n",
      "step 20800 , validation  accuracy 0.842105\n",
      "step 20800 , validation loss : 0.571057\n",
      "step 20800 , test  accuracy 0.74359\n",
      "step 20800 , test loss : 0.695743\n",
      "step 20900 , training  accuracy 1\n",
      "step 20900 , loss : 0.481253\n",
      "step 20900 , validation  accuracy 0.842105\n",
      "step 20900 , validation loss : 0.572409\n",
      "step 20900 , test  accuracy 0.74359\n",
      "step 20900 , test loss : 0.691615\n",
      "step 21000 , training  accuracy 1\n",
      "step 21000 , loss : 0.471166\n",
      "step 21000 , validation  accuracy 0.842105\n",
      "step 21000 , validation loss : 0.575433\n",
      "step 21000 , test  accuracy 0.769231\n",
      "step 21000 , test loss : 0.684994\n",
      "step 21100 , training  accuracy 1\n",
      "step 21100 , loss : 0.475259\n",
      "step 21100 , validation  accuracy 0.842105\n",
      "step 21100 , validation loss : 0.578129\n",
      "step 21100 , test  accuracy 0.794872\n",
      "step 21100 , test loss : 0.681221\n",
      "step 21200 , training  accuracy 1\n",
      "step 21200 , loss : 0.476607\n",
      "step 21200 , validation  accuracy 0.868421\n",
      "step 21200 , validation loss : 0.578504\n",
      "step 21200 , test  accuracy 0.794872\n",
      "step 21200 , test loss : 0.681148\n",
      "step 21300 , training  accuracy 1\n",
      "step 21300 , loss : 0.473985\n",
      "step 21300 , validation  accuracy 0.868421\n",
      "step 21300 , validation loss : 0.578652\n",
      "step 21300 , test  accuracy 0.769231\n",
      "step 21300 , test loss : 0.682212\n",
      "step 21400 , training  accuracy 1\n",
      "step 21400 , loss : 0.475227\n",
      "step 21400 , validation  accuracy 0.868421\n",
      "step 21400 , validation loss : 0.580165\n",
      "step 21400 , test  accuracy 0.769231\n",
      "step 21400 , test loss : 0.680647\n",
      "step 21500 , training  accuracy 1\n",
      "step 21500 , loss : 0.472364\n",
      "step 21500 , validation  accuracy 0.868421\n",
      "step 21500 , validation loss : 0.578647\n",
      "step 21500 , test  accuracy 0.769231\n",
      "step 21500 , test loss : 0.685897\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "\n",
    "    regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    pred=tf.nn.softmax(y_conv)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_))\n",
    "with tf.device('/gpu:0'):\n",
    "    cost = cost+regular\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(cost) #1e-4\n",
    "    with tf.name_scope(\"accuracy\"):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "batch_count=0\n",
    "max_acc=0\n",
    "if divide_flag ==True:\n",
    "    n_batch =len(train_images)\n",
    "    batch_count=0\n",
    "show_Exception_flag=True\n",
    "val_acc_list=[]\n",
    "val_loss_list=[]\n",
    "train_acc_list=[]\n",
    "train_loss_list=[]\n",
    "for i in range(iterate):    \n",
    "    if divide_flag ==True:\n",
    "        if batch_count >= n_batch:\n",
    "            batch_count =0\n",
    "        train_img =np.load(file_locate+train_images[batch_count])\n",
    "        train_lab =np.load(file_locate+train_labels[batch_count])\n",
    "    batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "   # batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "\n",
    "    if i%100 ==0: # in here add to validation \n",
    "        try:\n",
    "            val_accuracy = sess.run( accuracy , feed_dict={x:val_img , y_:val_lab , keep_prob: 1.0})        \n",
    "            val_loss = sess.run(cost , feed_dict = {x:val_img , y_: val_lab , keep_prob: 1.0})\n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "            test_accuracy,test_loss= sess.run([accuracy,cost]  , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})\n",
    "            \n",
    "            val_acc_list.append(val_accuracy)\n",
    "            val_loss_list.append(val_loss)\n",
    "            train_acc_list.append(train_accuracy)\n",
    "            train_loss_list.append(train_loss)\n",
    "            \n",
    "            if (val_accuracy+test_accuracy)/2 > max_acc:\n",
    "                print 'a'\n",
    "                save_numpy_weight(model_save_path)\n",
    "                max_acc=(val_accuracy+test_accuracy)/2\n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "            print(\"step %d , test  accuracy %g\" %(i,test_accuracy))\n",
    "            print(\"step %d , test loss : %g\" %(i,test_loss))\n",
    "            \n",
    "            f.write(val_str)\n",
    "            f.write(train_str)\n",
    "            if divide_flag ==True:\n",
    "                batch_count+=1\n",
    "        except Exception as e:\n",
    "            if show_Exception_flag:\n",
    "                print str(e)\n",
    "                show_Exception_flag=False\n",
    "            \n",
    "            list_acc=[]\n",
    "            list_loss=[]\n",
    "            n_divide=len(val_img)/batch_size\n",
    "            j=0\n",
    "            for j in range(n_divide):\n",
    "                # j*batch_size :(j+1)*batch_size\n",
    "                val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :(j+1)*batch_size] , y_:val_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                list_acc.append(float(val_accuracy))\n",
    "                list_loss.append(float(val_loss))\n",
    "            val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :] , y_:val_lab[ j*batch_size :  ] , keep_prob: 1.0})         \n",
    "            list_acc=np.asarray(list_acc)\n",
    "            list_loss= np.asarray(list_loss)\n",
    "            val_accuracy=np.mean(list_acc)\n",
    "            val_loss = np.mean(list_loss)\n",
    "            \n",
    "            val_acc_list.append(val_accuracy)\n",
    "            val_loss_list.append(val_loss)\n",
    "\n",
    "                        \n",
    "            list_acc=[]\n",
    "            list_loss=[]                \n",
    "            for j in range(n_divide):    \n",
    "                # j*batch_size :(j+1)*batch_size\n",
    "                test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                list_acc.append(float(test_accuracy))\n",
    "                list_loss.append(float(test_loss))\n",
    "            #right above code have to modify\n",
    "            test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :] , y_:val_lab[ j*batch_size :  ] , keep_prob: 1.0})         \n",
    "            list_acc.append(test_accuracy)\n",
    "            list_loss.append(test_loss)\n",
    "            \n",
    "\n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            \n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "            train_acc_list.append(train_accuracy)\n",
    "            train_loss_list.append(train_loss)\n",
    "            \n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "            \n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "            print(\"step %d , test  accuracy %g\" %(i,test_accuracy))\n",
    "            print(\"step %d , test loss : %g\" %(i,test_loss))           \n",
    "            \n",
    "            f.write(val_str)\n",
    "            f.write(train_str)\n",
    "            batch_count+=1\n",
    "            \n",
    "            val_acc_list.append(val_accuracy)\n",
    "            val_loss_list.append(val_loss)\n",
    "            train_acc_list.append(train_accuracy)\n",
    "            train_loss_list.append(train_loss)    \n",
    "\n",
    "        sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "\n",
    "np.save(model_save_path+'val_acc',np.asarray(val_acc_list))\n",
    "np.save(model_save_path+'val_loss',np.asarray(val_loss_list))\n",
    "np.save(model_save_path+'train_acc',np.asarray(train_acc_list))\n",
    "np.save(model_save_path+'train_loss',np.asarray(train_loss_list))\n",
    "\n",
    "softmax_=sess.run( pred , feed_dict={x:test_img  ,y_:test_lab, keep_prob: 1.0})\n",
    "test_accuracy,test_loss= sess.run([accuracy,cost]  , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})\n",
    "print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "f.write(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print softmax_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_img=np.load('/home/seongjung/save_numpy/1.npy')\n",
    "print np.shape(test_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    softmax_=sess.run( accuracy , feed_dict={x:test_img  , keep_prob: 1.0})\n",
    "    test_accuracy = sess.run( accuracy , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})        \n",
    "    test_loss = sess.run(cost , feed_dict = {x:test_img , y_: test_lab , keep_prob: 1.0})\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "except :\n",
    "    list_acc=[]\n",
    "    list_loss=[]\n",
    "    n_divide=len(test_img)/batch_size\n",
    "    for j in range(n_divide):\n",
    "\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "        list_acc.append(float(test_accuracy))\n",
    "        list_loss.append(float(test_loss))\n",
    "    test_accuracy , test_loss=sess.run([accuracy,cost] , feed_dict={x:test_img[(j+1)*batch_size : ] , y_:test_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "    #right above code have to modify\n",
    "\n",
    "    list_acc.append(test_accuracy)\n",
    "    list_loss.append(test_loss)\n",
    "    list_acc=np.asarray(list_acc)\n",
    "    list_loss= np.asarray(list_loss)\n",
    "\n",
    "    test_accuracy=np.mean(list_acc)\n",
    "    test_loss = np.mean(list_loss)\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(train_img[8])\n",
    "train_lab[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
