{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 32, 32, 3)\n",
      "32 32\n",
      "32 32\n"
     ]
    }
   ],
   "source": [
    "#conv Neural Network\n",
    "# tensorboard --logdir=/home/ncc/notebook/learn/tensorboard/log\n",
    "\"\"\"\n",
    "created by kim Seong jung\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os \n",
    "\n",
    "file_locate='./cifar_merge/'\n",
    "sess = tf.InteractiveSession()\n",
    "test_img=np.load(file_locate+'test_img.npy');\n",
    "try:\n",
    "    print np.shape(test_img)\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "except:\n",
    "    np.shape(test_img)\n",
    "    test_img=np.reshape(test_img , newshape = [np.shape(test_img)[0] , 32, 32 ,3] )\n",
    "    img_row = np.shape(test_img)[1]\n",
    "    img_col = np.shape(test_img)[2]\n",
    "\n",
    "    \n",
    "divide_flag= False\n",
    "batch_size=30\n",
    "print img_row ,img_col\n",
    "n_classes =10\n",
    "in_ch =3\n",
    "out_ch1=200\n",
    "out_ch2=200\n",
    "out_ch3=200\n",
    "out_ch4=200\n",
    "out_ch5=200\n",
    "\n",
    "\n",
    "fully_ch1=1024\n",
    "fully_ch2 =1024\n",
    "fully_ch3 =1024\n",
    "\n",
    "\n",
    "\n",
    "strides_1=[1,2,2,1]\n",
    "strides_2=[1,1,1,1]\n",
    "strides_3=[1,1,1,1]\n",
    "strides_4=[1,1,1,1]\n",
    "strides_5=[1,1,1,1]\n",
    "\n",
    "\n",
    "x= tf.placeholder(\"float\",shape=[None,img_col , img_row ,3],  name = 'x-input')\n",
    "y_=tf.placeholder(\"float\",shape=[None , n_classes] , name = 'y-input')\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "x_image= tf.reshape(x,[-1,img_row,img_col,3])\n",
    "\n",
    "iterate=1000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "weight_row =3 ; weight_col=3\n",
    "\n",
    "pooling_row_size1=int(img_row/2)\n",
    "pooling_row_size2=int(pooling_row_size1/2)\n",
    "pooling_row_size3=int(pooling_row_size2/2)\n",
    "pooling_row_size4=int(pooling_row_size3/2)\n",
    "pooling_row_size5=int(pooling_row_size4/2)\n",
    "pooling_col_size1=int(img_col/2)\n",
    "pooling_col_size2=int(pooling_col_size1/2)\n",
    "pooling_col_size3=int(pooling_col_size2/2)\n",
    "pooling_col_size4=int(pooling_col_size3/2)\n",
    "pooling_col_size5=int(pooling_col_size4/2)\n",
    "\n",
    "print img_col , img_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/user01/notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data (50000, 32, 32, 3)\n",
      "Training Data Label (50000, 10)\n",
      "Test Data Label (5000, 10)\n",
      "val Data Label (5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:3'):\n",
    "#with tf.device('/gpu:1'):\n",
    "\n",
    "    if divide_flag == False:\n",
    "        train_img=np.load(file_locate+'train_img.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_img)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n",
    "\n",
    "    if divide_flag == True:\n",
    "        train_img=np.load(file_locate+'train_img_1.npy');\n",
    "        train_lab=np.load(file_locate+'train_lab_1.npy');\n",
    "        val_img= np.load(file_locate+'val_img.npy');\n",
    "        val_lab = np.load(file_locate+'val_lab.npy');\n",
    "        test_img=np.load(file_locate+'test_img.npy');\n",
    "        test_lab=np.load(file_locate+'test_lab.npy');\n",
    "\n",
    "        print \"Training Data\",np.shape(train_img)\n",
    "        print \"Training Data Label\",np.shape(train_lab)\n",
    "        print \"Test Data Label\",np.shape(test_lab)\n",
    "        print \"val Data Label\" , np.shape(val_lab)\n",
    "\n",
    "        n_train= np.shape(train_img)[0]\n",
    "        n_train_lab = np.shape(train_lab)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"def weight_variable(name,shape):\n",
    "    #initial = tf.truncated_normal(shape , stddev=0.1)\n",
    "    initial = tf.get_variable(name,shape=shape , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    return tf.Variable(initial)\"\"\"\n",
    "with tf.device('/gpu:0'):\n",
    "    def bias_variable(shape , name):\n",
    "        initial = tf.constant(0.1 , shape=shape)\n",
    "        return tf.Variable(initial)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    def next_batch(batch_size , image , label):\n",
    "\n",
    "        a=np.random.randint(np.shape(image)[0] -batch_size)\n",
    "        batch_x = image[a:a+batch_size,:]\n",
    "        batch_y= label[a:a+batch_size,:]\n",
    "        return batch_x, batch_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "\n",
    "    def conv2d(x,w,strides_ , name):\n",
    "        return tf.nn.conv2d(x,w, strides = strides_, padding='SAME' , name=name )\n",
    "    def max_pool_2x2(x , name):\n",
    "        return tf.nn.max_pool(x , ksize=[1,2,2,1] ,strides = [1,2,2,1] , padding = 'SAME' , name=name  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    try:\n",
    "        w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv1 = tf.get_variable(\"W1\",[weight_row,weight_col,in_ch,out_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope(\"layer1\") as scope:\n",
    "    try:\n",
    "        b_conv1 = bias_variable([out_ch1] , name ='B1')\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv1 = bias_variable([out_ch1],name ='B1')\n",
    "                \n",
    "            \n",
    "            \n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    try:\n",
    "        w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv2 = tf.get_variable(\"W2\",[weight_row,weight_col,out_ch1,out_ch2] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "with tf.variable_scope('layer2') as scope:\n",
    "    try:\n",
    "        b_conv2= bias_variable([out_ch2],name ='B2')\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv2= bias_variable([out_ch2],name ='B2')\n",
    "                \n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    try:\n",
    "        w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv3 = tf.get_variable(\"W3\" ,[weight_row,weight_col,out_ch2,out_ch3] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer3') as scope:\n",
    "    try:\n",
    "        b_conv3 = bias_variable([out_ch3],name ='B3')\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv3 = bias_variable([out_ch3],name ='B3')\n",
    "        \n",
    "with tf.variable_scope('layer4') as scope:\n",
    "    try:\n",
    "        w_conv4 =tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch3,out_ch4] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv4 = tf.get_variable(\"W4\" ,[weight_row,weight_col,out_ch3,out_ch4] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer4') as scope:\n",
    "    try:\n",
    "        b_conv4 = bias_variable([out_ch4],name ='B4')\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv4 = bias_variable([out_ch4],name ='B4')\n",
    "        \n",
    "with tf.variable_scope('layer5') as scope:\n",
    "    try:\n",
    "        w_conv5 = tf.get_variable(\"W5\",[weight_row,weight_col,out_ch4,out_ch5] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        w_conv5 = tf.get_variable(\"W5\" ,[weight_row,weight_col,out_ch4,out_ch5] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "with tf.variable_scope('layer5') as scope:\n",
    "    try:\n",
    "        b_conv5 = bias_variable([out_ch5],name ='B5')\n",
    "    except:\n",
    "        scope.reuse_variables()\n",
    "        b_conv5 = bias_variable([out_ch5],name ='B5')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"convolution_1/Relu:0\", shape=(?, 16, 16, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"convolution_2/conv2_maxpool:0\", shape=(?, 8, 8, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"convolution_3/Relu:0\", shape=(?, 8, 8, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"convolution_4/Relu:0\", shape=(?, 8, 8, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"convolution_5/conv5_maxpool:0\", shape=(?, 4, 4, 200), dtype=float32, device=/device:GPU:0)\n",
      "end_conv Tensor(\"identity_h_conv5:0\", shape=(?, 4, 4, 200), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "#conncect hidden layer \n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope(\"convolution_1\") as scope:\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image , w_conv1 ,strides_1 , 'x_image_wconv1')+b_conv1)\n",
    "    with tf.variable_scope(\"convolution_2\") as scope:\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_conv1 , w_conv2 ,strides_2 , 'conv1_conv2')+b_conv2)\n",
    "        h_conv2 = max_pool_2x2(h_conv2 , 'conv2_maxpool')#pooling\n",
    "    with tf.variable_scope(\"convolution_3\") as scope:\n",
    "        h_conv3 = tf.nn.relu(conv2d(h_conv2 , w_conv3,strides_3,'conv2_conv3')+b_conv3)\n",
    "    with tf.variable_scope(\"convolution_4\") as scope:\n",
    "        h_conv4 = tf.nn.relu(conv2d(h_conv3 , w_conv4,strides_4,'conv3_conv4')+b_conv4)\n",
    "        h_pool4 = max_pool_2x2(h_conv4,'conv4_maxpool') #pooling \n",
    "    with tf.variable_scope(\"convolution_5\") as scope:\n",
    "        h_conv5 = tf.nn.relu(conv2d(h_conv4, w_conv5,strides_5 ,'conv4_conv5')+b_conv5)\n",
    "        h_conv5= max_pool_2x2(h_conv5 , 'conv5_maxpool') #pooling \n",
    "\n",
    "    print h_conv1\n",
    "    print h_conv2\n",
    "    print h_conv3\n",
    "    print h_conv4\n",
    "    print h_conv5\n",
    "    \n",
    "    \n",
    "    end_conv = tf.identity(h_conv5 , name='identity_h_conv5')\n",
    "    print 'end_conv' , end_conv\n",
    "    #print conv2d(h_pool1 , w_conv2).get_shape()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "end_conv_row=int(h_conv5.get_shape()[1])\n",
    "end_conv_col=int(h_conv5.get_shape()[2])\n",
    "end_conv_ch=int(h_conv5.get_shape()[3])\n",
    "#connect fully connected layer \n",
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope(\"fc1\") as scope:\n",
    "        try:\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc1=tf.get_variable(\"fc1_W\",[end_conv_col*end_conv_row*end_conv_ch,fully_ch1] , initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc1 = bias_variable([fully_ch1] , name='fc1_B')\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc1 = bias_variable([fully_ch1],'fc2_B')\n",
    "\n",
    "\n",
    "    with tf.device('/gpu:0'): # flat conv layer \n",
    "        end_flat_conv =tf.reshape(end_conv, [-1,end_conv_col*end_conv_row*end_conv_ch] , name='endlayer_reshape')\n",
    "\n",
    "    with tf.device('/gpu:0'): # connect flat layer with fully  connnected layer \n",
    "        h_fc1 = tf.nn.relu(tf.matmul(end_flat_conv , w_fc1 ,name='matmul_flat_fc1')+ b_fc1)\n",
    "        h_fc1 = tf.nn.dropout(h_fc1, keep_prob , name='fc1_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        try:\n",
    "            w_fc2 =tf.get_variable(\"fc2_W\",[fully_ch1 , fully_ch2],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc2 =tf.get_variable(\"fc2_W\",[fully_ch1 , fully_ch2],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc2 = bias_variable([fully_ch2] , 'fc2_B')\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc2 = bias_variable([fully_ch2], 'fc2_B')\n",
    "\n",
    "    with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "        h_fc2 = tf.nn.relu(tf.matmul(h_fc1 , w_fc2, name='matmul_fc1_fc2')+b_fc2 ,name= 'fc2_relu')\n",
    "        h_fc2= tf.nn.dropout(h_fc2 , keep_prob , name='fc2_dropout')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('fc3') as scope:\n",
    "        try:\n",
    "            w_fc3 =tf.get_variable(\"fc3_W\",[fully_ch2 , fully_ch3],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_fc3 =tf.get_variable(\"fc3_W\",[fully_ch2 , fully_ch3],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_fc3 = bias_variable([fully_ch3] ,name='fc3_B')\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_fc3 = bias_variable([fully_ch3],'fc3_B')\n",
    "\n",
    "    with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "        h_fc3 = tf.nn.relu(tf.matmul(h_fc2 , w_fc3 , name='matmul_fc2_w_fc3')+b_fc3 , name='fc3_relu')\n",
    "        h_fc3= tf.nn.dropout(h_fc3 , keep_prob , name='fc3_relu')\n",
    "        end_fc=tf.identity(h_fc3 , name='identityh_fc3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    with tf.variable_scope('end_layer') as scope:\n",
    "        try:\n",
    "            w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes ],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            w_end =tf.get_variable(\"end_W\",[fully_ch3 , n_classes],initializer = tf.contrib.layers.xavier_initializer())\n",
    "        try:\n",
    "            b_end = bias_variable([n_classes],name=\"end_B\")\n",
    "        except:\n",
    "            scope.reuse_variables()\n",
    "            b_end = bias_variable([n_classes],name=\"end_B\")\n",
    "\n",
    "    with tf.device('/gpu:0'):  # join flat layer with fully  connnected layer \n",
    "        y_conv = tf.matmul(h_fc3 , w_end , name=\"matmul_h_fc3_w_end\")+b_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is recorded at :106\n"
     ]
    }
   ],
   "source": [
    "#dirname = '/home/ncc/notebook/mammo/result/'\n",
    "\n",
    "dirname='/home/user01/notebook/'\n",
    "    \n",
    "count=0\n",
    "while(True):\n",
    "    if not os.path.isdir(dirname):\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    elif not os.path.isdir(dirname + str(count)):\n",
    "        dirname=dirname+str(count)\n",
    "        os.mkdir(dirname)\n",
    "        break\n",
    "    else:\n",
    "        count+=1\n",
    "print 'it is recorded at :'+str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open(dirname+\"/log.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_list(folder_path):\n",
    "    list_files=os.walk(folder_path).next()[2]\n",
    "    print list_files\n",
    "    ret_train_img_list=[]\n",
    "    ret_train_lab_list=[]\n",
    "    for i , ele in enumerate(list_files):\n",
    "\n",
    "        if 'train'  in ele and 'img'in ele:\n",
    "            ret_train_img_list.append(ele)\n",
    "        elif 'train' in ele  and  'lab' in ele:\n",
    "            ret_train_lab_list.append(ele)\n",
    "    return ret_train_img_list ,ret_train_lab_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_lab.npy', 'val_lab.npy', 'train_img.npy', 'test_img.npy', 'val_img.npy', 'train_lab.npy']\n"
     ]
    }
   ],
   "source": [
    "train_images , train_labels  = get_batch_list(file_locate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_img.npy']\n",
      "['train_lab.npy']\n"
     ]
    }
   ],
   "source": [
    "def atoi(text):\n",
    "    return int(text) if text.isdigit() else text\n",
    "\n",
    "def natural_keys(text):\n",
    "    return [ atoi(c) for c in re.split('(\\d+)', text) ]\n",
    "\n",
    "\n",
    "train_images.sort(key=natural_keys)\n",
    "train_labels.sort(key = natural_keys)\n",
    "print(train_images)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 , training  accuracy 0.0666667\n",
      "step 0 , loss : 35.0818\n",
      "step 0 , validation  accuracy 0.0976447\n",
      "step 0 , validation loss : 40.161\n",
      "step 100 , training  accuracy 0.333333\n",
      "step 100 , loss : 2.27469\n",
      "step 100 , validation  accuracy 0.222675\n",
      "step 100 , validation loss : 2.28392\n",
      "step 200 , training  accuracy 0.1\n",
      "step 200 , loss : 2.32305\n",
      "step 200 , validation  accuracy 0.258483\n",
      "step 200 , validation loss : 2.2681\n",
      "step 300 , training  accuracy 0.233333\n",
      "step 300 , loss : 2.27926\n",
      "step 300 , validation  accuracy 0.26503\n",
      "step 300 , validation loss : 2.26096\n",
      "step 400 , training  accuracy 0.233333\n",
      "step 400 , loss : 2.27232\n",
      "step 400 , validation  accuracy 0.286627\n",
      "step 400 , validation loss : 2.25178\n",
      "step 500 , training  accuracy 0.2\n",
      "step 500 , loss : 2.26219\n",
      "step 500 , validation  accuracy 0.277605\n",
      "step 500 , validation loss : 2.2542\n",
      "step 600 , training  accuracy 0.233333\n",
      "step 600 , loss : 2.25407\n",
      "step 600 , validation  accuracy 0.276727\n",
      "step 600 , validation loss : 2.25228\n",
      "step 700 , training  accuracy 0.233333\n",
      "step 700 , loss : 2.24022\n",
      "step 700 , validation  accuracy 0.304271\n",
      "step 700 , validation loss : 2.24117\n",
      "step 800 , training  accuracy 0.166667\n",
      "step 800 , loss : 2.24794\n",
      "step 800 , validation  accuracy 0.315329\n",
      "step 800 , validation loss : 2.23631\n",
      "step 900 , training  accuracy 0.266667\n",
      "step 900 , loss : 2.23073\n",
      "step 900 , validation  accuracy 0.32483\n",
      "step 900 , validation loss : 2.23481\n",
      "--- Training Time : 18.3825380802 ---\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "#sm_conv= tf.nn.softmax(y_conv)\n",
    "    #cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "    start_time = time.time()\n",
    "with tf.name_scope(\"training\"):\n",
    "    regular=0.01*(tf.reduce_sum(tf.square(y_conv)))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits( y_conv, y_ , name='softmax_cross_entropy') )\n",
    "    with tf.device('/gpu:0'):\n",
    "        cost = tf.add(cost,regular , name='add_cost_reg') \n",
    "        train_step = tf.train.AdamOptimizer(1e-4).minimize(cost) #1e-4\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            with tf.name_scope('correct_prediction'):\n",
    "                correct_prediction = tf.equal(tf.argmax(y_conv,1) ,tf.argmax(y_,1))\n",
    "            with tf.name_scope('accuracy'):\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction , \"float\")) \n",
    "tf.summary.scalar(\"accuracy \" , accuracy)\n",
    "sess = tf.Session()\n",
    "merge = tf.merge_all_summaries()\n",
    "writer= tf.train.SummaryWriter(\"./tensorboard3\"+'/validataion_accuracy' ,sess.graph)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "if divide_flag ==True:\n",
    "    n_batch =len(train_images)\n",
    "    batch_count=0\n",
    "\n",
    "for i in range(iterate):    \n",
    "    if divide_flag ==True:\n",
    "        if batch_count >= n_batch:\n",
    "            batch_count =0\n",
    "        train_img =np.load(file_locate+train_images[batch_count])\n",
    "    \n",
    "        train_lab =np.load(file_locate+train_labels[batch_count])\n",
    "    batch_xs , batch_ys = next_batch(batch_size, train_img , train_lab)\n",
    "   # batch_val_xs  , batch_val_ys = next_batch(20 , val_img , val_lab)\n",
    "    if i%100 ==0: # in here add to validation \n",
    "        try:\n",
    "            result,val_accuracy = sess.run(merge,accuracy , feed_dict={x:val_img , y_:val_lab , keep_prob: 1.0})        \n",
    "            val_loss = sess.run(cost , feed_dict = {x:val_img , y_: val_lab , keep_prob: 1.0})\n",
    "            writer.add_summary(result, step)\n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "\n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "          \n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "            \n",
    "            \n",
    "            f.write(val_str)\n",
    "            f.write(train_str)\n",
    "            if divide_flag ==True:\n",
    "                batch_count+=1\n",
    "        except :\n",
    "            list_acc=[]\n",
    "            list_loss=[]\n",
    "            n_divide=len(val_img)/batch_size\n",
    "            j=0\n",
    "            for j in range(n_divide):\n",
    "                \n",
    "                # j*batch_size :(j+1)*batch_size\n",
    "                val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :(j+1)*batch_size] , y_:val_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "                list_acc.append(float(val_accuracy))\n",
    "                list_loss.append(float(val_loss))\n",
    "            #right above code have to modify\n",
    "            val_accuracy,val_loss = sess.run([accuracy ,cost], feed_dict={x:val_img[ j*batch_size :] , y_:val_lab[ j*batch_size :  ] , keep_prob: 1.0})         \n",
    "            list_acc.append(val_accuracy)\n",
    "            list_loss.append(val_loss)\n",
    "            \n",
    "            list_acc=np.asarray(list_acc)\n",
    "            list_loss= np.asarray(list_loss)\n",
    "            \n",
    "            val_accuracy=np.mean(list_acc)\n",
    "            val_loss = np.mean(list_loss)\n",
    "            \n",
    "            #result = sess.run(sm_conv , feed_dict = {x:val_img , y_:batch_ys , keep_prob :1.0})\n",
    "            \n",
    "            train_accuracy = sess.run( accuracy , feed_dict={x:batch_xs , y_:batch_ys , keep_prob: 1.0})        \n",
    "            train_loss = sess.run(cost , feed_dict = {x:batch_xs, y_: batch_ys, keep_prob: 1.0})\n",
    "\n",
    "            print(\"step %d , training  accuracy %g\" %(i,train_accuracy))\n",
    "            print(\"step %d , loss : %g\" %(i,train_loss))\n",
    "            train_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(train_loss) +'\\tval accuracy:\\t'+str(train_accuracy)+'\\n'\n",
    "            \n",
    "            print(\"step %d , validation  accuracy %g\" %(i,val_accuracy))\n",
    "            print(\"step %d , validation loss : %g\" %(i,val_loss))\n",
    "            val_str = 'step:\\t'+str(i)+'\\tval_loss:\\t'+str(val_loss) +'\\tval accuracy:\\t'+str(val_accuracy)+'\\n'\n",
    "           \n",
    "            \n",
    "            f.write(val_str)\n",
    "            f.write(train_str)\n",
    "            if divide_flag ==True:\n",
    "                batch_count+=1\n",
    "    \n",
    "    sess.run(train_step ,feed_dict={x:batch_xs , y_:batch_ys , keep_prob : 0.7})\n",
    "    \n",
    "print(\"--- Training Time : %s ---\" % (time.time() - start_time))\n",
    "train_time=\"--- Training Time : ---:\\t\" +str(time.time() - start_time)\n",
    "f.write(train_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print ''\n",
    "print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2 , testidation  accuracy 0.3516\n",
      "step 2 , testidation loss : 11.0528\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    test_accuracy = sess.run( accuracy , feed_dict={x:test_img , y_:test_lab , keep_prob: 1.0})        \n",
    "    test_loss = sess.run(cost , feed_dict = {x:test_img , y_: test_lab , keep_prob: 1.0})\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "except :\n",
    "    list_acc=[]\n",
    "    list_loss=[]\n",
    "    n_divide=len(test_img)/batch_size\n",
    "    for j in range(n_divide):\n",
    "\n",
    "        # j*batch_size :(j+1)*batch_size\n",
    "        test_accuracy,test_loss = sess.run([accuracy ,cost], feed_dict={x:test_img[ j*batch_size :(j+1)*batch_size] , y_:test_lab[ j*batch_size :(j+1)*batch_size ] , keep_prob: 1.0})        \n",
    "        list_acc.append(float(test_accuracy))\n",
    "        list_loss.append(float(test_loss))\n",
    "    test_accuracy , test_loss=sess.run([accuracy,cost] , feed_dict={x:test_img[(j+1)*batch_size : ] , y_:test_lab[(j+1)*(batch_size) : ] , keep_prob : 1.0})\n",
    "    #right above code have to modify\n",
    "\n",
    "    list_acc.append(test_accuracy)\n",
    "    list_loss.append(test_loss)\n",
    "    list_acc=np.asarray(list_acc)\n",
    "    list_loss= np.asarray(list_loss)\n",
    "\n",
    "    test_accuracy=np.mean(list_acc)\n",
    "    test_loss = np.mean(list_loss)\n",
    "\n",
    "    #result = sess.run(sm_conv , feed_dict = {x:test_img , y_:batch_ys , keep_prob :1.0})\n",
    "    print(\"step %d , testidation  accuracy %g\" %(i,test_accuracy))\n",
    "    print(\"step %d , testidation loss : %g\" %(i,test_loss))\n",
    "    test_str = 'step:\\t'+str(i)+'\\ttest_loss:\\t'+str(test_loss) +'\\ttest accuracy:\\t'+str(test_accuracy)+'\\n'\n",
    "\n",
    "    f.write(test_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
